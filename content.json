{"posts":[{"title":"deep-learning-optimizer-2","text":"尽管梯度下降（gradient descent）很少直接用于深度学习， 但了解它是理解下一节随机梯度下降算法的关键。 例如，由于学习率过大，优化问题可能会发散，这种现象早已在梯度下降中出现。 同样地，预处理是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。 让我们从简单的一维梯度下降开始介绍。 1.2. 梯度下降尽管梯度下降（gradient descent）很少直接用于深度学习， 但了解它是理解下一节随机梯度下降算法的关键。 例如，由于学习率过大，优化问题可能会发散，这种现象早已在梯度下降中出现。 同样地，预处理是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。 让我们从简单的一维梯度下降开始介绍。 1.2.1. 一维梯度下降为什么梯度下降算法可以优化目标函数？ 一维中的梯度下降给我们很好的启发。 考虑一类连续可微实值函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$， 利用泰勒展开，我们可以得到$$f(x + \\epsilon) = f(x) + \\epsilon f’(x) + O(\\epsilon^2). \\tag{1.2.1}$$即在一阶近似中，$f(x + \\epsilon)$ 可通过$x$处的函数值$f(x)$和一阶导数$f’(x)$得出。我们可以假设在负梯度方向上移动的$\\epsilon$会减少$f$的值。为简单起见，选择固定步长$\\eta$, 然后取$\\epsilon=\\eta f’(x)$。将其代入泰勒展开式我们可以得到$$f(x - \\eta f’(x)) = f(x) - \\eta f’^2(x) + O(\\eta^2 f’^2(x)) \\tag{1.2.2}$$如果其导数$f’(x) \\neq 0$ 没有消失， 我们就能继续展开，这是因为$\\eta f’^2(x) &gt; 0$。 此外，我们总是可以令$\\eta$小到足以使高阶项变得不相关。 因此，$$f(x - \\eta f’(x)) \\lessapprox f(x). \\tag{1.2.3}$$这意味着，如果我们使用:$$x \\leftarrow x - \\eta f’(x) \\tag{1.2.4}$$来迭代$x$，函数$f(x)$的值可能会下降。 因此，在梯度下降中，我们首先选择初始值$x$和常数$\\eta &gt; 0$， 然后使用它们连续迭代$x$，直到停止条件达成。 例如，当梯度$|f’(x)|$的幅度足够小或迭代次数达到某个值时。 下面我们来展示如何实现梯度下降。为了简单起见，我们选用目标函数$f(x)=x^2$。 尽管我们知道$x=0$时$f(x)$能取得最小值， 但我们仍然使用这个简单的函数来观察$x$的变化。 12345%matplotlib inlineimport numpy as npimport torchfrom mpl_toolkits import mplot3dfrom d2l import torch as d2l 12345def f(x): return x**2def f_grad(x): return 2 * x 接下来，我们使用$x=10$作为初始值，并假设$\\eta=0.2$。 使用梯度下降法迭代$x$共10次，我们可以看到，$x$的值最终将接近最优解。 1234567891011def gd(eta, f_grad): x = 10 # init x position results = [x] for i in range(10): x -= eta * f_grad(x) results.append(float(x)) print(f'epoch 10, x:{x:f}') return resultsresults = gd(0.2, f_grad) epoch 10, x:0.060466 对进行$x$优化的过程可以绘制如下。 12345678def show_trace(results, f): n = max(abs(min(results)), abs(max(results))) f_line = torch.arange(-n, n, 0.01) d2l.set_figsize() d2l.plot([f_line, results], [[f(x) for x in f_line], [ f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])show_trace(results, f) 1.2.1.1. 学习率学习率（learning rate）决定目标函数能否收敛到局部最小值，以及何时收敛到最小值。 学习率$\\eta$可由算法设计者设置。 请注意，如果使用的学习率太小，将导致$x$的更新非常缓慢，需要更多的迭代。 例如，考虑同一优化问题中$\\eta=0.05$的进度。 如下所示，尽管经过了10个步骤，我们仍然离最优解很远。 1show_trace(gd(0.05, f_grad), f) epoch 10, x:3.486784 相反，如果我们使用过高的学习率，$|\\eta f’(x)|$对于一阶泰勒展开式可能太大。 也就是说，(1.2.2)中的$O(\\eta^2 f’^2(x))$可能变得显著了。 在这种情况下，$x$的迭代不能保证降低$f(x)$的值。 例如，当学习率为$\\eta = 1.1$时，$x$超出了最优解$x = 0$并逐渐发散。 1show_trace(gd(1.1, f_grad), f) epoch 10, x:61.917364 1.2.1.2. 局部最小值为了演示非凸函数的梯度下降，考虑函数$f(x) = x \\cdot \\cos(cx)$，其中$c$为某常数。 这个函数有无穷多个局部最小值。 根据我们选择的学习率，我们最终可能只会得到许多解的一个。 下面的例子说明了（不切实际的）高学习率如何导致较差的局部最小值。 123456789c = torch.tensor(0.15 * np.pi)def f(x): # 目标函数 return x * torch.cos(c * x)def f_grad(x): # 目标函数的梯度 return torch.cos(c * x) - c * x * torch.sin(c * x)show_trace(gd(2, f_grad), f) epoch 10, x:-1.528166 1.2.2. 多元梯度下降现在我们对单变量的情况有了更好的理解，让我们考虑一下$\\mathbf{x} = [x_1, x_2, \\dots, x_d]^T$的情况。 即目标函数$f: \\mathbb{R^d} \\rightarrow \\mathbb{R}$将向量映射成标量。 相应地，它的梯度也是多元的，它是一个由$d$个偏导数组成的向量：$$\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d} \\right]^T. \\tag{1.2.5}$$梯度中的每个偏导数元素$\\partial f(\\mathbf{x}) / \\partial x_i$代表了当输入$x_i$时$f$在$\\mathbf{x}$处的变化率。 和先前单变量的情况一样，我们可以对多变量函数使用相应的泰勒近似来思考。 具体来说，$$f(\\mathbf{x} + \\epsilon) = f(\\mathbf{x}) + \\epsilon^T \\nabla f(\\mathbf{x}) + O(||\\epsilon||^2) \\tag{1.2.6}$$换句话说，在$\\epsilon$的二阶项中， 最陡下降的方向由负梯度$-\\nabla f(\\mathbf{x})$得出。 选择合适的学习率$\\eta &gt; 0$来生成典型的梯度下降算法：$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x}). \\tag{1.2.7}$$这个算法在实践中的表现如何呢？ 我们构造一个目标函数$f(\\mathbf{x}) = x_1^2 + 2x_2^2$， 并有二维向量$\\mathbf{x} = [x_1, x_2]^T$作为输入， 标量作为输出。 梯度由$\\nabla f(\\mathbf{x}) = [2x_1, 4x_2]^T$给出。 我们将从初始位置$[-5, -2]$通过梯度下降观察$\\mathbf{x}$的轨迹。 1234567891011121314151617181920212223def train_2d(trainer, steps=20, f_grad=None): #@save &quot;&quot;&quot;用定制的训练机优化2D目标函数&quot;&quot;&quot; # s1和s2是稍后将使用的内部状态变量 x1, x2, s1, s2 = -5, -2, 0, 0 results = [(x1, x2)] for i in range(steps): if f_grad: x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad) else: x1, x2, s1, s2 = trainer(x1, x2, s1, s2) results.append((x1, x2)) print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}') return resultsdef show_trace_2d(f, results): #@save &quot;&quot;&quot;显示优化过程中2D变量的轨迹&quot;&quot;&quot; d2l.set_figsize() d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e') x1, x2 = torch.meshgrid(torch.arange(-5.5, 1.0, 0.1), torch.arange(-3.0, 1.0, 0.1), indexing='ij') d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4') d2l.plt.xlabel('x1') d2l.plt.ylabel('x2') 接下来，我们观察学习率$\\eta = 0.1$时优化变量$\\mathbf{x}$的轨迹。 可以看到，经过20步之后，$\\mathbf{x}$的值接近其位于$[0, 0]$的最小值。 虽然进展相当顺利，但相当缓慢。 123456789101112def f_2d(x1, x2): # 目标函数 return x1 ** 2 + 2 * x2 ** 2def f_2d_grad(x1, x2): # 目标函数的梯度 return (2 * x1, 4 * x2)def gd_2d(x1, x2, s1, s2, f_grad): g1, g2 = f_grad(x1, x2) return (x1 - eta * g1, x2 - eta * g2, 0, 0)eta = 0.1show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad)) epoch 20, x1: -0.057646, x2: -0.000073 1.2.3. 小结 学习率的大小很重要：学习率太大会使模型发散，学习率太小会没有进展。 梯度下降会可能陷入局部极小值，而得不到全局最小值。 在高维模型中，调整学习率是很复杂的。","link":"/2024/10/20/deep-learning-optimizer-2/"},{"title":"deep-learning-optimizer-3","text":"在前面中，我们一直在训练过程中使用随机梯度下降，但没有解释它为什么起作用。为了解释这一点，我们刚在 1.3中描述了梯度下降的基本原则。本节继续更详细地说明随机梯度下（stochastic gradient descent）。 1.3. 随机梯度下降在前面中，我们一直在训练过程中使用随机梯度下降，但没有解释它为什么起作用。为了解释这一点，我们刚在 1.3中描述了梯度下降的基本原则。本节继续更详细地说明随机梯度下（stochastic gradient descent）。 1234%matplotlib inlineimport mathimport torchfrom d2l import torch as d2l 1.3.1. 随机梯度更新在深度学习中，目标函数通常是训练数据集中每个样本的损失函数的平均值。给定$n$个样本的训练数据集，我们假设$f_i(\\mathbf{x})$是关于索引$i$的训练样本的损失函数，其中$\\mathbf{x}$是参数向量。然后我们得到目标函数$$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\mathbf{x}). \\tag{1.3.1}$$$\\mathbf{x}$的目标函数的梯度计算为$$\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\mathbf{x}) \\tag{1.3.2}$$ 如果使用梯度下降法，则每个自变量迭代的计算代价为$O(n)$，它随$n$线性增长。因此，当训练数据集较大时，每次迭代的梯度下降计算代价将较高。 随机梯度下降（SGD）可降低每次迭代时的计算代价。在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引$i$，其中$i \\in {i,\\dots, n}$，并计算梯度 $\\nabla f_i(\\mathbf{x})$以更新$\\mathbf{x}$：$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}), \\tag{1.3.3}$$其中 $\\eta$ 是学习率。我们可以看到，每次迭代的计算代价从梯度下降的 $O(n)$降至常数O(1)。此外，我们要强调，随机梯度 $\\nabla f_i(\\mathbf{x})$ 是对完整梯度$\\nabla f(\\mathbf{x})$的无偏估计，因为$$\\mathbb{E}i \\nabla f_i(\\mathbf{x}) = \\frac{1}{n} \\sum{i=1}^{n} \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x}). \\tag{1.3.4}$$这意味着，平均而言，随机梯度是对梯度的良好估计。 现在，我们将把它与梯度下降进行比较，方法是向梯度添加均值为0、方差为1的随机噪声，以模拟随机梯度下降。 1234567891011121314151617181920def f(x1, x2): # 目标函数 return x1 ** 2 + 2 * x2 ** 2def f_grad(x1, x2): # 目标函数的梯度 return 2 * x1, 4 * x2def sgd(x1, x2, s1, s2, f_grad): g1, g2 = f_grad(x1, x2) # 模拟有噪声的梯度 g1 += torch.normal(0.0, 1, (1,)).item() g2 += torch.normal(0.0, 1, (1,)).item() eta_t = eta * lr() return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)def constant_lr(): return 1eta = 0.1lr = constant_lr # 常数学习速度d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad)) epoch 50, x1: -0.002198, x2: 0.017078 正如我们所看到的，随机梯度下降中变量的轨迹比我们在 11.3节中观察到的梯度下降中观察到的轨迹嘈杂得多。这是由于梯度的随机性质。也就是说，即使我们接近最小值，我们仍然受到通过$\\eta \\nabla f_i(\\mathbf{x})$的瞬间梯度所注入的不确定性的影响。即使经过50次迭代，质量仍然不那么好。更糟糕的是，经过额外的步骤，它不会得到改善。 1.3.2. 小结 对于凸问题，我们可以证明，对于广泛的学习率选择，随机梯度下降将收敛到最优解。 如果学习率太小或太大，就会出现问题。实际上，通常只有经过多次实验后才能找到合适的学习率。 当训练数据集中有更多样本时，计算梯度下降的每次迭代的代价更高，因此在这些情况下，首选随机梯度下降。 随机梯度下降的最优性保证在非凸情况下一般不可用","link":"/2024/10/20/deep-learning-optimizer-3/"},{"title":"deep-learning-optimizer-4","text":"基于梯度的学习方法中遇到了两个极端情况： 使用完整数据集来计算梯度并更新参数， 以及一次处理一个训练样本来取得进展。 二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。 而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。 这暗示了两者之间可能有折中方案，这便涉及到小批量随机梯度下降（minibatch gradient descent）。 1.4. 小批量随机梯度下降到目前为止，我们在基于梯度的学习方法中遇到了两个极端情况： 使用完整数据集来计算梯度并更新参数， 以及一次处理一个训练样本来取得进展。 二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。 而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。 这暗示了两者之间可能有折中方案，这便涉及到小批量随机梯度下降（minibatch gradient descent）。 1.4.1 小批量使用随机梯度下降时，我们每次只处理一个训练样本。 也就是说，每当我们执行$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g_t}$ 其中$$\\mathbf{g_t} = \\partial_w f(\\mathbf{x}_t, \\mathbf{w}). \\tag{1.4.1}$$每次取出一个样本，没有充分利用CPU和GPU的并行计算能力。这对与于大型数据集来说尤其成问题。 当我们使用小批量随机梯度下降时，我们每次处理一个由多个训练样本组成的“小批量”。 也就是说，我们计算$\\mathbf{g_t}$如下：$$\\mathbf{g_t} = \\frac{1}{|\\mathcal{B}t|} \\sum{i \\in \\mathcal{B}_t} \\partial_w f(\\mathbf{x}_i, \\mathbf{w}). \\tag{1.4.2}$$ 由于$\\mathbf{x_t}$和小批量$\\mathcal{B}_t$的所有元素都是从训练集中随机抽出的，因此梯度的期望保持不变。 另一方面，方差显著降低。 由于小批量梯度由正在被平均计算的$b \\in |\\beta_t|$个独立梯度组成，其标准差降低了$b^{-\\frac{1}{2}}$。 这本身就是一件好事，因为这意味着更新与完整的梯度更接近了。 直观来说，这表明选择大型的小批量$\\mathcal{B}_t$将是普遍可行的。 然而，经过一段时间后，与计算代价的线性增长相比，标准差的额外减少是微乎其微的。 在实践中我们选择一个足够大的小批量，它可以提供良好的计算效率同时仍适合GPU的内存。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import numpy as npimport torchfrom torch import nnfrom d2l import torch as d2l#@saved2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat', '76e5be1548fd8222e5074cf0faae75edff8cf93f')#@savedef get_data_ch11(batch_size=10, n=1500): data = np.genfromtxt(d2l.download('airfoil'), dtype=np.float32, delimiter='\\t') data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0)) data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]), batch_size, is_train=True) return data_iter, data.shape[1]-1def sgd(params, states, hyperparams): for p in params: p.data.sub_(hyperparams['lr'] * p.grad) p.grad.data.zero_()#@savedef train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2): # 初始化模型 w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1), requires_grad=True) b = torch.zeros((1), requires_grad=True) net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss # 训练模型 animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35]) n, timer = 0, d2l.Timer() for _ in range(num_epochs): for X, y in data_iter: l = loss(net(X), y).mean() l.backward() trainer_fn([w, b], states, hyperparams) n += X.shape[0] if n % 200 == 0: timer.stop() animator.add(n/X.shape[0]/len(data_iter), (d2l.evaluate_loss(net, data_iter, loss),)) timer.start() print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch') return timer.cumsum(), animator.Y[0]def train_sgd(lr, batch_size, num_epochs=2): data_iter, feature_dim = get_data_ch11(batch_size) return train_ch11( sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)gd_res = train_sgd(1, 1500, 10) loss: 0.243, 0.011 sec/epoch 1sgd_res = train_sgd(0.005, 1) loss: 0.245, 0.036 sec/epoch 1mini1_res = train_sgd(.4, 100) loss: 0.243, 0.001 sec/epoch 1mini2_res = train_sgd(.05, 10) loss: 0.244, 0.005 sec/epoch 12345d2l.set_figsize([6, 3])d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))), 'time (sec)', 'loss', xlim=[1e-2, 10], legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])d2l.plt.gca().set_xscale('log') 1.4.2 小结 随机梯度下降的“统计效率”与大批量一次处理数据的“计算效率”之间存在权衡。小批量随机梯度下降提供了两全其美的答案：计算和统计效率。 在小批量随机梯度下降中，我们处理通过训练数据的随机排列获得的批量数据（即每个观测值只处理一次，但按随机顺序）。 在训练期间降低学习率有助于训练。 一般来说，小批量随机梯度下降比随机梯度下降和梯度下降的速度快，收敛风险较小。","link":"/2024/10/20/deep-learning-optimizer-4/"},{"title":"deep-learning-optimizer-5","text":"随机梯度下降法在每次迭代中，沿着目标函数的负梯度方向更新参数。然而，随机梯度下降法容易陷入局部最优解，且收敛速度慢。本节介绍动量法，它是一种加速梯度下降的优化算法。 1.5. 动量法前面一小节中，我们介绍了随机梯度下降法。随机梯度下降法在每次迭代中，沿着目标函数的负梯度方向更新参数。然而，随机梯度下降法容易陷入局部最优解，且收敛速度慢。本节介绍动量法，它是一种加速梯度下降的优化算法。 1.5.1. 基础本节将探讨更有效的优化算法，尤其是针对实验中常见的某些类型的优化问题。 1.5.1.1. 泄漏平均值上一节中我们讨论了小批量随机梯度下降作为加速计算的手段。 它也有很好的副作用，即平均梯度减小了方差。 小批量随机梯度下降可以通过以下方式计算：$$\\mathbf{g}{t, t-1} = \\partial_w \\frac{1}{\\mathcal{|B_t|}} \\sum{i \\in \\mathcal{B_t}} f(\\mathbf{x_i}, \\mathbf{w_{t-1}}) = \\frac{1}{\\mathcal{|B_t|}} \\sum_{i \\in \\mathcal{B_t}} \\mathcal{h}_{i, t-1}. \\tag{1.5.1}$$ 为了保持记法简单，在这里我们使用$\\mathbf{h}{i, t-1} = \\partial_w f(\\mathbf{x_i}, \\mathbf{w{t-1}})$作为样本$i$的随机梯度下降，使用时间$t-1$时更新的权重$t-1$。 如果我们能够从方差减少的影响中受益，甚至超过小批量上的梯度平均值，那很不错。 完成这项任务的一种选择是用泄漏平均值（leaky average）取代梯度计算：$$\\mathbf{v_t} = \\beta \\mathbf{v_t} + \\mathbf{g_{t, t-1}}$$其中$\\beta \\in (0, 1)$。 这有效地将瞬时梯度替换为多个“过去”梯度的平均值。 $\\mathbf{v}$被称为动量（momentum）， 它累加了过去的梯度。 为了更详细地解释，让我们递归地将$\\mathbf{v_t}$扩展到$$\\mathbf{v_t} = \\beta^2 \\mathbf{v_{t-2}} + \\beta \\mathbf{g_{t-1, t-2}} + \\mathbf{g_{t, t-1}} = \\dots, = \\sum_{\\tau = 0}^{t-1} \\beta^{\\tau} \\mathbf{g_{t-\\tau, t-\\tau - 1}}.$$ 其中，较大的$\\beta$相当于长期平均值，而较小的$\\beta$相对于梯度法只是略有修正。 新的梯度替换不再指向特定实例下降最陡的方向，而是指向过去梯度的加权平均值的方向。 这使我们能够实现对单批量计算平均值的大部分好处，而不产生实际计算其梯度的代价。 上述推理构成了“加速”梯度方法的基础，例如具有动量的梯度。 在优化问题条件不佳的情况下（例如，有些方向的进展比其他方向慢得多，类似狭窄的峡谷），“加速”梯度还额外享受更有效的好处。 此外，它们允许我们对随后的梯度计算平均值，以获得更稳定的下降方向。 诚然，即使是对于无噪声凸问题，加速度这方面也是动量如此起效的关键原因之一。 正如人们所期望的，由于其功效，动量是深度学习及其后优化中一个深入研究的主题。 例如，请参阅文章，观看深入分析和互动动画。 动量是由 (Polyak, 1964)提出的。 (Nesterov, 2018)在凸优化的背景下进行了详细的理论讨论。 长期以来，深度学习的动量一直被认为是有益的。 有关实例的详细信息，请参阅 (Sutskever et al2013)的讨论。 1.5.1.2. 条件不佳的问题为了更好地了解动量法的几何属性，回想我们在前面中使用了$f(\\mathbf{x}) = x_1^2 + 2x_2^2$ ，即中度扭曲的椭球目标。 我们通过向$x_1$方向伸展它来进一步扭曲这个函数$$f(\\mathbf{x}) = 0.1x_1^2 + 2x_2^2.$$ 与之前一样，$f$在$(0, 0)$有最小值， 该函数在$x_1$的方向上非常平坦。 让我们看看在这个新函数上执行梯度下降时会发生什么。 123456789101112%matplotlib inlineimport torchfrom d2l import torch as d2leta = 0.4def f_2d(x1, x2): return 0.1 * x1 ** 2 + 2 * x2 ** 2def gd_2d(x1, x2, s1, s2): return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d)) epoch 20, x1: -0.943467, x2: -0.000073 从构造来看，$x_2$方向的梯度比水平$x_1$方向的梯度大得多，变化也快得多。 因此，我们陷入两难：如果选择较小的学习率，我们会确保不会在$x_2$方向发散，但要承受在$x_1$方向的缓慢收敛。相反，如果学习率较高，我们在$x_1$方向上进展很快，但在$x_2$方向将会发散。 下面的例子说明了即使学习率从$0.4$略微提高到$0.6$，也会发生变化。$x_1$方向上的收敛有所改善，但整体来看解的质量更差了。 12eta = 0.6d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d)) epoch 20, x1: -0.387814, x2: -1673.365109 1.5.1.3. 动量法动量法（momentum）使我们能够解决上面描述的梯度下降问题。 观察上面的优化轨迹，我们可能会直觉到计算过去的平均梯度效果会很好。 毕竟，在方向上，这将聚合非常对齐的梯度，从而增加我们在每一步中覆盖的距离。 相反，在梯度振荡的方向，由于相互抵消了对方的振荡，聚合梯度将减小步长大小。 使用$\\mathbf{v_t}$而不是梯度$\\mathbf{g_t}$可以生成以下更新等式：$$\\mathbf{v_t} \\leftarrow \\beta \\mathbf{v_{t-1}} + \\mathbf{g_{t, t-1}}, \\\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\eta_t \\mathbf{v_t}.$$请注意，对于$\\beta = 0$，我们恢复常规的梯度下降。 在深入研究它的数学属性之前，让我们快速看一下算法在实验中的表现如何。 1234567def momentum_2d(x1, x2, v1, v2): v1 = beta * v1 + 0.2 * x1 v2 = beta * v2 + 4 * x2 return x1 - eta * v1, x2 - eta * v2, v1, v2eta, beta = 0.6, 0.5d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d)) epoch 20, x1: 0.007188, x2: 0.002553 正如所见，尽管学习率与我们以前使用的相同，动量法仍然很好地收敛了。 让我们看看当降低动量参数时会发生什么。 将其减半至$\\beta = 0.25$会导致一条几乎没有收敛的轨迹。 尽管如此，它比没有动量时解将会发散要好得多。 12eta, beta = 0.6, 0.25d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d)) epoch 20, x1: -0.126340, x2: -0.186632 请注意，我们可以将动量法与随机梯度下降，特别是小批量随机梯度下降结合起来。 唯一的变化是，在这种情况下，我们将梯度替换为$\\mathbf{g_{t, t-1}} \\mathbf{g_{t}}$。 为了方便起见，我们在时间$t=0$初始化$\\mathbf{v_0} = 0$。 1.5.2.1. 从零开始实现相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度。 它与梯度以及优化问题的变量具有相同的形状。 在下面的实现中，我们称这些变量为states。 1234567891011def init_momentum_states(feature_dim): v_w = torch.zeros((feature_dim, 1)) v_b = torch.zeros(1) return (v_w, v_b)def sgd_momentum(params, states, hyperparams): for p, v in zip(params, states): with torch.no_grad(): v[:] = hyperparams['momentum'] * v + p.grad p[:] -= hyperparams['lr'] * v p.grad.data.zero_() 1.5.3 小结 动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。 对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。 动量法可以防止在随机梯度下降的优化过程停滞的问题。 动量法的实现非常简单，但它需要我们存储额外的状态向量","link":"/2024/10/20/deep-learning-optimizer-5/"},{"title":"deep-learning-optimizer-6","text":"AdaGrad算法 (Duchi et al., 2011)通过将粗略的计数器$s(i, t)$替换为先前观察所得梯度的平方之和来解决这个问题。 它使用$s(i, t+1) = s(i, t) + (\\partial_i f(\\mathbf{x}))^2$来调整学习率。 这有两个好处：首先，我们不再需要决定梯度何时算足够大。 其次，它会随梯度的大小自动变化。通常对应于较大梯度的坐标会显著缩小，而其他梯度较小的坐标则会得到更平滑的处理。 在实际应用中，它促成了计算广告学及其相关问题中非常有效的优化程序。 但是，它遮盖了AdaGrad固有的一些额外优势，这些优势在预处理环境中很容易被理解。 1.6. AdaGrad算法1.6.1. 稀疏特征和学习率为了获得良好的准确性，我们大多希望在训练的过程中降低学习率，速度通常为$O(t^{-\\frac{1}{2}})$或更低。 关于稀疏特征（即只在偶尔出现的特征）的模型训练，这对自然语言来说很常见。 例如，我们看到“预先条件”这个词比“学习”这个词的可能性要小得多。 但是，它在计算广告学和个性化协同过滤等其他领域也很常见。 只有在这些不常见的特征出现时，与其相关的参数才会得到有意义的更新。 鉴于学习率下降，我们可能最终会面临这样的情况：常见特征的参数相当迅速地收敛到最佳值，而对于不常见的特征，我们仍缺乏足够的观测以确定其最佳值。 换句话说，学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。 解决此问题的一个方法是记录我们看到特定特征的次数，然后将其用作调整学习率。 即我们可以使用大小为$\\eta_i = \\frac{\\eta_0}{\\sqrt{s(i, t) + c}}$的学习率，而不是 $\\eta_i = \\frac{\\eta_0}{\\sqrt{t+ c}}$。 在这里 $s(i, t)$计下了我们截至$t$时观察到功能$i$的次数。 这其实很容易实施且不产生额外损耗。 AdaGrad算法 (Duchi et al., 2011)通过将粗略的计数器$s(i, t)$替换为先前观察所得梯度的平方之和来解决这个问题。 它使用$s(i, t+1) = s(i, t) + (\\partial_i f(\\mathbf{x}))^2$来调整学习率。 这有两个好处：首先，我们不再需要决定梯度何时算足够大。 其次，它会随梯度的大小自动变化。通常对应于较大梯度的坐标会显著缩小，而其他梯度较小的坐标则会得到更平滑的处理。 在实际应用中，它促成了计算广告学及其相关问题中非常有效的优化程序。 但是，它遮盖了AdaGrad固有的一些额外优势，这些优势在预处理环境中很容易被理解。 1.6.2. 算法让我们接着上面正式开始讨论。 我们使用变量$\\mathbf{s_t}$来累加过去的梯度方差，如下所示：$$\\mathbf{g_t} = \\partial_w l(y_t, f(\\mathbf{x_t, w})), \\$$$$\\mathbf{s_t} = \\mathbf{s_{t-1}} + \\mathbf{g_t}^2,$$$$\\mathbf{w_t} = \\mathbf{w_{t-1}} - \\frac{\\eta}{\\sqrt{\\mathbf{s_t} + \\epsilon}} \\cdot \\mathbf{g_t}.$$与之前一样，$\\eta$是学习率，$\\epsilon$是一个为维持数值稳定性而添加的常数，用来确保我们不会除以$0$。最后，我们初始化$\\mathbf{s_0} = 0$。 就像在动量法中我们需要跟踪一个辅助变量一样，在AdaGrad算法中，我们允许每个坐标有单独的学习率。 与SGD算法相比，这并没有明显增加AdaGrad的计算代价，因为主要计算用在$\\partial_w l(y_t, f(\\mathbf{x_t, w}))$及其导数。 眼下让我们先看看它在二次凸问题中的表现如何。 我们仍然以同一函数为例：$$f(\\mathbf{x}) = 0.1x_1^2 + 2x_2^2.$$我们将使用与之前相同的学习率来实现AdaGrad算法，即$\\eta = 0.4$。可以看到，自变量的迭代轨迹较平滑。 但由于$\\mathbf{s_t}$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。 12345678910111213141516171819%matplotlib inlineimport mathimport torchfrom d2l import torch as d2ldef adagrad_2d(x1, x2, s1, s2): eps = 1e-6 g1, g2 = 0.2 * x1, 4 * x2 s1 += g1 ** 2 s2 += g2 ** 2 x1 -= eta / math.sqrt(s1 + eps) * g1 x2 -= eta / math.sqrt(s2 + eps) * g2 return x1, x2, s1, s2def f_2d(x1, x2): return 0.1 * x1 ** 2 + 2 * x2 ** 2eta = 0.4d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d)) epoch 20, x1: -2.382563, x2: -0.158591 将学习率提高到$2$，可以看到更好的表现。 这已经表明，即使在无噪声的情况下，学习率的降低可能相当剧烈，我们需要确保参数能够适当地收敛。 12eta = 2d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d)) epoch 20, x1: -0.002295, x2: -0.000000 1.6.3. 从零开始实现同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。 123456789101112def init_adagrad_states(feature_dim): s_w = torch.zeros((feature_dim, 1)) s_b = torch.zeros(1) return (s_w, s_b)def adagrad(params, states, hyperparams): eps = 1e-6 for p, s in zip(params, states): with torch.no_grad(): s[:] += torch.square(p.grad) p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps) p.grad.data.zero_() 1.6.4. 小结 AdaGrad算法会在单个坐标层面动态降低学习率。 AdaGrad算法利用梯度的大小作为调整进度速率的手段：用较小的学习率来补偿带有较大梯度的坐标 如果优化问题的结构相当不均匀，AdaGrad算法可以帮助缓解扭曲。 在深度学习问题上，AdaGrad算法有时在降低学习率方面可能过于剧烈。","link":"/2024/10/20/deep-learning-optimizer-6/"},{"title":"deep-learning-optimizer-7","text":"(Tieleman and Hinton, 2012)建议以RMSProp算法作为将速率调度与坐标自适应学习率分离的简单修复方法。 1.7. RMSProp算法11.7节中的关键问题之一，是学习率按预定时间表$O(t^{-\\frac{1}{2}})$显著降低。 虽然这通常适用于凸问题，但对于深度学习中遇到的非凸问题，可能并不理想。 但是，作为一个预处理Adagrad算法按坐标顺序的适应性是非常可取的。 (Tieleman and Hinton, 2012)建议以RMSProp算法作为将速率调度与坐标自适应学习率分离的简单修复方法。 问题在于，Adagrad算法将梯度$\\mathbf{g_t}$的平方累加成状态矢量$\\mathbf{s_t} = \\mathbf{s_{t-1} + \\mathbf{g_t}^2}$。 因此，由于缺乏规范化，没有约束力，$\\mathbf{s_t}$持续增长，几乎上是在算法收敛时呈线性递增。 另一种方法是按动量法中的方式使用泄漏平均值，即$\\mathbf{s_t} \\leftarrow \\gamma \\mathbf{s_{t-1} + (1 - \\gamma)\\mathbf{g_t}^2}$，其中参数$\\gamma &gt; 0$。 保持所有其它部分不变就产生了RMSProp算法。 1.7.1. 算法让我们详细写出这些方程式。$$\\mathbf{s_t} \\leftarrow \\gamma \\mathbf{s_{t-1}} + (1 - \\gamma) \\mathbf{g_t}^2, \\\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\frac{\\eta}{\\sqrt{\\mathbf{s_t}+ \\epsilon} } \\odot \\mathbf{g_t}.$$ 常数 $\\epsilon$通常设置为$10^{-6}$，以确保我们不会因除以零或步长过大而受到影响。 鉴于这种扩展，我们现在可以自由控制学习率$\\eta$，而不考虑基于每个坐标应用的缩放。 1.7.2. 从零开始实现和之前一样，我们使用二次函数$f(\\mathbf{x}) = 0.1x_1^2 + 2x_2^2$来观察RMSProp算法的轨迹。 回想在上一节，当我们使用学习率为0.4的Adagrad算法时，变量在算法的后期阶段移动非常缓慢，因为学习率衰减太快。 RMSProp算法中不会发生这种情况，因为$\\eta$是单独控制的 1234567891011121314151617import mathimport torchfrom d2l import torch as d2ldef rmsprop_2d(x1, x2, s1, s2): g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6 s1 = gamma * s1 + (1 - gamma) * g1 ** 2 s2 = gamma * s2 + (1 - gamma) * g2 ** 2 x1 -= eta / math.sqrt(s1 + eps) * g1 x2 -= eta / math.sqrt(s2 + eps) * g2 return x1, x2, s1, s2def f_2d(x1, x2): return 0.1 * x1 ** 2 + 2 * x2 ** 2eta, gamma = 0.4, 0.9d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d)) 1.7.3. 小结 RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。 RMSProp算法与动量法都使用泄漏平均值。但是，RMSProp算法使用该技术来调整按系数顺序的预处理器。 在实验中，学习率需要由实验者调度。 系数$\\gamma$决定了在调整每坐标比例时历史记录的时长。","link":"/2024/10/20/deep-learning-optimizer-7/"},{"title":"deep-learning-optimizer-8","text":"Adam算法 (Kingma and Ba, 2014)将所有这些技术汇总到一个高效的学习算法中。 不出预料，作为深度学习中使用的更强大和有效的优化算法之一，它非常受欢迎。但是它并非没有问题，尤其是 (Reddi et al., 2019)表明，有时Adam算法可能由于方差控制不良而发散。 在完善工作中， (Zaheer et al., 2018)给Adam算法提供了一个称为Yogi的热补丁来解决这些问题。 1.8. Adam算法我们已经学习了许多有效优化的技术。 在本节讨论之前，我们先详细回顾一下这些技术： 随机梯度下降在解决优化问题时比梯度下降更有效 在一个小批量中使用更大的观测值集，可以通过向量化提供额外效率。这是高效的多机、多GPU和整体并行处理的关键。 动量法中我们添加了一种机制，用于汇总过去梯度的历史以加速收敛。 AdaGrad算法通过对每个坐标缩放来实现高效计算的预处理器。 Adam算法 (Kingma and Ba, 2014)将所有这些技术汇总到一个高效的学习算法中。 不出预料，作为深度学习中使用的更强大和有效的优化算法之一，它非常受欢迎。但是它并非没有问题，尤其是 (Reddi et al., 2019)表明，有时Adam算法可能由于方差控制不良而发散。 在完善工作中， (Zaheer et al., 2018)给Adam算法提供了一个称为Yogi的热补丁来解决这些问题。 下面我们了解一下Adam算法。 1.8.1. 算法Adam算法的关键组成部分之一是：它使用指数加权移动平均值来估算梯度的动量和二次矩，即它使用状态变量 $$\\mathbf{v_t} \\leftarrow \\beta_1 \\mathbf{v_{t-1}} + (1-\\beta_1) \\mathbf{g_t}, \\\\mathbf{s_t} \\leftarrow \\beta_2 \\mathbf{s_{t-1}} + (1 - \\beta_2) \\mathbf{g_t}^2.$$ 这里$\\beta_1$和$beta_2$是非负加权参数。 常将它们设置为$\\beta_1 = 0.9$和$\\beta_2=0.999$。 也就是说，方差估计的移动远远慢于动量估计的移动。 注意，如果我们初始化$\\mathbf{v_0}=\\mathbf{s_0} = 0$，就会获得一个相当大的初始偏差。 我们可以通过使用$\\sum_{t=0}^{t} \\beta^i = \\frac{1 - \\beta^t}{1- \\beta}$来解决这个问题。 有了正确的估计，我们现在可以写出更新方程。 首先，我们以非常类似于RMSProp算法的方式重新缩放梯度以获得 $$\\mathbf{g_t}’ = \\frac{\\eta \\mathbf{v_t}}{\\sqrt{\\mathbf{s_t}} + \\epsilon}$$ 与RMSProp不同，我们的更新使用动量$\\mathbf{v_t}$而不是梯度本身, 最后，我们简单更新：$$\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\mathbf{g_t}’.$$回顾Adam算法，它的设计灵感很清楚： 首先，动量和规模在状态变量中清晰可见， 它们相当独特的定义使我们移除偏项（这可以通过稍微不同的初始化和更新条件来修正）。 其次，RMSProp算法中两项的组合都非常简单。 最后，明确的学习率使我们能够控制步长来解决收敛问题。 1.8.2. 实现123import torchimport torch.cudatorch.cuda.is_available() True 12345678910111213141516171819202122import torchfrom d2l import torch as d2ldef init_adam_states(feature_dim): v_w, v_b = torch.zeros((feature_dim, 1)), torch.zeros(1) s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1) return ((v_w, s_w), (v_b, s_b))def adam(params, states, hyperparams): beta1, beta2, eps = 0.9, 0.999, 1e-6 for p, (v, s) in zip(params, states): with torch.no_grad(): v[:] = beta1 * v + (1 - beta1) * p.grad s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad) v_bias_corr = v / (1 - beta1 ** hyperparams['t']) s_bias_corr = s / (1 - beta2 ** hyperparams['t']) p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps) p.grad.data.zero_() hyperparams['t'] += 1 1.8.3. 小结 Adam算法将许多优化算法的功能结合到了相当强大的更新规则中。 Adam算法在RMSProp算法基础上创建的","link":"/2024/10/20/deep-learning-optimizer-8/"},{"title":"深度学习中的优化算法","text":"深度学习中有许多优化算法，在使用过程中通常我们只需要调用相应的api即可，但了解其原理有助于我们更好地使用它们。优化算法对于深度学习非常重要。一方面，训练复杂的深度学习模型可能需要数小时、几天甚至数周。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原则及其超参数的作用将使我们能够以有针对性的方式调整超参数，以提高深度学习模型的性能。 This is the rest of the content that will only appear when you click “Read more.” 1. 深度学习中的优化算法深度学习中有许多优化算法，在使用过程中通常我们只需要调用相应的api即可，但了解其原理有助于我们更好地使用它们。优化算法对于深度学习非常重要。一方面，训练复杂的深度学习模型可能需要数小时、几天甚至数周。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原则及其超参数的作用将使我们能够以有针对性的方式调整超参数，以提高深度学习模型的性能。 在本次讨论中，我们将深入探讨常见的深度学习优化算法。首先，深度学习中出现的几乎所有优化问题都是非凸的。尽管如此，在凸问题背景下设计和分析算法是非常有启发性的。正是出于这个原因，开始将会介绍凸优化的入门，以及凸目标函数上非常简单的随机梯度下降算法的证明。然后，我们将从梯度下降算法开始，然后讨论随机梯度下降算法、小批量梯度下降算法、动量梯度下降算法、AdaGrad算法、RMSProp算法、Adam算法等。我们将通过具体的代码示例来演示这些算法的工作原理，并讨论它们的优缺点。 1.1. 优化和深度学习在深度学习中，我们通常使用梯度下降算法来优化模型参数。梯度下降算法是一种迭代优化算法，它通过计算目标函数的梯度来更新模型参数，以最小化目标函数。在深度学习中，目标函数通常是模型的损失函数，用于衡量模型预测结果与真实标签之间的差异。对于深度学习问题，我们通常会先定义损失函数。一旦我们有了损失函数，我们就可以使用优化算法来尝试最小化损失。按照传统惯例，大多数优化算法都关注的是最小化。如果我们需要最大化目标，那么有一个简单的解决方案：在目标函数前加负号即可。 1.1.1. 优化目标尽管优化提供了一种最大限度地减少深度学习损失函数的方法，但本质上，优化和深度学习的目标是根本不同的。前者主要关注的是最小化目标，后者则关注在给定有限数据量的情况下寻找合适的模型。例如，训练误差和泛化误差通常不同：由于优化算法的目标函数通常是基于训练数据集的损失函数，因此优化的目标是减少训练误差。但是，深度学习的目标是减少泛化误差。为了实现后者，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合。 1.1.2. 深度学习中的优化挑战深度学习优化存在许多挑战。其中最令人烦恼的是局部最小值、鞍点和梯度消失。 1.1.2.1. 局部最小值对于任何目标函数$f(x)$, 如果在$x$处对应的$f(x)$值小于在$x$附近任意其他点的$f(x)$值，那么$f(x)$可能是局部最小值。如果$f(x)$在$x$处的值是整个域中目标函数的最小值，那么$f(x)$是全局最小值。例如，给定函数$$f(x)= x \\times \\cos(\\pi x) \\quad\\text{for} -1.0 \\leq x \\leq 2.0$$我们可以近似该函数的局部最小值和全局最小值。 123456%matplotlib inlineimport numpy as npimport torchfrom mpl_toolkits import mplot3dfrom d2l import torch as d2l 123456789101112def f(x): return x * torch.cos(np.pi * x)def annotate(text, xy, xytext): #@save d2l.plt.gca().annotate(text, xy=xy, xytext=xytext, arrowprops=dict(arrowstyle='-&gt;'))x = torch.arange(-1.0, 2.0, 0.01)d2l.plot(x, [f(x), ], 'x', 'f(x)')annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))annotate('global minimum', (1.1, -0.95), (0.6, 0.8)) 深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数局部最优，而不是全局最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一。在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中跳出。 1.1.2.2 鞍点除了局部最小值之外，鞍点是梯度消失的另一个原因。鞍点（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数$f(x)=x^3$。它的一阶和二阶导数在$x = 0$时消失。这时优化可能会停止，尽管它不是最小值。 123x = torch.arange(-2.0, 2.0, 0.01)d2l.plot(x, [x**3], 'x', 'f(x)')annotate('saddle point', (0, -0.2), (-0.52, -5.0)) 如下例所示，较高维度的鞍点甚至更加隐蔽。考虑这个函数$f(x, y)=x^2-y^2$。它的鞍点为$(0, 0)$。这是关于$y$的最大值，也是关于$x$的最小值。此外，它看起来像个马鞍，这就是鞍点的名字由来。 12345678910111213x, y = torch.meshgrid( torch.linspace(-1.0, 1.0, 101), torch.linspace(-1.0, 1.0, 101))z = x**2 - y**2ax = d2l.plt.figure().add_subplot(111, projection='3d')ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})ax.plot([0], [0], [0], 'rx')ticks = [-1, 0, 1]d2l.plt.xticks(ticks)d2l.plt.yticks(ticks)ax.set_zticks(ticks)d2l.plt.xlabel('x')d2l.plt.ylabel('y') /home/ippc-zq/miniconda3/envs/yolov5/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] 1.1.2.3. 梯度消失可能遇到的最隐蔽问题是梯度消失。例如，假设我们想最小化函数$f(x)=\\tanh(x)$，然后我们恰好从 $f(x)$ 开始。正如我们所看到的那样，$f$的梯度接近零。更具体地说，$f’(x)=1-\\tanh^2(x)$因此是$f’(4)=0.0013$。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。 123x = torch.arange(-2.0, 5.0, 0.01)d2l.plot(x, [torch.tanh(x)], 'x', 'f(x)')annotate('vanishing gradient', (4, 1), (2, 0.0)) 正如我们所看到的那样，深度学习的优化充满挑战。幸运的是，有一系列强大的算法表现良好，即使对于初学者也很容易使用。此外，没有必要找到最优解。局部最优解或其近似解仍然非常有用。 1.1.3. 小结 优化问题可能有许多局部最小值。 一个问题可能有很多的鞍点，因为问题通常不是凸的。 梯度消失可能会导致优化停滞，重参数化通常会有所帮助。对参数进行良好的初始化也可能是有益的。","link":"/2024/10/14/test-asset/"},{"title":"Essay-Writing-Tips","text":"在本学期的论文写作课程中，学到了许多写作的方法和技巧。这些方法不仅提高了写作效率，也对学术论文的结构和内容有更深入的理解. 本文中的内容主要参考了闵帆老师的博客 论文写作总结1. 尽量使用Latex书写学术论文LaTeX是一种用于排版的系统，广泛用于学术、技术和科学出版。它是基于TeX排版引擎的，提供了一种高层次的文档编写方式，使用户可以专注于内容的写作，而不是排版的细节。相较于word，在处理复杂排版时更加灵活。 1.1. 基本特点 高质量排版: LaTeX能够生成高质量的文档，尤其是在数学公式的排版上，具有无与伦比的精确度和美观性 自动化功能: LaTeX可以自动处理参考文献、目录、图表、公式编号等，减少人工干预的需要，提高效率。 可拓展性强: 通过宏包，可以扩展LaTeX的功能，支持不同领域的需求，如图形绘制、符号使用、算法排版等。 1.2. 使用和安装Latex有许多发行版，这里我选择比较常见的texlive2024安装在Ubuntu24.04的操作系统上。 1.2.1. 方式一： apt 安装(一步到位) 更新包管理列表:1sudo apt update &amp;&amp; sudo apt upgrade 安装Tex Live:1sudo apt install texlive 一步到位， 只需等待命令行安装完成的提示即可，不用配值任何的环境变量 1.2.2. 方式二: 手动安装(复杂一点)首先下载texlive的源文件到我们的主机上， 这里我选择在TeX Live官网下载对应的源码文件。 下载压缩包1wget https://mirror.ctan.org/systems/texlive/tlnet/install-tl-unx.tar.gz 解压文件1tar -xvf install-tl-unx.tar.gz 进入解压目录执行1sudo ./install-tl-* 最后将texlive添加到相关变量中(下面第一条最重要)123export PATH=/usr/local/texlive/2024/bin/x86_64-linux:$PATHexport INFOPATH=/usr/local/texlive/2024/texmf-dist/doc/info:$INFOPATHexport MANPATH=/usr/local/texlive/2024/texmf-dist/doc/man:$MANPATH 检查安装完成。执行下面命令，如果输出Tex的版本号等相关配置信息，则证明安装成功。1tex --version 1.2.3. 使用方法相关的使用方法，我整理了一些网站和博客，可供参考： Overleaf Learn LaTeX in 30 Minutes CSDN: LaTeX学习笔记：一文入门LaTeX(超详细) 30分钟快速上手LaTex Latex 使用中的一些奇怪的小技巧 先就这些吧，后续如果遇到好的帖子，再附上。 2. 论文写作中慎用的单词和短语在学术论文写作中需谨慎使用的词汇与短语，一些常见的错误用法会对论文的质量的产生影响。 避免句首使用And: 在句子开头使用And是禁忌，因其通常不增加实际意义。 合理位置放置Only: Only应靠后放置，以体现更广泛的研究能力，而非局限性。 禁止使用Easy: 如果一个问题很简单，就没有必要进行学术研究，因此应避免使用这个词。 Solve的使用限制: 应避免随意使用solve，除非处理的是已解决的问题或具有明确的理论支持。 谨慎使用缩写: 在正式学术论文中，缩写如 have’t 和 don’t 是不合适的，应使用完整形式如 have not 和 do not 3. 符号系统与数学表达式 不要对式子、符号进行额外的、特殊的处理,包括强行增加空格、花括号等. 使用LaTeX处理数学表达式相比Word具有显著优势，避免了格式错误。 变量通常用斜体表示，而常数和运算符则用正体表示，这一点在LaTeX中很容易实现。 数学表达式的排版时应包括必要的标点符号，以确保表达完整且专业。 如果要给一个式子$y=f(x)$编号, 就可以用123\\begin{equation}\\label{equation: sample} y = f(x)\\end{equation} 如果需要与内容匹配的括号, 可以用 \\left(, \\left[ 之类. 如$$\\sum_{j=1}^{2} \\sum_{i=l+1}^{l+u} v_i^{(j)} \\left( L \\left( \\bar{g}_j(\\mathbf{x_i}), g_j(\\mathbf{x_i}) \\right) - \\lambda^{(j)} \\right). \\tag{1}$$ 可以在arxiv找一篇已经发表论文的.tex文件来学习数学表达式. 4. 中肯而闪亮的论文题目论文题目对于学术文章至关重要，它不仅需要吸引读者的注意力，还要易于理解和检索。有效的标题通常具有挑战性，使用常见术语，并控制在适当的字数范围内。此外，标题应准确反映论文的主要贡献，避免使用“基于”等词汇。 论文题目必须具有吸引力，以确保审稿人和读者的兴趣。 题目应使用该领域内常用的术语，以便读者能够轻松理解。 流行的术语有助于提高论文的可检索性和引用率。(xxx is all your need!) 理想的标题长度控制在40-60个字母之间，短标题可能代表更高的创新性。 如果论文的主要贡献是算法，标题应包含该算法的名称缩写，以突出其价值。 5. 摘要摘要通常由三个部分构成：已有工作的评述、本文工作的描述及实验结果。通过具体的句型示例，指导作者如何有效地表达重要问题及其解决方案，继而提升摘要质量，以满足学术期刊的要求。 摘要的功能是提炼出论文的重要内容，分为三个部分：已有工作、本文工作与实验结果。 问题的重要性需要清晰表达，以便凸显研究的相关性。 需描述已有工作的现状，从而展现研究是建立在前人工作的基础之上。 明确指出已有工作的局限性，以展示研究的必要性。 本文的工作应详细阐述，包括算法的具体描述和主要贡献。 提供实验设置，确保数据的来源和数量得以清晰呈现。 实验结果应准确展示研究的效率和准确性，强调研究的实际意义。 6. 关键词关键词是用于检索论文的一种重要的方式. 虽然现在人们可以进行全文检索, 但关键词仍然很常用. 关键词常被看作摘要的一种补充。 关键词一般由1-3个单词组成,总数为3-5个,需按字母顺序排列。 有些期刊支持两类关键词。 index term: 由期刊提供, 只能从投稿网站给定的列表选择 keywords: 作者自己按需写。 7. 引言在计算机英文论文中，引言（Introduction）需要讲述完整故事，其重要性类似于电影剧本，对整个论文的评判有重要影响。好的引言能让审稿人在读完后基本判断论文质量，读者也常据此决定是否引用。若引言不佳，可能导致论文被否定。 写作方式 采用八股式扩展摘要： 最常见的方式是按照与摘要相同节奏，将摘要每一句扩展成引言中的一段，每段首句（主题句）可由摘要相应句子稍作修改而来，主题句为断言，其余句子用于支撑。 控制段落长度： 每段建议有 5 - 10 句，50 - 150 个单词。相邻过短段落应合并，过长段落应拆分或精简，避免段落过短显得零碎或过长使读者疲劳。 常见风格 “开局一张图”: 计算机领域（特别是顶会）流行在引言中配图，图的内容可包括核心技术、算法框架、运行实例、效果对比等，若图质量高可加分，否则不如不配。部分期刊还要求提供 Graphical abstract 达到类似效果。 列出论文贡献: 有些期刊和会议要求在实验陈述段落之后单独列出论文贡献，数量以两三点为宜，需与前文方法步骤等有所区别。 引言的层次要求 单词级：不能有拼写、语法错误。 句子级: 表达应简洁、得体、有力量，用尽量简短句子描述意思，可重复使用少数几种句式。 段落级: 内容丰富，从不同角度分析同一事情，常用角度如 WWH（what、why、how）。 章节级： 条理清晰、节奏明快，参考摘要 10 句及相应节奏。同时强调了引言应体现论文的创新之处。 8. 文献综述绝大多数的参考文献应在文献综述中引用. 每篇论文都应有文献综述： 表示对前人工作的尊重, 我们是站在巨人的肩头上当读者不清楚某些技 可以在不同的地方描述：引言(简略)， 第 2 节专门的 Related work(详细) 文献需要进行分门别类的介绍 文献综述的注意事项 避免简单罗列与无意义综述 合理评述相关工作 确保参考文献在正文中引用 不要一次性引用太多文献 9. 算法伪代码算法伪代码是论文的核心之一，它能够清晰地展示算法的逻辑和流程，帮助读者理解论文所提出算法的具体实现方式。 算法伪代码的书写规范 输入输出说明： 明确写出算法的输入和输出内容，使读者清楚算法所需的数据 方法名处理： 方法名根据情况决定是否书写，如果该方法会被其他方法调用则必须写。 注释添加： 为主要步骤添加注释，解释每一步的功能和目的，增强伪代码的可读性。 长度控制： 伪代码长度建议控制在 15 - 30 行，以保持简洁性和清晰度 复杂度分析： 一般需要进行时间、空间复杂度分析 数学式子运用： 使用数学式子或引用已有数学式子来准确表达算法中的计算和逻辑关系。 10. 实验实验部分是重头戏， 对于很多机器学习论文, 实验部分占据了论文一半的篇幅. 数据集要点: 数据集的数量与规模：数据集应数量充足、覆盖领域广，一般 12 - 20 个公开数据集为宜，特殊情况可采用人造或采样方式；数据集规模大些为佳，如结构化数据个样本和 100 个 实验目的呈现: 运用自问自答模式，实验前提出如算法准确性相关问题，实验后依据结果解答，即便可能不被部分审稿人认可，但必要时可调整。 内部比较内容: 通过实验展示参数变化对性能的影响，注意选择关键参数；比较主要方案与其变种，以确定最优方案 外部比较要素: 与经典、基准和先进算法对比，根据数据量和展示需求选择合适方式（柱状图、表格、折线图），重要对比置于最后，要有充分文字分析，且要分析自身方案劣势。 11. 结论作为人们的阅读习惯, 最后一部分总是要看的. 通常审稿人和读者都会逐句阅读本部分. 结论部分的特点与要求 受关注度高: 作为论文的最后一部分，通常会被审稿人和读者逐句阅读 篇幅简短: 一般 5 句话左右即可，避免过于冗长。 内容具体且区别于摘要 进一步工作部分的意义与撰写建议 重要性：读者往往非常关注这部分内容，因为它为后续研究提供了思路。从研究价值角度看，为后续研究打开一扇门比完全解决一个问题更具意义。 撰写方式：可以列出 3 至 5 条进一步工作的方向，这部分内容不算在 Conclusion 的字数内。 12. 参考文献参考文献部分隐藏的错误数量超过你的想像. Latex与bib文件优势 Latex提供的bib文件在参考文献管理方面具有很大优势，大大减轻了作者负担，是其相较于Winword的重要优势之一，且每篇参考文献只需编写7 - 8行代码即可管理相关信息。 参考文献的编写注意事项 避免直接拷贝网上bibitem: 应使用正确模板并填入文献内容，以此避免多数问题 格式规范 - 等号列对齐: 如同编写程序需保持良好习惯一样，在编写bib文件时，要保证格式正确 参考文献命名规则 名称要有意义且具唯一性 名字格式统一 题目大小写处理: 意题目的大小写，可使用花括号进行必要的强制设置，但避免过度控制. 格式检查与调整 为确保格式正确，应检查Latex生成的pdf文件，并且偶尔需要根据投稿期刊要求对bibitem进行修改。 通过这次论文写作课程的学习，我学习到了论文写作的的一些技巧，希望能再以后论文写作时，把这些技巧用上并熟练!!!","link":"/2024/10/31/Essay-Writing-Tips/"},{"title":"自监督学习(Self-Supervesed Learning)","text":"Self-supervised learning(SSL)允许模型使用给定的没有标注的数据集自动的去学习一个“good”的表征空间，Specifically, 如果我们的数据集是一堆图片，self-supervised learning 去学习一个模型来生成 “good” representation vector. 自监督学习(Self-Supervised Learning)什么是Self-Supervised learning?现代机器学习(ML)需要大量带有便签的训练数据，但是通常获得大量人工标注的数据（human-labeled）是非常有挑战性的或者是非常昂贵的. 所以能否有一种方式能让我们要求机器去自动学习一个模型，这个模型能生成(generate good visual representations)好的视觉表征而不需要一个带有标签的数据集吗？ 自监督学习就是一种很好的方式！ Self-supervised learning(SSL)允许模型使用给定的没有标注的数据集自动的去学习一个“good”的表征空间，Specifically, 如果我们的数据集是一堆图片，self-supervised learning 去学习一个模型来生成 “good” representation vector 对于图片来说。 之所以SSL这种方法在最近非常火，是因为这种在一种数据集学习到的模型通常也能在其他数据集上表现的很好，而这里的其他数据集往往是模型没有训练过或者见过的！ 什么是一个好的表征(good representation)“好的”表示向量需要捕捉图像与数据集其余部分相关的重要特征。这意味着，数据集中表示语义相似实体的图像应该具有相似的表示向量，而数据集中的不同图像应该具有不同的表示向量。例如，苹果的两幅图像应该具有相似的表示向量，而苹果的图像和香蕉的图像应该具有不同的表示向量。 Contrastive Learning: SimCLRSimCLR 引入了一种新的架构，它使用 contrastive learning 来学习良好的视觉表示。对比学习旨在为相似的图像学习相似的表示，为不同的图像学习不同的表示。这个简单的想法让我们无需使用任何标签就能训练出一个出奇好的模型。 具体来说，对于数据集中的每个图像，SimCLR 都会生成该图像的两个不同增强视图，称为positive pair。然后，鼓励模型为这对图像生成类似的表示向量。请参阅下面的架构说明（论文中的图 2）。 12from IPython.display import ImageImage('images/simclr_fig2.png', width=500) 给定一个图像 x，SimCLR 使用两种不同的数据增强方案 t 和 t’ 来生成正图像对 $\\tilde{x}_i$ 和 **$\\tilde{x}_j$**。$f$ 是一个基本的编码器网络，它从增强的数据样本中提取表示向量，分别产生 $h_i$ 和 **$h_j$**。最后，一个小的神经网络投影头 $g$ 将表示向量映射到应用对比损失的空间。对比损失的目标是最大化最终向量 $z_i = g(h_i)$ 和 $z_j = g(h_j)$ 之间的一致性。我们稍后将更详细地讨论对比损失，您将可以实现它。训练完成后，我们丢弃投影头 $g$，仅使用 $f$ 和表示 $h$ 来执行下游任务，例如分类。 数据增强(Data Augmentation) 随机调整大小并裁剪为 32x32。(Random resize and crop to 32x32) 以0.5的概率水平翻转图像(Horizontally flip the image) 0.8的概率(probability)应用color jitter(包含饱和度，对比度等) 0.2的概率将image convert to grayscale(灰度图) 12345678910111213141516171819from PIL import Imageimport torchvisionfrom torchvision.datasets import CIFAR10def test_data_augmentation(correct_output=None): train_transform = compute_train_transform(seed=2147483647) trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=2, shuffle=False, num_workers=2) dataiter = iter(trainloader) images, labels = next(dataiter) img = torchvision.utils.make_grid(images) img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() output = images print(&quot;Maximum error in data augmentation: %g&quot;%rel_error(output.numpy(), correct_output.numpy()))test_data_augmentation(answers['data_augmentation']) Base Encoder and Projection Head接下来的步骤是将基础编码器和投影头应用于增强样本 $\\tilde{x}_i$ 和 **$\\tilde{x}_j$**。 基础编码器 $f$ 提取增强样本的表示向量。SimCLR 论文发现使用更深更宽的模型可以提高性能，因此选择 ResNet 作为基础编码器。基础编码器的输出是表示向量 $h_i = f(\\tilde{x}_i$) 和 **$h_j = f(\\tilde{x}_j$)**。 投影头 $g$ 是一个小型神经网络，它将表示向量 $h_i$ 和 $h_j$ 映射到应用对比损失的空间。论文发现使用非线性投影头可以提高其前一层的表示质量。具体来说，他们使用具有一个隐藏层的 MLP 作为投影头 $g$。然后根据输出 $z_i = g(h_i$) 和 $z_j = g(h_j$) 计算对比损失。 SimCLR: Contrastive Loss一个小批量的 $N$ 个训练图像产生总共 $2N$ 个数据增强示例。对于每个positive pair $(i, j)$ 的增强示例，对比损失函数旨在最大化向量 $z_i$ 和 $z_j$ 的一致性。具体来说，损失是正则的温标缩放交叉熵损失（temperature-scaled cross entropy），旨在最大化 $z_i$ 和 $z_j$ 相对于批量中所有其他增强示例的一致性：$$l ; (i,j) = -\\log \\frac{\\exp(;\\text{sim}(z_i, z_j) ;/ ;\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{k \\neq i} \\exp(; \\text{sim}(z_i, z_k) ; / ; \\tau)}$$ 其中 $\\mathbb{1} \\in {0, 1}$ 是一个指示函数，如果 $k\\neq i$ 则输出 $1$，否则输出 $0$。$\\tau$ 是一个温度参数，它决定了指数增加的速度。 $sim(z_i, z_j) = \\frac{z_i \\cdot z_j}{|| z_i || || z_j ||}$ 是向量 $z_i$ 和 $z_j$ 之间的（标准化）点积。$z_i$ 和 $z_j$ 之间的相似度越高，点积越大，分子就越大。分母通过对 $z_i$ 和批次中的所有其他增强示例 $k$ 求和来标准化该值。归一化值的范围是$(0,1)$，其中接近 $1$ 的高分对应于正对$(i，j)$之间的高相似度以及 $i$ 与批次中的其他增强示例 $k$ 之间的低相似度。然后，负对数将范围$(0,1)$映射到损失值$(\\inf,0)$。 总损失是针对批次中的所有正对$(i，j)$计算的。让 $z = [z_1，z_2，…，z_{2N}]$ 包括批次中的所有增强示例，其中 $z_{1}…z_{N}$ 是左分支的输出，$z_{N+1}…z_{2N}$ 是右分支的输出。因此，对于 $\\forall k \\in [1，N]$，正对为$(z_{k}，z_{k + N})$。 那么总损失$L$为： $$L = \\frac{1}{2N} \\sum_{k=1}^{N} [; l(k,; k+N) + l(k+N,; k) ;]$$ 关键代码实现如下: 123456789101112131415def sim(z_i, z_j): &quot;&quot;&quot;Normalized dot product between two vectors. Inputs: - z_i: 1xD tensor. - z_j: 1xD tensor. Returns: - A scalar value that is the normalized dot product between z_i and z_j. &quot;&quot;&quot; norm_dot_product = None norm_dot_product = torch.sum(z_i * z_j) / ( (torch.sqrt(torch.sum(z_i ** 2)) * torch.sqr(torch.sum(z_j ** 2)) )) return norm_dot_product 12345678910111213141516171819202122232425262728293031323334353637def simclr_loss_vectorized(out_left, out_right, tau, device='cuda'): &quot;&quot;&quot;Compute the contrastive loss L over a batch (vectorized version). No loops are allowed. Inputs and output are the same as in simclr_loss_naive. &quot;&quot;&quot; N = out_left.shape[0] # Concatenate out_left and out_right into a 2*N x D tensor. out = torch.cat([out_left, out_right], dim=0) # [2*N, D] # Compute similarity matrix between all pairs of augmented examples in the batch. sim_matrix = compute_sim_matrix(out) # [2*N, 2*N] exponential = None exponential =torch.exp(sim_matrix / tau).to(device) # (2N, 2N) # This binary mask zeros out terms where k=i. mask = (torch.ones_like(exponential, device=device) - torch.eye(2 * N, device=device)).to(device).bool() # apply the binary mask. exponential = exponential.masked_select(mask).view(2 * N, -1) # [2*N, 2*N-1] denom = None denom = torch.sum(exponential, dim=1, keepdim=True, device=device) sim_pos_pairs = sim_positive_pairs(out_left, out_right).repeat(2, 1).to(device) # (N, 1) numerator = None numerator = torch.exp(sim_pos_pairs / tau) loss = torch.sum(torch.log(denom) - torch.log(numerator)) loss /= (2 * N) return loss Finetune a Linear Layer for Classification最后从SimCLR 模型中移除投影头，并添加一个线性层来微调一个简单的分类任务。线性层之前的所有层都被冻结，只有最后一个线性层中的权重被训练。","link":"/2024/11/24/Contrastive_Learning/"},{"title":"Self-Attention, Transformers","text":"Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin. Self-Attention, TransformerAttention ExplorationMulti-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin. Recall that attention can be viewed as an operation on a $query \\in \\mathbb{R^d}$, a set of value vectors $ {v_1,\\dots ,v_n }, v_i \\in \\mathbb{R^d}$, and a set of key vectors $\\left{k_1, \\dots ,k_n \\right}, k_i \\in \\mathbb{R^d},$ specified as follows:$$c = \\sum_{i=1}^{n} v_i \\alpha_i \\tag{1}$$$$\\alpha_i = \\frac{\\exp(k_i^{\\top}q)}{\\sum_{j=1}^{n} \\exp(k_i^{\\top} q)} \\tag{2}$$with $alpha = { \\alpha_1, \\dots ,\\alpha_n } $ termed the “attention weights”. Observe that the output $c \\in \\mathbb{R^d} $ is an average over the value vectors weighted with respect to $\\alpha$ One advantage of attention is that it’s particularly easy to“copy” a value vector to the output c. In this problem, we’ll motivate why this is the case. The distribution $\\alpha$ is typically relatively “diffuse”; the probability mass is spread out between many different $\\alpha_i$ However, this is not always the case. Describe (in one sentence)under what conditions the categorical distribution $\\alpha$ puts almost all of its weight on some $\\alpha_j$, where $j \\in {1, \\dots, n }(i.e. ; \\alpha_j \\gg \\sum_{i \\neq j} \\alpha_i)$ A: when the query is much similar with $k_i, i \\in {i,\\dots, n } $ and query is not similar with all other $k_j, j\\neq i$. It’s mean only $\\alpha_i$ is enough big, and other $\\alpha_j$ is small. Under the conditions you gave in describe the output c. output $c$ is more likely with $v_i$. Because $c$ is a average weight sum of value vectors. An average of two. Instead of focusing on just one vector $v_j$ , a Transformer modelmight want to incorporate information from $multiple$ source vectors.Consider the case where we instead want to incorporate information from two vectors $v_a$ and $v_b$,with corresponding key vectors $k_a$ and $k_b$. Assume that (1) all key vectors are orthogonal, so $k^\\top k_j $ for all $ i \\neq j$ and all key vectors have norm 1. Find an expression for a query vector$q$ such that $c \\approx \\frac{1}{2}(v_a + v_b)$, and justify your answer. Orthogonality: $k_a^\\top k_b=0$ meaning $k_a$ and $k_b$ are orthogonal Normalization: $||k_a|| = ||k_b|| = 1$ meaning $k_a^\\top k_a = 1$ The attention weights for $v_a$ and $v_b$ are determined by the query $q$: $$ \\alpha_a = \\frac{\\exp(k_a^\\top q)}{\\sum_{j=1}^{n} \\exp(k_j^\\top q)}, ; \\alpha_b = \\frac{\\exp(k_b^\\top q)}{\\sum_{j=1}^{n} \\exp(k_j^\\top q)} $$ to make sure $\\alpha_a$ and $\\alpha_b$ are same. A natural choice is the normalized sum of $k_a$ and $k_b$ $$ q = \\frac{k_a + k_b}{||k_a + k_b||} $$Justification: Denominator: only keep $\\frac{2}{||k_a + k_b||} $ molecular: $\\alpha_i = 0 $ for each $i \\neq a,b $ so $\\alpha_a = \\frac{1}{||k_a + k_b||}$, $\\alpha_b$ is the same value.","link":"/2024/11/27/Self-Attention-Transformers/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"},{"name":"essay writing","slug":"essay-writing","link":"/tags/essay-writing/"}],"categories":[],"pages":[]}