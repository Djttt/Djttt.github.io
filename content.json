{"posts":[{"title":"deep-learning-optimizer-2","text":"尽管梯度下降（gradient descent）很少直接用于深度学习， 但了解它是理解下一节随机梯度下降算法的关键。 例如，由于学习率过大，优化问题可能会发散，这种现象早已在梯度下降中出现。 同样地，预处理是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。 让我们从简单的一维梯度下降开始介绍。 1.2. 梯度下降尽管梯度下降（gradient descent）很少直接用于深度学习， 但了解它是理解下一节随机梯度下降算法的关键。 例如，由于学习率过大，优化问题可能会发散，这种现象早已在梯度下降中出现。 同样地，预处理是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。 让我们从简单的一维梯度下降开始介绍。 1.2.1. 一维梯度下降为什么梯度下降算法可以优化目标函数？ 一维中的梯度下降给我们很好的启发。 考虑一类连续可微实值函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$， 利用泰勒展开，我们可以得到$$f(x + \\epsilon) = f(x) + \\epsilon f’(x) + O(\\epsilon^2). \\tag{1.2.1}$$即在一阶近似中，$f(x + \\epsilon)$ 可通过$x$处的函数值$f(x)$和一阶导数$f’(x)$得出。我们可以假设在负梯度方向上移动的$\\epsilon$会减少$f$的值。为简单起见，选择固定步长$\\eta$, 然后取$\\epsilon=\\eta f’(x)$。将其代入泰勒展开式我们可以得到$$f(x - \\eta f’(x)) = f(x) - \\eta f’^2(x) + O(\\eta^2 f’^2(x)) \\tag{1.2.2}$$如果其导数$f’(x) \\neq 0$ 没有消失， 我们就能继续展开，这是因为$\\eta f’^2(x) &gt; 0$。 此外，我们总是可以令$\\eta$小到足以使高阶项变得不相关。 因此，$$f(x - \\eta f’(x)) \\lessapprox f(x). \\tag{1.2.3}$$这意味着，如果我们使用$$x \\leftarrow x - \\eta f’(x) \\tag{1.2.4}$$来迭代$x$，函数$f(x)$的值可能会下降。 因此，在梯度下降中，我们首先选择初始值$x$和常数$\\eta &gt; 0$， 然后使用它们连续迭代$x$，直到停止条件达成。 例如，当梯度$|f’(x)|$的幅度足够小或迭代次数达到某个值时。 下面我们来展示如何实现梯度下降。为了简单起见，我们选用目标函数$f(x)=x^2$。 尽管我们知道$x=0$时$f(x)$能取得最小值， 但我们仍然使用这个简单的函数来观察$x$的变化。 12345%matplotlib inlineimport numpy as npimport torchfrom mpl_toolkits import mplot3dfrom d2l import torch as d2l 12345def f(x): return x**2def f_grad(x): return 2 * x 接下来，我们使用$x=10$作为初始值，并假设$\\eta=0.2$。 使用梯度下降法迭代$x$共10次，我们可以看到，$x$的值最终将接近最优解。 1234567891011def gd(eta, f_grad): x = 10 # init x position results = [x] for i in range(10): x -= eta * f_grad(x) results.append(float(x)) print(f'epoch 10, x:{x:f}') return resultsresults = gd(0.2, f_grad) epoch 10, x:0.060466 对进行$x$优化的过程可以绘制如下。 12345678def show_trace(results, f): n = max(abs(min(results)), abs(max(results))) f_line = torch.arange(-n, n, 0.01) d2l.set_figsize() d2l.plot([f_line, results], [[f(x) for x in f_line], [ f(x) for x in results]], 'x', 'f(x)', fmts=['-', '-o'])show_trace(results, f) 1.2.1.1. 学习率学习率（learning rate）决定目标函数能否收敛到局部最小值，以及何时收敛到最小值。 学习率$\\eta$可由算法设计者设置。 请注意，如果使用的学习率太小，将导致$x$的更新非常缓慢，需要更多的迭代。 例如，考虑同一优化问题中$\\eta=0.05$的进度。 如下所示，尽管经过了10个步骤，我们仍然离最优解很远。 1show_trace(gd(0.05, f_grad), f) epoch 10, x:3.486784 相反，如果我们使用过高的学习率，$|\\eta f’(x)|$对于一阶泰勒展开式可能太大。 也就是说，(1.2.2)中的$O(\\eta^2 f’^2(x))$可能变得显著了。 在这种情况下，$x$的迭代不能保证降低$f(x)$的值。 例如，当学习率为$\\eta = 1.1$时，$x$超出了最优解$x = 0$并逐渐发散。 1show_trace(gd(1.1, f_grad), f) epoch 10, x:61.917364 1.2.1.2. 局部最小值为了演示非凸函数的梯度下降，考虑函数$f(x) = x \\cdot \\cos(cx)$，其中$c$为某常数。 这个函数有无穷多个局部最小值。 根据我们选择的学习率，我们最终可能只会得到许多解的一个。 下面的例子说明了（不切实际的）高学习率如何导致较差的局部最小值。 123456789c = torch.tensor(0.15 * np.pi)def f(x): # 目标函数 return x * torch.cos(c * x)def f_grad(x): # 目标函数的梯度 return torch.cos(c * x) - c * x * torch.sin(c * x)show_trace(gd(2, f_grad), f) epoch 10, x:-1.528166 1.2.2. 多元梯度下降现在我们对单变量的情况有了更好的理解，让我们考虑一下$\\mathbf{x} = [x_1, x_2, \\dots, x_d]^T$的情况。 即目标函数$f: \\mathbb{R^d} \\rightarrow \\mathbb{R}$将向量映射成标量。 相应地，它的梯度也是多元的，它是一个由$d$个偏导数组成的向量：$$\\nabla f(\\mathbf{x}) = \\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d} \\right]^T. \\tag{1.2.5}$$梯度中的每个偏导数元素$\\partial f(\\mathbf{x}) / \\partial x_i$代表了当输入$x_i$时$f$在$\\mathbf{x}$处的变化率。 和先前单变量的情况一样，我们可以对多变量函数使用相应的泰勒近似来思考。 具体来说，$$f(\\mathbf{x} + \\epsilon) = f(\\mathbf{x}) + \\epsilon^T \\nabla f(\\mathbf{x}) + O(||\\epsilon||^2) \\tag{1.2.6}$$换句话说，在$\\epsilon$的二阶项中， 最陡下降的方向由负梯度$-\\nabla f(\\mathbf{x})$得出。 选择合适的学习率$\\eta &gt; 0$来生成典型的梯度下降算法：$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x}). \\tag{1.2.7}$$这个算法在实践中的表现如何呢？ 我们构造一个目标函数$f(\\mathbf{x}) = x_1^2 + 2x_2^2$， 并有二维向量$\\mathbf{x} = [x_1, x_2]^T$作为输入， 标量作为输出。 梯度由$\\nabla f(\\mathbf{x}) = [2x_1, 4x_2]^T$给出。 我们将从初始位置$[-5, -2]$通过梯度下降观察$\\mathbf{x}$的轨迹。 1234567891011121314151617181920212223def train_2d(trainer, steps=20, f_grad=None): #@save &quot;&quot;&quot;用定制的训练机优化2D目标函数&quot;&quot;&quot; # s1和s2是稍后将使用的内部状态变量 x1, x2, s1, s2 = -5, -2, 0, 0 results = [(x1, x2)] for i in range(steps): if f_grad: x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad) else: x1, x2, s1, s2 = trainer(x1, x2, s1, s2) results.append((x1, x2)) print(f'epoch {i + 1}, x1: {float(x1):f}, x2: {float(x2):f}') return resultsdef show_trace_2d(f, results): #@save &quot;&quot;&quot;显示优化过程中2D变量的轨迹&quot;&quot;&quot; d2l.set_figsize() d2l.plt.plot(*zip(*results), '-o', color='#ff7f0e') x1, x2 = torch.meshgrid(torch.arange(-5.5, 1.0, 0.1), torch.arange(-3.0, 1.0, 0.1), indexing='ij') d2l.plt.contour(x1, x2, f(x1, x2), colors='#1f77b4') d2l.plt.xlabel('x1') d2l.plt.ylabel('x2') 接下来，我们观察学习率$\\eta = 0.1$时优化变量$\\mathbf{x}$的轨迹。 可以看到，经过20步之后，$\\mathbf{x}$的值接近其位于$[0, 0]$的最小值。 虽然进展相当顺利，但相当缓慢。 123456789101112def f_2d(x1, x2): # 目标函数 return x1 ** 2 + 2 * x2 ** 2def f_2d_grad(x1, x2): # 目标函数的梯度 return (2 * x1, 4 * x2)def gd_2d(x1, x2, s1, s2, f_grad): g1, g2 = f_grad(x1, x2) return (x1 - eta * g1, x2 - eta * g2, 0, 0)eta = 0.1show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad)) epoch 20, x1: -0.057646, x2: -0.000073 1.2.3. 小结 学习率的大小很重要：学习率太大会使模型发散，学习率太小会没有进展。 梯度下降会可能陷入局部极小值，而得不到全局最小值。 在高维模型中，调整学习率是很复杂的。","link":"/2024/10/20/deep-learning-optimizer-2/"},{"title":"deep-learning-optimizer-3","text":"在前面中，我们一直在训练过程中使用随机梯度下降，但没有解释它为什么起作用。为了解释这一点，我们刚在 1.3中描述了梯度下降的基本原则。本节继续更详细地说明随机梯度下（stochastic gradient descent）。 1.3. 随机梯度下降在前面中，我们一直在训练过程中使用随机梯度下降，但没有解释它为什么起作用。为了解释这一点，我们刚在 1.3中描述了梯度下降的基本原则。本节继续更详细地说明随机梯度下（stochastic gradient descent）。 1234%matplotlib inlineimport mathimport torchfrom d2l import torch as d2l 1.3.1. 随机梯度更新在深度学习中，目标函数通常是训练数据集中每个样本的损失函数的平均值。给定$n$个样本的训练数据集，我们假设$f_i(\\mathbf{x})$是关于索引$i$的训练样本的损失函数，其中$\\mathbf{x}$是参数向量。然后我们得到目标函数$$f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^{n} f_i(\\mathbf{x}). \\tag{1.3.1}$$$\\mathbf{x}$的目标函数的梯度计算为$$\\nabla f(\\mathbf{x}) = \\frac{1}{n} \\sum_{i=1}^{n} \\nabla f_i(\\mathbf{x}) \\tag{1.3.2}$$ 如果使用梯度下降法，则每个自变量迭代的计算代价为$O(n)$，它随$n$线性增长。因此，当训练数据集较大时，每次迭代的梯度下降计算代价将较高。 随机梯度下降（SGD）可降低每次迭代时的计算代价。在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引$i$，其中$i \\in {i,\\dots, n}$，并计算梯度 $\\nabla f_i(\\mathbf{x})$以更新$\\mathbf{x}$：$$\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}), \\tag{1.3.3}$$其中 $\\eta$ 是学习率。我们可以看到，每次迭代的计算代价从梯度下降的 $O(n)$降至常数O(1)。此外，我们要强调，随机梯度 $\\nabla f_i(\\mathbf{x})$ 是对完整梯度$\\nabla f(\\mathbf{x})$的无偏估计，因为$$\\mathbb{E}i \\nabla f_i(\\mathbf{x}) = \\frac{1}{n} \\sum{i=1}^{n} \\nabla f_i(\\mathbf{x}) = \\nabla f(\\mathbf{x}). \\tag{1.3.4}$$这意味着，平均而言，随机梯度是对梯度的良好估计。 现在，我们将把它与梯度下降进行比较，方法是向梯度添加均值为0、方差为1的随机噪声，以模拟随机梯度下降。 1234567891011121314151617181920def f(x1, x2): # 目标函数 return x1 ** 2 + 2 * x2 ** 2def f_grad(x1, x2): # 目标函数的梯度 return 2 * x1, 4 * x2def sgd(x1, x2, s1, s2, f_grad): g1, g2 = f_grad(x1, x2) # 模拟有噪声的梯度 g1 += torch.normal(0.0, 1, (1,)).item() g2 += torch.normal(0.0, 1, (1,)).item() eta_t = eta * lr() return (x1 - eta_t * g1, x2 - eta_t * g2, 0, 0)def constant_lr(): return 1eta = 0.1lr = constant_lr # 常数学习速度d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=50, f_grad=f_grad)) epoch 50, x1: -0.002198, x2: 0.017078 正如我们所看到的，随机梯度下降中变量的轨迹比我们在 11.3节中观察到的梯度下降中观察到的轨迹嘈杂得多。这是由于梯度的随机性质。也就是说，即使我们接近最小值，我们仍然受到通过$\\eta \\nabla f_i(\\mathbf{x})$的瞬间梯度所注入的不确定性的影响。即使经过50次迭代，质量仍然不那么好。更糟糕的是，经过额外的步骤，它不会得到改善。 1.3.2. 小结 对于凸问题，我们可以证明，对于广泛的学习率选择，随机梯度下降将收敛到最优解。 如果学习率太小或太大，就会出现问题。实际上，通常只有经过多次实验后才能找到合适的学习率。 当训练数据集中有更多样本时，计算梯度下降的每次迭代的代价更高，因此在这些情况下，首选随机梯度下降。 随机梯度下降的最优性保证在非凸情况下一般不可用","link":"/2024/10/20/deep-learning-optimizer-3/"},{"title":"deep-learning-optimizer-4","text":"基于梯度的学习方法中遇到了两个极端情况： 使用完整数据集来计算梯度并更新参数， 以及一次处理一个训练样本来取得进展。 二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。 而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。 这暗示了两者之间可能有折中方案，这便涉及到小批量随机梯度下降（minibatch gradient descent）。 1.4. 小批量随机梯度下降到目前为止，我们在基于梯度的学习方法中遇到了两个极端情况： 使用完整数据集来计算梯度并更新参数， 以及一次处理一个训练样本来取得进展。 二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。 而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。 这暗示了两者之间可能有折中方案，这便涉及到小批量随机梯度下降（minibatch gradient descent）。 1.4.1 小批量使用随机梯度下降时，我们每次只处理一个训练样本。 也就是说，每当我们执行$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g_t}$ 其中$$\\mathbf{g_t} = \\partial_w f(\\mathbf{x}_t, \\mathbf{w}). \\tag{1.4.1}$$每次取出一个样本，没有充分利用CPU和GPU的并行计算能力。这对与于大型数据集来说尤其成问题。 当我们使用小批量随机梯度下降时，我们每次处理一个由多个训练样本组成的“小批量”。 也就是说，我们计算$\\mathbf{g_t}$如下：$$\\mathbf{g_t} = \\frac{1}{|\\mathcal{B}t|} \\sum{i \\in \\mathcal{B}_t} \\partial_w f(\\mathbf{x}_i, \\mathbf{w}). \\tag{1.4.2}$$ 由于$\\mathbf{x_t}$和小批量$\\mathcal{B}_t$的所有元素都是从训练集中随机抽出的，因此梯度的期望保持不变。 另一方面，方差显著降低。 由于小批量梯度由正在被平均计算的$b \\in |\\beta_t|$个独立梯度组成，其标准差降低了$b^{-\\frac{1}{2}}$。 这本身就是一件好事，因为这意味着更新与完整的梯度更接近了。 直观来说，这表明选择大型的小批量$\\mathcal{B}_t$将是普遍可行的。 然而，经过一段时间后，与计算代价的线性增长相比，标准差的额外减少是微乎其微的。 在实践中我们选择一个足够大的小批量，它可以提供良好的计算效率同时仍适合GPU的内存。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import numpy as npimport torchfrom torch import nnfrom d2l import torch as d2l#@saved2l.DATA_HUB['airfoil'] = (d2l.DATA_URL + 'airfoil_self_noise.dat', '76e5be1548fd8222e5074cf0faae75edff8cf93f')#@savedef get_data_ch11(batch_size=10, n=1500): data = np.genfromtxt(d2l.download('airfoil'), dtype=np.float32, delimiter='\\t') data = torch.from_numpy((data - data.mean(axis=0)) / data.std(axis=0)) data_iter = d2l.load_array((data[:n, :-1], data[:n, -1]), batch_size, is_train=True) return data_iter, data.shape[1]-1def sgd(params, states, hyperparams): for p in params: p.data.sub_(hyperparams['lr'] * p.grad) p.grad.data.zero_()#@savedef train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2): # 初始化模型 w = torch.normal(mean=0.0, std=0.01, size=(feature_dim, 1), requires_grad=True) b = torch.zeros((1), requires_grad=True) net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss # 训练模型 animator = d2l.Animator(xlabel='epoch', ylabel='loss', xlim=[0, num_epochs], ylim=[0.22, 0.35]) n, timer = 0, d2l.Timer() for _ in range(num_epochs): for X, y in data_iter: l = loss(net(X), y).mean() l.backward() trainer_fn([w, b], states, hyperparams) n += X.shape[0] if n % 200 == 0: timer.stop() animator.add(n/X.shape[0]/len(data_iter), (d2l.evaluate_loss(net, data_iter, loss),)) timer.start() print(f'loss: {animator.Y[0][-1]:.3f}, {timer.avg():.3f} sec/epoch') return timer.cumsum(), animator.Y[0]def train_sgd(lr, batch_size, num_epochs=2): data_iter, feature_dim = get_data_ch11(batch_size) return train_ch11( sgd, None, {'lr': lr}, data_iter, feature_dim, num_epochs)gd_res = train_sgd(1, 1500, 10) loss: 0.243, 0.011 sec/epoch 1sgd_res = train_sgd(0.005, 1) loss: 0.245, 0.036 sec/epoch 1mini1_res = train_sgd(.4, 100) loss: 0.243, 0.001 sec/epoch 1mini2_res = train_sgd(.05, 10) loss: 0.244, 0.005 sec/epoch 12345d2l.set_figsize([6, 3])d2l.plot(*list(map(list, zip(gd_res, sgd_res, mini1_res, mini2_res))), 'time (sec)', 'loss', xlim=[1e-2, 10], legend=['gd', 'sgd', 'batch size=100', 'batch size=10'])d2l.plt.gca().set_xscale('log') 1.4.2 小结 随机梯度下降的“统计效率”与大批量一次处理数据的“计算效率”之间存在权衡。小批量随机梯度下降提供了两全其美的答案：计算和统计效率。 在小批量随机梯度下降中，我们处理通过训练数据的随机排列获得的批量数据（即每个观测值只处理一次，但按随机顺序）。 在训练期间降低学习率有助于训练。 一般来说，小批量随机梯度下降比随机梯度下降和梯度下降的速度快，收敛风险较小。","link":"/2024/10/20/deep-learning-optimizer-4/"},{"title":"deep-learning-optimizer-5","text":"随机梯度下降法在每次迭代中，沿着目标函数的负梯度方向更新参数。然而，随机梯度下降法容易陷入局部最优解，且收敛速度慢。本节介绍动量法，它是一种加速梯度下降的优化算法。 1.5. 动量法前面一小节中，我们介绍了随机梯度下降法。随机梯度下降法在每次迭代中，沿着目标函数的负梯度方向更新参数。然而，随机梯度下降法容易陷入局部最优解，且收敛速度慢。本节介绍动量法，它是一种加速梯度下降的优化算法。 1.5.1. 基础本节将探讨更有效的优化算法，尤其是针对实验中常见的某些类型的优化问题。 1.5.1.1. 泄漏平均值上一节中我们讨论了小批量随机梯度下降作为加速计算的手段。 它也有很好的副作用，即平均梯度减小了方差。 小批量随机梯度下降可以通过以下方式计算：$$\\mathbf{g}{t, t-1} = \\partial_w \\frac{1}{\\mathcal{|B_t|}} \\sum{i \\in \\mathcal{B_t}} f(\\mathbf{x_i}, \\mathbf{w_{t-1}}) = \\frac{1}{\\mathcal{|B_t|}} \\sum_{i \\in \\mathcal{B_t}} \\mathcal{h}_{i, t-1}. \\tag{1.5.1}$$ 为了保持记法简单，在这里我们使用$\\mathbf{h}{i, t-1} = \\partial_w f(\\mathbf{x_i}, \\mathbf{w{t-1}})$作为样本$i$的随机梯度下降，使用时间$t-1$时更新的权重$t-1$。 如果我们能够从方差减少的影响中受益，甚至超过小批量上的梯度平均值，那很不错。 完成这项任务的一种选择是用泄漏平均值（leaky average）取代梯度计算：$$\\mathbf{v_t} = \\beta \\mathbf{v_t} + \\mathbf{g_{t, t-1}}$$其中$\\beta \\in (0, 1)$。 这有效地将瞬时梯度替换为多个“过去”梯度的平均值。 $\\mathbf{v}$被称为动量（momentum）， 它累加了过去的梯度。 为了更详细地解释，让我们递归地将$\\mathbf{v_t}$扩展到$$\\mathbf{v_t} = \\beta^2 \\mathbf{v_{t-2}} + \\beta \\mathbf{g_{t-1, t-2}} + \\mathbf{g_{t, t-1}} = \\dots, = \\sum_{\\tau = 0}^{t-1} \\beta^{\\tau} \\mathbf{g_{t-\\tau, t-\\tau - 1}}.$$ 其中，较大的$\\beta$相当于长期平均值，而较小的$\\beta$相对于梯度法只是略有修正。 新的梯度替换不再指向特定实例下降最陡的方向，而是指向过去梯度的加权平均值的方向。 这使我们能够实现对单批量计算平均值的大部分好处，而不产生实际计算其梯度的代价。 上述推理构成了“加速”梯度方法的基础，例如具有动量的梯度。 在优化问题条件不佳的情况下（例如，有些方向的进展比其他方向慢得多，类似狭窄的峡谷），“加速”梯度还额外享受更有效的好处。 此外，它们允许我们对随后的梯度计算平均值，以获得更稳定的下降方向。 诚然，即使是对于无噪声凸问题，加速度这方面也是动量如此起效的关键原因之一。 正如人们所期望的，由于其功效，动量是深度学习及其后优化中一个深入研究的主题。 例如，请参阅文章，观看深入分析和互动动画。 动量是由 (Polyak, 1964)提出的。 (Nesterov, 2018)在凸优化的背景下进行了详细的理论讨论。 长期以来，深度学习的动量一直被认为是有益的。 有关实例的详细信息，请参阅 (Sutskever et al2013)的讨论。 1.5.1.2. 条件不佳的问题为了更好地了解动量法的几何属性，回想我们在前面中使用了$f(\\mathbf{x}) = x_1^2 + 2x_2^2$ ，即中度扭曲的椭球目标。 我们通过向$x_1$方向伸展它来进一步扭曲这个函数$$f(\\mathbf{x}) = 0.1x_1^2 + 2x_2^2.$$ 与之前一样，$f$在$(0, 0)$有最小值， 该函数在$x_1$的方向上非常平坦。 让我们看看在这个新函数上执行梯度下降时会发生什么。 123456789101112%matplotlib inlineimport torchfrom d2l import torch as d2leta = 0.4def f_2d(x1, x2): return 0.1 * x1 ** 2 + 2 * x2 ** 2def gd_2d(x1, x2, s1, s2): return (x1 - eta * 0.2 * x1, x2 - eta * 4 * x2, 0, 0)d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d)) epoch 20, x1: -0.943467, x2: -0.000073 从构造来看，$x_2$方向的梯度比水平$x_1$方向的梯度大得多，变化也快得多。 因此，我们陷入两难：如果选择较小的学习率，我们会确保不会在$x_2$方向发散，但要承受在$x_1$方向的缓慢收敛。相反，如果学习率较高，我们在$x_1$方向上进展很快，但在$x_2$方向将会发散。 下面的例子说明了即使学习率从$0.4$略微提高到$0.6$，也会发生变化。$x_1$方向上的收敛有所改善，但整体来看解的质量更差了。 12eta = 0.6d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d)) epoch 20, x1: -0.387814, x2: -1673.365109 1.5.1.3. 动量法动量法（momentum）使我们能够解决上面描述的梯度下降问题。 观察上面的优化轨迹，我们可能会直觉到计算过去的平均梯度效果会很好。 毕竟，在方向上，这将聚合非常对齐的梯度，从而增加我们在每一步中覆盖的距离。 相反，在梯度振荡的方向，由于相互抵消了对方的振荡，聚合梯度将减小步长大小。 使用$\\mathbf{v_t}$而不是梯度$\\mathbf{g_t}$可以生成以下更新等式：$$\\mathbf{v_t} \\leftarrow \\beta \\mathbf{v_{t-1}} + \\mathbf{g_{t, t-1}}, \\\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\eta_t \\mathbf{v_t}.$$请注意，对于$\\beta = 0$，我们恢复常规的梯度下降。 在深入研究它的数学属性之前，让我们快速看一下算法在实验中的表现如何。 1234567def momentum_2d(x1, x2, v1, v2): v1 = beta * v1 + 0.2 * x1 v2 = beta * v2 + 4 * x2 return x1 - eta * v1, x2 - eta * v2, v1, v2eta, beta = 0.6, 0.5d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d)) epoch 20, x1: 0.007188, x2: 0.002553 正如所见，尽管学习率与我们以前使用的相同，动量法仍然很好地收敛了。 让我们看看当降低动量参数时会发生什么。 将其减半至$\\beta = 0.25$会导致一条几乎没有收敛的轨迹。 尽管如此，它比没有动量时解将会发散要好得多。 12eta, beta = 0.6, 0.25d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d)) epoch 20, x1: -0.126340, x2: -0.186632 请注意，我们可以将动量法与随机梯度下降，特别是小批量随机梯度下降结合起来。 唯一的变化是，在这种情况下，我们将梯度替换为$\\mathbf{g_{t, t-1}} \\mathbf{g_{t}}$。 为了方便起见，我们在时间$t=0$初始化$\\mathbf{v_0} = 0$。 1.5.2.1. 从零开始实现相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度。 它与梯度以及优化问题的变量具有相同的形状。 在下面的实现中，我们称这些变量为states。 1234567891011def init_momentum_states(feature_dim): v_w = torch.zeros((feature_dim, 1)) v_b = torch.zeros(1) return (v_w, v_b)def sgd_momentum(params, states, hyperparams): for p, v in zip(params, states): with torch.no_grad(): v[:] = hyperparams['momentum'] * v + p.grad p[:] -= hyperparams['lr'] * v p.grad.data.zero_() 1.5.3 小结 动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。 对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。 动量法可以防止在随机梯度下降的优化过程停滞的问题。 动量法的实现非常简单，但它需要我们存储额外的状态向量","link":"/2024/10/20/deep-learning-optimizer-5/"},{"title":"deep-learning-optimizer-6","text":"AdaGrad算法 (Duchi et al., 2011)通过将粗略的计数器$s(i, t)$替换为先前观察所得梯度的平方之和来解决这个问题。 它使用$s(i, t+1) = s(i, t) + (\\partial_i f(\\mathbf{x}))^2$来调整学习率。 这有两个好处：首先，我们不再需要决定梯度何时算足够大。 其次，它会随梯度的大小自动变化。通常对应于较大梯度的坐标会显著缩小，而其他梯度较小的坐标则会得到更平滑的处理。 在实际应用中，它促成了计算广告学及其相关问题中非常有效的优化程序。 但是，它遮盖了AdaGrad固有的一些额外优势，这些优势在预处理环境中很容易被理解。 1.6. AdaGrad算法1.6.1. 稀疏特征和学习率为了获得良好的准确性，我们大多希望在训练的过程中降低学习率，速度通常为$O(t^{-\\frac{1}{2}})$或更低。 关于稀疏特征（即只在偶尔出现的特征）的模型训练，这对自然语言来说很常见。 例如，我们看到“预先条件”这个词比“学习”这个词的可能性要小得多。 但是，它在计算广告学和个性化协同过滤等其他领域也很常见。 只有在这些不常见的特征出现时，与其相关的参数才会得到有意义的更新。 鉴于学习率下降，我们可能最终会面临这样的情况：常见特征的参数相当迅速地收敛到最佳值，而对于不常见的特征，我们仍缺乏足够的观测以确定其最佳值。 换句话说，学习率要么对于常见特征而言降低太慢，要么对于不常见特征而言降低太快。 解决此问题的一个方法是记录我们看到特定特征的次数，然后将其用作调整学习率。 即我们可以使用大小为$\\eta_i = \\frac{\\eta_0}{\\sqrt{s(i, t) + c}}$的学习率，而不是 $\\eta_i = \\frac{\\eta_0}{\\sqrt{t+ c}}$。 在这里 $s(i, t)$计下了我们截至$t$时观察到功能$i$的次数。 这其实很容易实施且不产生额外损耗。 AdaGrad算法 (Duchi et al., 2011)通过将粗略的计数器$s(i, t)$替换为先前观察所得梯度的平方之和来解决这个问题。 它使用$s(i, t+1) = s(i, t) + (\\partial_i f(\\mathbf{x}))^2$来调整学习率。 这有两个好处：首先，我们不再需要决定梯度何时算足够大。 其次，它会随梯度的大小自动变化。通常对应于较大梯度的坐标会显著缩小，而其他梯度较小的坐标则会得到更平滑的处理。 在实际应用中，它促成了计算广告学及其相关问题中非常有效的优化程序。 但是，它遮盖了AdaGrad固有的一些额外优势，这些优势在预处理环境中很容易被理解。 1.6.2. 算法让我们接着上面正式开始讨论。 我们使用变量$\\mathbf{s_t}$来累加过去的梯度方差，如下所示：$$\\mathbf{g_t} = \\partial_w l(y_t, f(\\mathbf{x_t, w})), \\$$$$\\mathbf{s_t} = \\mathbf{s_{t-1}} + \\mathbf{g_t}^2,$$$$\\mathbf{w_t} = \\mathbf{w_{t-1}} - \\frac{\\eta}{\\sqrt{\\mathbf{s_t} + \\epsilon}} \\cdot \\mathbf{g_t}.$$与之前一样，$\\eta$是学习率，$\\epsilon$是一个为维持数值稳定性而添加的常数，用来确保我们不会除以$0$。最后，我们初始化$\\mathbf{s_0} = 0$。 就像在动量法中我们需要跟踪一个辅助变量一样，在AdaGrad算法中，我们允许每个坐标有单独的学习率。 与SGD算法相比，这并没有明显增加AdaGrad的计算代价，因为主要计算用在$\\partial_w l(y_t, f(\\mathbf{x_t, w}))$及其导数。 眼下让我们先看看它在二次凸问题中的表现如何。 我们仍然以同一函数为例：$$f(\\mathbf{x}) = 0.1x_1^2 + 2x_2^2.$$我们将使用与之前相同的学习率来实现AdaGrad算法，即$\\eta = 0.4$。可以看到，自变量的迭代轨迹较平滑。 但由于$\\mathbf{s_t}$的累加效果使学习率不断衰减，自变量在迭代后期的移动幅度较小。 12345678910111213141516171819%matplotlib inlineimport mathimport torchfrom d2l import torch as d2ldef adagrad_2d(x1, x2, s1, s2): eps = 1e-6 g1, g2 = 0.2 * x1, 4 * x2 s1 += g1 ** 2 s2 += g2 ** 2 x1 -= eta / math.sqrt(s1 + eps) * g1 x2 -= eta / math.sqrt(s2 + eps) * g2 return x1, x2, s1, s2def f_2d(x1, x2): return 0.1 * x1 ** 2 + 2 * x2 ** 2eta = 0.4d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d)) epoch 20, x1: -2.382563, x2: -0.158591 将学习率提高到$2$，可以看到更好的表现。 这已经表明，即使在无噪声的情况下，学习率的降低可能相当剧烈，我们需要确保参数能够适当地收敛。 12eta = 2d2l.show_trace_2d(f_2d, d2l.train_2d(adagrad_2d)) epoch 20, x1: -0.002295, x2: -0.000000 1.6.3. 从零开始实现同动量法一样，AdaGrad算法需要对每个自变量维护同它一样形状的状态变量。 123456789101112def init_adagrad_states(feature_dim): s_w = torch.zeros((feature_dim, 1)) s_b = torch.zeros(1) return (s_w, s_b)def adagrad(params, states, hyperparams): eps = 1e-6 for p, s in zip(params, states): with torch.no_grad(): s[:] += torch.square(p.grad) p[:] -= hyperparams['lr'] * p.grad / torch.sqrt(s + eps) p.grad.data.zero_() 1.6.4. 小结 AdaGrad算法会在单个坐标层面动态降低学习率。 AdaGrad算法利用梯度的大小作为调整进度速率的手段：用较小的学习率来补偿带有较大梯度的坐标 如果优化问题的结构相当不均匀，AdaGrad算法可以帮助缓解扭曲。 在深度学习问题上，AdaGrad算法有时在降低学习率方面可能过于剧烈。","link":"/2024/10/20/deep-learning-optimizer-6/"},{"title":"deep-learning-optimizer-7","text":"(Tieleman and Hinton, 2012)建议以RMSProp算法作为将速率调度与坐标自适应学习率分离的简单修复方法。 1.7. RMSProp算法11.7节中的关键问题之一，是学习率按预定时间表$O(t^{-\\frac{1}{2}})$显著降低。 虽然这通常适用于凸问题，但对于深度学习中遇到的非凸问题，可能并不理想。 但是，作为一个预处理Adagrad算法按坐标顺序的适应性是非常可取的。 (Tieleman and Hinton, 2012)建议以RMSProp算法作为将速率调度与坐标自适应学习率分离的简单修复方法。 问题在于，Adagrad算法将梯度$\\mathbf{g_t}$的平方累加成状态矢量$\\mathbf{s_t} = \\mathbf{s_{t-1} + \\mathbf{g_t}^2}$。 因此，由于缺乏规范化，没有约束力，$\\mathbf{s_t}$持续增长，几乎上是在算法收敛时呈线性递增。 另一种方法是按动量法中的方式使用泄漏平均值，即$\\mathbf{s_t} \\leftarrow \\gamma \\mathbf{s_{t-1} + (1 - \\gamma)\\mathbf{g_t}^2}$，其中参数$\\gamma &gt; 0$。 保持所有其它部分不变就产生了RMSProp算法。 1.7.1. 算法让我们详细写出这些方程式。$$\\mathbf{s_t} \\leftarrow \\gamma \\mathbf{s_{t-1}} + (1 - \\gamma) \\mathbf{g_t}^2, \\\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\frac{\\eta}{\\sqrt{\\mathbf{s_t}+ \\epsilon} } \\odot \\mathbf{g_t}.$$ 常数 $\\epsilon$通常设置为$10^{-6}$，以确保我们不会因除以零或步长过大而受到影响。 鉴于这种扩展，我们现在可以自由控制学习率$\\eta$，而不考虑基于每个坐标应用的缩放。 1.7.2. 从零开始实现和之前一样，我们使用二次函数$f(\\mathbf{x}) = 0.1x_1^2 + 2x_2^2$来观察RMSProp算法的轨迹。 回想在上一节，当我们使用学习率为0.4的Adagrad算法时，变量在算法的后期阶段移动非常缓慢，因为学习率衰减太快。 RMSProp算法中不会发生这种情况，因为$\\eta$是单独控制的 1234567891011121314151617import mathimport torchfrom d2l import torch as d2ldef rmsprop_2d(x1, x2, s1, s2): g1, g2, eps = 0.2 * x1, 4 * x2, 1e-6 s1 = gamma * s1 + (1 - gamma) * g1 ** 2 s2 = gamma * s2 + (1 - gamma) * g2 ** 2 x1 -= eta / math.sqrt(s1 + eps) * g1 x2 -= eta / math.sqrt(s2 + eps) * g2 return x1, x2, s1, s2def f_2d(x1, x2): return 0.1 * x1 ** 2 + 2 * x2 ** 2eta, gamma = 0.4, 0.9d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d)) 1.7.3. 小结 RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。 RMSProp算法与动量法都使用泄漏平均值。但是，RMSProp算法使用该技术来调整按系数顺序的预处理器。 在实验中，学习率需要由实验者调度。 系数$\\gamma$决定了在调整每坐标比例时历史记录的时长。","link":"/2024/10/20/deep-learning-optimizer-7/"},{"title":"deep-learning-optimizer-8","text":"Adam算法 (Kingma and Ba, 2014)将所有这些技术汇总到一个高效的学习算法中。 不出预料，作为深度学习中使用的更强大和有效的优化算法之一，它非常受欢迎。但是它并非没有问题，尤其是 (Reddi et al., 2019)表明，有时Adam算法可能由于方差控制不良而发散。 在完善工作中， (Zaheer et al., 2018)给Adam算法提供了一个称为Yogi的热补丁来解决这些问题。 1.8. Adam算法我们已经学习了许多有效优化的技术。 在本节讨论之前，我们先详细回顾一下这些技术： 随机梯度下降在解决优化问题时比梯度下降更有效 在一个小批量中使用更大的观测值集，可以通过向量化提供额外效率。这是高效的多机、多GPU和整体并行处理的关键。 动量法中我们添加了一种机制，用于汇总过去梯度的历史以加速收敛。 AdaGrad算法通过对每个坐标缩放来实现高效计算的预处理器。 Adam算法 (Kingma and Ba, 2014)将所有这些技术汇总到一个高效的学习算法中。 不出预料，作为深度学习中使用的更强大和有效的优化算法之一，它非常受欢迎。但是它并非没有问题，尤其是 (Reddi et al., 2019)表明，有时Adam算法可能由于方差控制不良而发散。 在完善工作中， (Zaheer et al., 2018)给Adam算法提供了一个称为Yogi的热补丁来解决这些问题。 下面我们了解一下Adam算法。 1.8.1. 算法Adam算法的关键组成部分之一是：它使用指数加权移动平均值来估算梯度的动量和二次矩，即它使用状态变量 $$\\mathbf{v_t} \\leftarrow \\beta_1 \\mathbf{v_{t-1}} + (1-\\beta_1) \\mathbf{g_t}, \\\\mathbf{s_t} \\leftarrow \\beta_2 \\mathbf{s_{t-1}} + (1 - \\beta_2) \\mathbf{g_t}^2.$$ 这里$\\beta_1$和$beta_2$是非负加权参数。 常将它们设置为$\\beta_1 = 0.9$和$\\beta_2=0.999$。 也就是说，方差估计的移动远远慢于动量估计的移动。 注意，如果我们初始化$\\mathbf{v_0}=\\mathbf{s_0} = 0$，就会获得一个相当大的初始偏差。 我们可以通过使用$\\sum_{t=0}^{t} \\beta^i = \\frac{1 - \\beta^t}{1- \\beta}$来解决这个问题。 有了正确的估计，我们现在可以写出更新方程。 首先，我们以非常类似于RMSProp算法的方式重新缩放梯度以获得 $$\\mathbf{g_t}’ = \\frac{\\eta \\mathbf{v_t}}{\\sqrt{\\mathbf{s_t}} + \\epsilon}$$ 与RMSProp不同，我们的更新使用动量$\\mathbf{v_t}$而不是梯度本身, 最后，我们简单更新：$$\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\mathbf{g_t}’.$$回顾Adam算法，它的设计灵感很清楚： 首先，动量和规模在状态变量中清晰可见， 它们相当独特的定义使我们移除偏项（这可以通过稍微不同的初始化和更新条件来修正）。 其次，RMSProp算法中两项的组合都非常简单。 最后，明确的学习率使我们能够控制步长来解决收敛问题。 1.8.2. 实现123import torchimport torch.cudatorch.cuda.is_available() True 12345678910111213141516171819202122import torchfrom d2l import torch as d2ldef init_adam_states(feature_dim): v_w, v_b = torch.zeros((feature_dim, 1)), torch.zeros(1) s_w, s_b = torch.zeros((feature_dim, 1)), torch.zeros(1) return ((v_w, s_w), (v_b, s_b))def adam(params, states, hyperparams): beta1, beta2, eps = 0.9, 0.999, 1e-6 for p, (v, s) in zip(params, states): with torch.no_grad(): v[:] = beta1 * v + (1 - beta1) * p.grad s[:] = beta2 * s + (1 - beta2) * torch.square(p.grad) v_bias_corr = v / (1 - beta1 ** hyperparams['t']) s_bias_corr = s / (1 - beta2 ** hyperparams['t']) p[:] -= hyperparams['lr'] * v_bias_corr / (torch.sqrt(s_bias_corr) + eps) p.grad.data.zero_() hyperparams['t'] += 1 1.8.3. 小结 Adam算法将许多优化算法的功能结合到了相当强大的更新规则中。 Adam算法在RMSProp算法基础上创建的","link":"/2024/10/20/deep-learning-optimizer-8/"},{"title":"深度学习中的优化算法","text":"深度学习中有许多优化算法，在使用过程中通常我们只需要调用相应的api即可，但了解其原理有助于我们更好地使用它们。优化算法对于深度学习非常重要。一方面，训练复杂的深度学习模型可能需要数小时、几天甚至数周。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原则及其超参数的作用将使我们能够以有针对性的方式调整超参数，以提高深度学习模型的性能。 This is the rest of the content that will only appear when you click “Read more.” 1. 深度学习中的优化算法深度学习中有许多优化算法，在使用过程中通常我们只需要调用相应的api即可，但了解其原理有助于我们更好地使用它们。优化算法对于深度学习非常重要。一方面，训练复杂的深度学习模型可能需要数小时、几天甚至数周。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原则及其超参数的作用将使我们能够以有针对性的方式调整超参数，以提高深度学习模型的性能。 在本次讨论中，我们将深入探讨常见的深度学习优化算法。首先，深度学习中出现的几乎所有优化问题都是非凸的。尽管如此，在凸问题背景下设计和分析算法是非常有启发性的。正是出于这个原因，开始将会介绍凸优化的入门，以及凸目标函数上非常简单的随机梯度下降算法的证明。然后，我们将从梯度下降算法开始，然后讨论随机梯度下降算法、小批量梯度下降算法、动量梯度下降算法、AdaGrad算法、RMSProp算法、Adam算法等。我们将通过具体的代码示例来演示这些算法的工作原理，并讨论它们的优缺点。 1.1. 优化和深度学习在深度学习中，我们通常使用梯度下降算法来优化模型参数。梯度下降算法是一种迭代优化算法，它通过计算目标函数的梯度来更新模型参数，以最小化目标函数。在深度学习中，目标函数通常是模型的损失函数，用于衡量模型预测结果与真实标签之间的差异。对于深度学习问题，我们通常会先定义损失函数。一旦我们有了损失函数，我们就可以使用优化算法来尝试最小化损失。按照传统惯例，大多数优化算法都关注的是最小化。如果我们需要最大化目标，那么有一个简单的解决方案：在目标函数前加负号即可。 1.1.1. 优化目标尽管优化提供了一种最大限度地减少深度学习损失函数的方法，但本质上，优化和深度学习的目标是根本不同的。前者主要关注的是最小化目标，后者则关注在给定有限数据量的情况下寻找合适的模型。例如，训练误差和泛化误差通常不同：由于优化算法的目标函数通常是基于训练数据集的损失函数，因此优化的目标是减少训练误差。但是，深度学习的目标是减少泛化误差。为了实现后者，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合。 1.1.2. 深度学习中的优化挑战深度学习优化存在许多挑战。其中最令人烦恼的是局部最小值、鞍点和梯度消失。 1.1.2.1. 局部最小值对于任何目标函数$f(x)$, 如果在$x$处对应的$f(x)$值小于在$x$附近任意其他点的$f(x)$值，那么$f(x)$可能是局部最小值。如果$f(x)$在$x$处的值是整个域中目标函数的最小值，那么$f(x)$是全局最小值。例如，给定函数$$f(x)= x \\times \\cos(\\pi x) \\quad\\text{for} -1.0 \\leq x \\leq 2.0$$我们可以近似该函数的局部最小值和全局最小值。 123456%matplotlib inlineimport numpy as npimport torchfrom mpl_toolkits import mplot3dfrom d2l import torch as d2l 123456789101112def f(x): return x * torch.cos(np.pi * x)def annotate(text, xy, xytext): #@save d2l.plt.gca().annotate(text, xy=xy, xytext=xytext, arrowprops=dict(arrowstyle='-&gt;'))x = torch.arange(-1.0, 2.0, 0.01)d2l.plot(x, [f(x), ], 'x', 'f(x)')annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))annotate('global minimum', (1.1, -0.95), (0.6, 0.8)) 深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数局部最优，而不是全局最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一。在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中跳出。 1.1.2.2 鞍点除了局部最小值之外，鞍点是梯度消失的另一个原因。鞍点（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数$f(x)=x^3$。它的一阶和二阶导数在$x = 0$时消失。这时优化可能会停止，尽管它不是最小值。 123x = torch.arange(-2.0, 2.0, 0.01)d2l.plot(x, [x**3], 'x', 'f(x)')annotate('saddle point', (0, -0.2), (-0.52, -5.0)) 如下例所示，较高维度的鞍点甚至更加隐蔽。考虑这个函数$f(x, y)=x^2-y^2$。它的鞍点为$(0, 0)$。这是关于$y$的最大值，也是关于$x$的最小值。此外，它看起来像个马鞍，这就是鞍点的名字由来。 12345678910111213x, y = torch.meshgrid( torch.linspace(-1.0, 1.0, 101), torch.linspace(-1.0, 1.0, 101))z = x**2 - y**2ax = d2l.plt.figure().add_subplot(111, projection='3d')ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})ax.plot([0], [0], [0], 'rx')ticks = [-1, 0, 1]d2l.plt.xticks(ticks)d2l.plt.yticks(ticks)ax.set_zticks(ticks)d2l.plt.xlabel('x')d2l.plt.ylabel('y') /home/ippc-zq/miniconda3/envs/yolov5/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] 1.1.2.3. 梯度消失可能遇到的最隐蔽问题是梯度消失。例如，假设我们想最小化函数$f(x)=\\tanh(x)$，然后我们恰好从 $f(x)$ 开始。正如我们所看到的那样，$f$的梯度接近零。更具体地说，$f’(x)=1-\\tanh^2(x)$因此是$f’(4)=0.0013$。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。 123x = torch.arange(-2.0, 5.0, 0.01)d2l.plot(x, [torch.tanh(x)], 'x', 'f(x)')annotate('vanishing gradient', (4, 1), (2, 0.0)) 正如我们所看到的那样，深度学习的优化充满挑战。幸运的是，有一系列强大的算法表现良好，即使对于初学者也很容易使用。此外，没有必要找到最优解。局部最优解或其近似解仍然非常有用。 1.1.3. 小结 优化问题可能有许多局部最小值。 一个问题可能有很多的鞍点，因为问题通常不是凸的。 梯度消失可能会导致优化停滞，重参数化通常会有所帮助。对参数进行良好的初始化也可能是有益的。","link":"/2024/10/14/test-asset/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"}],"categories":[],"pages":[]}