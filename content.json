{"posts":[{"title":"test_asset","text":"This is the introduction of the post… This is the rest of the content that will only appear when you click “Read more.” 1. 深度学习中的优化算法深度学习中有许多优化算法，在使用过程中通常我们只需要调用相应的api即可，但了解其原理有助于我们更好地使用它们。优化算法对于深度学习非常重要。一方面，训练复杂的深度学习模型可能需要数小时、几天甚至数周。优化算法的性能直接影响模型的训练效率。另一方面，了解不同优化算法的原则及其超参数的作用将使我们能够以有针对性的方式调整超参数，以提高深度学习模型的性能。 在本次讨论中，我们将深入探讨常见的深度学习优化算法。首先，深度学习中出现的几乎所有优化问题都是非凸的。尽管如此，在凸问题背景下设计和分析算法是非常有启发性的。正是出于这个原因，开始将会介绍凸优化的入门，以及凸目标函数上非常简单的随机梯度下降算法的证明。然后，我们将从梯度下降算法开始，然后讨论随机梯度下降算法、小批量梯度下降算法、动量梯度下降算法、AdaGrad算法、RMSProp算法、Adam算法等。我们将通过具体的代码示例来演示这些算法的工作原理，并讨论它们的优缺点。 1.1. 优化和深度学习在深度学习中，我们通常使用梯度下降算法来优化模型参数。梯度下降算法是一种迭代优化算法，它通过计算目标函数的梯度来更新模型参数，以最小化目标函数。在深度学习中，目标函数通常是模型的损失函数，用于衡量模型预测结果与真实标签之间的差异。对于深度学习问题，我们通常会先定义损失函数。一旦我们有了损失函数，我们就可以使用优化算法来尝试最小化损失。按照传统惯例，大多数优化算法都关注的是最小化。如果我们需要最大化目标，那么有一个简单的解决方案：在目标函数前加负号即可。 1.1.1. 优化目标尽管优化提供了一种最大限度地减少深度学习损失函数的方法，但本质上，优化和深度学习的目标是根本不同的。前者主要关注的是最小化目标，后者则关注在给定有限数据量的情况下寻找合适的模型。例如，训练误差和泛化误差通常不同：由于优化算法的目标函数通常是基于训练数据集的损失函数，因此优化的目标是减少训练误差。但是，深度学习的目标是减少泛化误差。为了实现后者，除了使用优化算法来减少训练误差之外，我们还需要注意过拟合。 1.1.2. 深度学习中的优化挑战深度学习优化存在许多挑战。其中最令人烦恼的是局部最小值、鞍点和梯度消失。 1.1.2.1. 局部最小值对于任何目标函数$f(x)$, 如果在$x$处对应的$f(x)$值小于在$x$附近任意其他点的$f(x)$值，那么$f(x)$可能是局部最小值。如果$f(x)$在$x$处的值是整个域中目标函数的最小值，那么$f(x)$是全局最小值。例如，给定函数$$f(x)= x \\times \\cos(\\pi x) \\quad\\text{for} -1.0 \\leq x \\leq 2.0$$我们可以近似该函数的局部最小值和全局最小值。 123456%matplotlib inlineimport numpy as npimport torchfrom mpl_toolkits import mplot3dfrom d2l import torch as d2l 123456789101112def f(x): return x * torch.cos(np.pi * x)def annotate(text, xy, xytext): #@save d2l.plt.gca().annotate(text, xy=xy, xytext=xytext, arrowprops=dict(arrowstyle='-&gt;'))x = torch.arange(-1.0, 2.0, 0.01)d2l.plot(x, [f(x), ], 'x', 'f(x)')annotate('local minimum', (-0.3, -0.25), (-0.77, -1.0))annotate('global minimum', (1.1, -0.95), (0.6, 0.8)) 深度学习模型的目标函数通常有许多局部最优解。当优化问题的数值解接近局部最优值时，随着目标函数解的梯度接近或变为零，通过最终迭代获得的数值解可能仅使目标函数局部最优，而不是全局最优。只有一定程度的噪声可能会使参数跳出局部最小值。事实上，这是小批量随机梯度下降的有利特性之一。在这种情况下，小批量上梯度的自然变化能够将参数从局部极小值中跳出。 1.1.2.2 鞍点除了局部最小值之外，鞍点是梯度消失的另一个原因。鞍点（saddle point）是指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置。考虑这个函数$f(x)=x^3$。它的一阶和二阶导数在$x = 0$时消失。这时优化可能会停止，尽管它不是最小值。 123x = torch.arange(-2.0, 2.0, 0.01)d2l.plot(x, [x**3], 'x', 'f(x)')annotate('saddle point', (0, -0.2), (-0.52, -5.0)) 如下例所示，较高维度的鞍点甚至更加隐蔽。考虑这个函数$f(x, y)=x^2-y^2$。它的鞍点为$(0, 0)$。这是关于$y$的最大值，也是关于$x$的最小值。此外，它看起来像个马鞍，这就是鞍点的名字由来。 12345678910111213x, y = torch.meshgrid( torch.linspace(-1.0, 1.0, 101), torch.linspace(-1.0, 1.0, 101))z = x**2 - y**2ax = d2l.plt.figure().add_subplot(111, projection='3d')ax.plot_wireframe(x, y, z, **{'rstride': 10, 'cstride': 10})ax.plot([0], [0], [0], 'rx')ticks = [-1, 0, 1]d2l.plt.xticks(ticks)d2l.plt.yticks(ticks)ax.set_zticks(ticks)d2l.plt.xlabel('x')d2l.plt.ylabel('y') /home/ippc-zq/miniconda3/envs/yolov5/lib/python3.8/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.) return _VF.meshgrid(tensors, **kwargs) # type: ignore[attr-defined] 1.1.2.3. 梯度消失可能遇到的最隐蔽问题是梯度消失。例如，假设我们想最小化函数$f(x)=\\tanh(x)$，然后我们恰好从 $f(x)$ 开始。正如我们所看到的那样，$f$的梯度接近零。更具体地说，$f’(x)=1-\\tanh^2(x)$因此是$f’(4)=0.0013$。因此，在我们取得进展之前，优化将会停滞很长一段时间。事实证明，这是在引入ReLU激活函数之前训练深度学习模型相当棘手的原因之一。 123x = torch.arange(-2.0, 5.0, 0.01)d2l.plot(x, [torch.tanh(x)], 'x', 'f(x)')annotate('vanishing gradient', (4, 1), (2, 0.0)) 正如我们所看到的那样，深度学习的优化充满挑战。幸运的是，有一系列强大的算法表现良好，即使对于初学者也很容易使用。此外，没有必要找到最优解。局部最优解或其近似解仍然非常有用。 1.1.3. 小结 优化问题可能有许多局部最小值。 一个问题可能有很多的鞍点，因为问题通常不是凸的。 梯度消失可能会导致优化停滞，重参数化通常会有所帮助。对参数进行良好的初始化也可能是有益的。","link":"/2024/10/14/test-asset/"}],"tags":[{"name":"deep learning","slug":"deep-learning","link":"/tags/deep-learning/"}],"categories":[],"pages":[]}