{"summary":"<p>随机梯度下降法在每次迭代中，沿着目标函数的负梯度方向更新参数。然而，随机梯度下降法容易陷入局部最优解，且收敛速度慢。本节介绍动量法，它是一种加速梯度下降的优化算法。</p>\n<span id=\"more\"></span>\n\n<h2 id=\"1-5-动量法\"><a href=\"#1-5-动量法\" class=\"headerlink\" title=\"1.5. 动量法\"></a>1.5. 动量法</h2><p>前面一小节中，我们介绍了随机梯度下降法。随机梯度下降法在每次迭代中，沿着目标函数的负梯度方向更新参数。然而，随机梯度下降法容易陷入局部最优解，且收敛速度慢。本节介绍动量法，它是一种加速梯度下降的优化算法。</p>\n<h3 id=\"1-5-1-基础\"><a href=\"#1-5-1-基础\" class=\"headerlink\" title=\"1.5.1. 基础\"></a>1.5.1. 基础</h3><p>本节将探讨更有效的优化算法，尤其是针对实验中常见的某些类型的优化问题。</p>\n<h4 id=\"1-5-1-1-泄漏平均值\"><a href=\"#1-5-1-1-泄漏平均值\" class=\"headerlink\" title=\"1.5.1.1. 泄漏平均值\"></a>1.5.1.1. 泄漏平均值</h4><p>上一节中我们讨论了小批量随机梯度下降作为加速计算的手段。 它也有很好的副作用，即平均梯度减小了方差。 小批量随机梯度下降可以通过以下方式计算：<br>$$<br>\\mathbf{g}<em>{t, t-1} &#x3D; \\partial_w \\frac{1}{\\mathcal{|B_t|}} \\sum</em>{i \\in \\mathcal{B_t}} f(\\mathbf{x_i}, \\mathbf{w_{t-1}}) &#x3D; \\frac{1}{\\mathcal{|B_t|}} \\sum_{i \\in \\mathcal{B_t}} \\mathcal{h}_{i, t-1}.   \\tag{1.5.1}<br>$$</p>\n<p>为了保持记法简单，在这里我们使用$\\mathbf{h}<em>{i, t-1} &#x3D; \\partial_w f(\\mathbf{x_i}, \\mathbf{w</em>{t-1}})$作为样本$i$的随机梯度下降，使用时间$t-1$时更新的权重$t-1$。 如果我们能够从方差减少的影响中受益，甚至超过小批量上的梯度平均值，那很不错。 完成这项任务的一种选择是用泄漏平均值（leaky average）取代梯度计算：<br>$$<br>\\mathbf{v_t} &#x3D; \\beta \\mathbf{v_t} + \\mathbf{g_{t, t-1}}<br>$$<br>其中$\\beta \\in (0, 1)$。 这有效地将瞬时梯度替换为多个“过去”梯度的平均值。 $\\mathbf{v}$被称为动量（momentum）， 它累加了过去的梯度。 为了更详细地解释，让我们递归地将$\\mathbf{v_t}$扩展到<br>$$<br>\\mathbf{v_t} &#x3D; \\beta^2 \\mathbf{v_{t-2}} + \\beta \\mathbf{g_{t-1, t-2}} + \\mathbf{g_{t, t-1}} &#x3D; \\dots, &#x3D; \\sum_{\\tau &#x3D; 0}^{t-1} \\beta^{\\tau} \\mathbf{g_{t-\\tau, t-\\tau - 1}}.<br>$$</p>\n<p>其中，较大的$\\beta$相当于长期平均值，而较小的$\\beta$相对于梯度法只是略有修正。 新的梯度替换不再指向特定实例下降最陡的方向，而是指向过去梯度的加权平均值的方向。 这使我们能够实现对单批量计算平均值的大部分好处，而不产生实际计算其梯度的代价。</p>\n<p>上述推理构成了“加速”梯度方法的基础，例如具有动量的梯度。 在优化问题条件不佳的情况下（例如，有些方向的进展比其他方向慢得多，类似狭窄的峡谷），“加速”梯度还额外享受更有效的好处。 此外，它们允许我们对随后的梯度计算平均值，以获得更稳定的下降方向。 诚然，即使是对于无噪声凸问题，加速度这方面也是动量如此起效的关键原因之一。</p>\n<p>正如人们所期望的，由于其功效，动量是深度学习及其后优化中一个深入研究的主题。 例如，请参阅文章，观看深入分析和互动动画。 动量是由 (Polyak, 1964)提出的。 (Nesterov, 2018)在凸优化的背景下进行了详细的理论讨论。 长期以来，深度学习的动量一直被认为是有益的。 有关实例的详细信息，请参阅 (Sutskever et al2013)的讨论。</p>\n<h4 id=\"1-5-1-2-条件不佳的问题\"><a href=\"#1-5-1-2-条件不佳的问题\" class=\"headerlink\" title=\"1.5.1.2. 条件不佳的问题\"></a>1.5.1.2. 条件不佳的问题</h4><p>为了更好地了解动量法的几何属性，回想我们在前面中使用了$f(\\mathbf{x}) &#x3D; x_1^2 + 2x_2^2$ ，即中度扭曲的椭球目标。 我们通过向$x_1$方向伸展它来进一步扭曲这个函数<br>$$<br>f(\\mathbf{x}) &#x3D; 0.1x_1^2 + 2x_2^2.<br>$$</p>\n<p>与之前一样，$f$在$(0, 0)$<br>有最小值， 该函数在$x_1$的方向上非常平坦。 让我们看看在这个新函数上执行梯度下降时会发生什么。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\">eta = <span class=\"number\">0.4</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f_2d</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0.1</span> * x1 ** <span class=\"number\">2</span> + <span class=\"number\">2</span> * x2 ** <span class=\"number\">2</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gd_2d</span>(<span class=\"params\">x1, x2, s1, s2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (x1 - eta * <span class=\"number\">0.2</span> * x1, x2 - eta * <span class=\"number\">4</span> * x2, <span class=\"number\">0</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 20, x1: -0.943467, x2: -0.000073\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-5/11_5_3_1.svg\" alt=\"svg\"></p>\n<p>从构造来看，$x_2$方向的梯度比水平$x_1$方向的梯度大得多，变化也快得多。 因此，我们陷入两难：如果选择较小的学习率，我们会确保不会在$x_2$方向发散，但要承受在$x_1$方向的缓慢收敛。相反，如果学习率较高，我们在$x_1$方向上进展很快，但在$x_2$方向将会发散。 下面的例子说明了即使学习率从$0.4$略微提高到$0.6$，也会发生变化。$x_1$<br>方向上的收敛有所改善，但整体来看解的质量更差了。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">eta = <span class=\"number\">0.6</span></span><br><span class=\"line\">d2l.show_trace_2d(f_2d, d2l.train_2d(gd_2d))</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 20, x1: -0.387814, x2: -1673.365109\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-5/11_5_5_1.svg\" alt=\"svg\"></p>\n<h4 id=\"1-5-1-3-动量法\"><a href=\"#1-5-1-3-动量法\" class=\"headerlink\" title=\"1.5.1.3. 动量法\"></a>1.5.1.3. 动量法</h4><p>动量法（momentum）使我们能够解决上面描述的梯度下降问题。 观察上面的优化轨迹，我们可能会直觉到计算过去的平均梯度效果会很好。 毕竟，在<br>方向上，这将聚合非常对齐的梯度，从而增加我们在每一步中覆盖的距离。 相反，在梯度振荡的<br>方向，由于相互抵消了对方的振荡，聚合梯度将减小步长大小。 使用$\\mathbf{v_t}$而不是梯度$\\mathbf{g_t}$可以生成以下更新等式：<br>$$<br>\\mathbf{v_t} \\leftarrow \\beta \\mathbf{v_{t-1}} + \\mathbf{g_{t, t-1}}, \\<br>\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\eta_t \\mathbf{v_t}.<br>$$<br>请注意，对于$\\beta &#x3D; 0$，我们恢复常规的梯度下降。 在深入研究它的数学属性之前，让我们快速看一下算法在实验中的表现如何。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">momentum_2d</span>(<span class=\"params\">x1, x2, v1, v2</span>):</span><br><span class=\"line\">    v1 = beta * v1 + <span class=\"number\">0.2</span> * x1</span><br><span class=\"line\">    v2 = beta * v2 + <span class=\"number\">4</span> * x2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x1 - eta * v1, x2 - eta * v2, v1, v2</span><br><span class=\"line\"></span><br><span class=\"line\">eta, beta = <span class=\"number\">0.6</span>, <span class=\"number\">0.5</span></span><br><span class=\"line\">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 20, x1: 0.007188, x2: 0.002553\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-5/11_5_7_1.svg\" alt=\"svg\"></p>\n<p>正如所见，尽管学习率与我们以前使用的相同，动量法仍然很好地收敛了。 让我们看看当降低动量参数时会发生什么。 将其减半至$\\beta &#x3D; 0.25$会导致一条几乎没有收敛的轨迹。 尽管如此，它比没有动量时解将会发散要好得多。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">eta, beta = <span class=\"number\">0.6</span>, <span class=\"number\">0.25</span></span><br><span class=\"line\">d2l.show_trace_2d(f_2d, d2l.train_2d(momentum_2d))</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 20, x1: -0.126340, x2: -0.186632\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-5/11_5_9_1.svg\" alt=\"svg\"></p>\n<p>请注意，我们可以将动量法与随机梯度下降，特别是小批量随机梯度下降结合起来。 唯一的变化是，在这种情况下，我们将梯度<br>替换为$\\mathbf{g_{t, t-1}} \\mathbf{g_{t}}$。 为了方便起见，我们在时间$t&#x3D;0$初始化$\\mathbf{v_0} &#x3D; 0$。</p>\n<h4 id=\"1-5-2-1-从零开始实现\"><a href=\"#1-5-2-1-从零开始实现\" class=\"headerlink\" title=\"1.5.2.1. 从零开始实现\"></a>1.5.2.1. 从零开始实现</h4><p>相比于小批量随机梯度下降，动量方法需要维护一组辅助变量，即速度。 它与梯度以及优化问题的变量具有相同的形状。 在下面的实现中，我们称这些变量为states。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_momentum_states</span>(<span class=\"params\">feature_dim</span>):</span><br><span class=\"line\">    v_w = torch.zeros((feature_dim, <span class=\"number\">1</span>))</span><br><span class=\"line\">    v_b = torch.zeros(<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (v_w, v_b)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd_momentum</span>(<span class=\"params\">params, states, hyperparams</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p, v <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(params, states):</span><br><span class=\"line\">        <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">            v[:] = hyperparams[<span class=\"string\">&#x27;momentum&#x27;</span>] * v + p.grad</span><br><span class=\"line\">            p[:] -= hyperparams[<span class=\"string\">&#x27;lr&#x27;</span>] * v</span><br><span class=\"line\">        p.grad.data.zero_()</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"1-5-3-小结\"><a href=\"#1-5-3-小结\" class=\"headerlink\" title=\"1.5.3 小结\"></a>1.5.3 小结</h3><ul>\n<li><p>动量法用过去梯度的平均值来替换梯度，这大大加快了收敛速度。</p>\n</li>\n<li><p>对于无噪声梯度下降和嘈杂随机梯度下降，动量法都是可取的。</p>\n</li>\n<li><p>动量法可以防止在随机梯度下降的优化过程停滞的问题。</p>\n</li>\n<li><p>动量法的实现非常简单，但它需要我们存储额外的状态向量</p>\n</li>\n</ul>\n"}