{"summary":"<p>尽管梯度下降（gradient descent）很少直接用于深度学习， 但了解它是理解下一节随机梯度下降算法的关键。 例如，由于学习率过大，优化问题可能会发散，这种现象早已在梯度下降中出现。 同样地，预处理是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。 让我们从简单的一维梯度下降开始介绍。</p>\n<span id=\"more\"></span>\n\n\n<h2 id=\"1-2-梯度下降\"><a href=\"#1-2-梯度下降\" class=\"headerlink\" title=\"1.2. 梯度下降\"></a>1.2. 梯度下降</h2><p>尽管梯度下降（gradient descent）很少直接用于深度学习， 但了解它是理解下一节随机梯度下降算法的关键。 例如，由于学习率过大，优化问题可能会发散，这种现象早已在梯度下降中出现。 同样地，预处理是梯度下降中的一种常用技术， 还被沿用到更高级的算法中。 让我们从简单的一维梯度下降开始介绍。</p>\n<h3 id=\"1-2-1-一维梯度下降\"><a href=\"#1-2-1-一维梯度下降\" class=\"headerlink\" title=\"1.2.1. 一维梯度下降\"></a>1.2.1. 一维梯度下降</h3><p>为什么梯度下降算法可以优化目标函数？ 一维中的梯度下降给我们很好的启发。 考虑一类连续可微实值函数$f: \\mathbb{R} \\rightarrow \\mathbb{R}$， 利用泰勒展开，我们可以得到<br>$$<br>f(x + \\epsilon) &#x3D; f(x) + \\epsilon f’(x) + O(\\epsilon^2). \\tag{1.2.1}<br>$$<br>即在一阶近似中，$f(x + \\epsilon)$ 可通过$x$处的函数值$f(x)$和一阶导数$f’(x)$得出。我们可以假设在负梯度方向上移动的$\\epsilon$会减少$f$的值。为简单起见，选择固定步长$\\eta$, 然后取$\\epsilon&#x3D;\\eta f’(x)$。将其代入泰勒展开式我们可以得到<br>$$<br>f(x - \\eta f’(x)) &#x3D; f(x) - \\eta f’^2(x) + O(\\eta^2 f’^2(x)) \\tag{1.2.2}<br>$$<br>如果其导数$f’(x) \\neq 0$ 没有消失， 我们就能继续展开，这是因为$\\eta f’^2(x) &gt; 0$。 此外，我们总是可以令$\\eta$小到足以使高阶项变得不相关。 因此，<br>$$<br>f(x - \\eta f’(x)) \\lessapprox f(x). \\tag{1.2.3}<br>$$<br>这意味着，如果我们使用:<br>$$<br>x \\leftarrow x - \\eta f’(x)  \\tag{1.2.4}<br>$$<br>来迭代$x$，函数$f(x)$的值可能会下降。 因此，在梯度下降中，我们首先选择初始值$x$和常数$\\eta &gt;  0$， 然后使用它们连续迭代$x$，直到停止条件达成。 例如，当梯度$|f’(x)|$的幅度足够小或迭代次数达到某个值时。</p>\n<p>下面我们来展示如何实现梯度下降。为了简单起见，我们选用目标函数$f(x)&#x3D;x^2$。 尽管我们知道$x&#x3D;0$时$f(x)$能取得最小值， 但我们仍然使用这个简单的函数来观察$x$的变化。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> mpl_toolkits <span class=\"keyword\">import</span> mplot3d</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x**<span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f_grad</span>(<span class=\"params\">x</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">2</span> * x</span><br></pre></td></tr></table></figure>\n\n<p>接下来，我们使用$x&#x3D;10$作为初始值，并假设$\\eta&#x3D;0.2$。 使用梯度下降法迭代$x$共10次，我们可以看到，$x$的值最终将接近最优解。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gd</span>(<span class=\"params\">eta, f_grad</span>):</span><br><span class=\"line\">    x = <span class=\"number\">10</span>          <span class=\"comment\"># init x position </span></span><br><span class=\"line\">    results = [x]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">10</span>):</span><br><span class=\"line\">        x -= eta * f_grad(x)</span><br><span class=\"line\">        results.append(<span class=\"built_in\">float</span>(x))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch 10, x:<span class=\"subst\">&#123;x:f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> results</span><br><span class=\"line\"></span><br><span class=\"line\">results = gd(<span class=\"number\">0.2</span>, f_grad)</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 10, x:0.060466\n</code></pre>\n<p>对进行$x$优化的过程可以绘制如下。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_trace</span>(<span class=\"params\">results, f</span>):</span><br><span class=\"line\">    n = <span class=\"built_in\">max</span>(<span class=\"built_in\">abs</span>(<span class=\"built_in\">min</span>(results)), <span class=\"built_in\">abs</span>(<span class=\"built_in\">max</span>(results)))</span><br><span class=\"line\">    f_line = torch.arange(-n, n, <span class=\"number\">0.01</span>)</span><br><span class=\"line\">    d2l.set_figsize()</span><br><span class=\"line\">    d2l.plot([f_line, results], [[f(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> f_line], [</span><br><span class=\"line\">        f(x) <span class=\"keyword\">for</span> x <span class=\"keyword\">in</span> results]], <span class=\"string\">&#x27;x&#x27;</span>, <span class=\"string\">&#x27;f(x)&#x27;</span>, fmts=[<span class=\"string\">&#x27;-&#x27;</span>, <span class=\"string\">&#x27;-o&#x27;</span>])</span><br><span class=\"line\"></span><br><span class=\"line\">show_trace(results, f)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/2024/10/20/deep-learning-optimizer-2/11_2_7_0.svg\" alt=\"svg\"></p>\n<h4 id=\"1-2-1-1-学习率\"><a href=\"#1-2-1-1-学习率\" class=\"headerlink\" title=\"1.2.1.1. 学习率\"></a>1.2.1.1. 学习率</h4><p>学习率（learning rate）决定目标函数能否收敛到局部最小值，以及何时收敛到最小值。 学习率$\\eta$可由算法设计者设置。 请注意，如果使用的学习率太小，将导致$x$的更新非常缓慢，需要更多的迭代。 例如，考虑同一优化问题中$\\eta&#x3D;0.05$的进度。 如下所示，尽管经过了10个步骤，我们仍然离最优解很远。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show_trace(gd(<span class=\"number\">0.05</span>, f_grad), f)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 10, x:3.486784\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-2/11_2_9_1.svg\" alt=\"svg\"></p>\n<p>相反，如果我们使用过高的学习率，$|\\eta f’(x)|$对于一阶泰勒展开式可能太大。 也就是说，(1.2.2)中的$O(\\eta^2 f’^2(x))$可能变得显著了。 在这种情况下，$x$的迭代不能保证降低$f(x)$的值。 例如，当学习率为$\\eta &#x3D; 1.1$时，$x$超出了最优解$x &#x3D; 0$并逐渐发散。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">show_trace(gd(<span class=\"number\">1.1</span>, f_grad), f)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 10, x:61.917364\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-2/11_2_11_1.svg\" alt=\"svg\"></p>\n<h4 id=\"1-2-1-2-局部最小值\"><a href=\"#1-2-1-2-局部最小值\" class=\"headerlink\" title=\"1.2.1.2. 局部最小值\"></a>1.2.1.2. 局部最小值</h4><p>为了演示非凸函数的梯度下降，考虑函数$f(x) &#x3D; x  \\cdot \\cos(cx)$，其中$c$为某常数。 这个函数有无穷多个局部最小值。 根据我们选择的学习率，我们最终可能只会得到许多解的一个。 下面的例子说明了（不切实际的）高学习率如何导致较差的局部最小值。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">c = torch.tensor(<span class=\"number\">0.15</span> * np.pi)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x</span>):  <span class=\"comment\"># 目标函数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x * torch.cos(c * x)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f_grad</span>(<span class=\"params\">x</span>):  <span class=\"comment\"># 目标函数的梯度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.cos(c * x) - c * x * torch.sin(c * x)</span><br><span class=\"line\"></span><br><span class=\"line\">show_trace(gd(<span class=\"number\">2</span>, f_grad), f)</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 10, x:-1.528166\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-2/11_2_13_1.svg\" alt=\"svg\"></p>\n<h3 id=\"1-2-2-多元梯度下降\"><a href=\"#1-2-2-多元梯度下降\" class=\"headerlink\" title=\"1.2.2. 多元梯度下降\"></a>1.2.2. 多元梯度下降</h3><p>现在我们对单变量的情况有了更好的理解，让我们考虑一下$\\mathbf{x} &#x3D; [x_1, x_2, \\dots, x_d]^T$的情况。 即目标函数<br>$f: \\mathbb{R^d} \\rightarrow \\mathbb{R}$将向量映射成标量。 相应地，它的梯度也是多元的，它是一个由$d$个偏导数组成的向量：<br>$$<br>\\nabla f(\\mathbf{x}) &#x3D; \\left[\\frac{\\partial f(\\mathbf{x})}{\\partial x_1},\\frac{\\partial f(\\mathbf{x})}{\\partial x_2}, \\dots, \\frac{\\partial f(\\mathbf{x})}{\\partial x_d} \\right]^T.   \\tag{1.2.5}<br>$$<br>梯度中的每个偏导数元素$\\partial f(\\mathbf{x}) &#x2F; \\partial x_i$代表了当输入$x_i$时$f$在$\\mathbf{x}$处的变化率。 和先前单变量的情况一样，我们可以对多变量函数使用相应的泰勒近似来思考。 具体来说，<br>$$<br>f(\\mathbf{x} + \\epsilon) &#x3D; f(\\mathbf{x}) + \\epsilon^T \\nabla f(\\mathbf{x}) + O(||\\epsilon||^2) \\tag{1.2.6}<br>$$<br>换句话说，在$\\epsilon$的二阶项中， 最陡下降的方向由负梯度$-\\nabla f(\\mathbf{x})$得出。 选择合适的学习率$\\eta &gt; 0$来生成典型的梯度下降算法：<br>$$<br>\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f(\\mathbf{x}).  \\tag{1.2.7}<br>$$<br>这个算法在实践中的表现如何呢？ 我们构造一个目标函数$f(\\mathbf{x}) &#x3D; x_1^2 + 2x_2^2$， 并有二维向量$\\mathbf{x} &#x3D; [x_1, x_2]^T$作为输入， 标量作为输出。 梯度由$\\nabla f(\\mathbf{x}) &#x3D; [2x_1, 4x_2]^T$给出。 我们将从初始位置$[-5, -2]$通过梯度下降观察$\\mathbf{x}$的轨迹。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_2d</span>(<span class=\"params\">trainer, steps=<span class=\"number\">20</span>, f_grad=<span class=\"literal\">None</span></span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;用定制的训练机优化2D目标函数&quot;&quot;&quot;</span></span><br><span class=\"line\">    <span class=\"comment\"># s1和s2是稍后将使用的内部状态变量</span></span><br><span class=\"line\">    x1, x2, s1, s2 = -<span class=\"number\">5</span>, -<span class=\"number\">2</span>, <span class=\"number\">0</span>, <span class=\"number\">0</span></span><br><span class=\"line\">    results = [(x1, x2)]</span><br><span class=\"line\">    <span class=\"keyword\">for</span> i <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(steps):</span><br><span class=\"line\">        <span class=\"keyword\">if</span> f_grad:</span><br><span class=\"line\">            x1, x2, s1, s2 = trainer(x1, x2, s1, s2, f_grad)</span><br><span class=\"line\">        <span class=\"keyword\">else</span>:</span><br><span class=\"line\">            x1, x2, s1, s2 = trainer(x1, x2, s1, s2)</span><br><span class=\"line\">        results.append((x1, x2))</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;epoch <span class=\"subst\">&#123;i + <span class=\"number\">1</span>&#125;</span>, x1: <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(x1):f&#125;</span>, x2: <span class=\"subst\">&#123;<span class=\"built_in\">float</span>(x2):f&#125;</span>&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> results</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">show_trace_2d</span>(<span class=\"params\">f, results</span>):  <span class=\"comment\">#@save</span></span><br><span class=\"line\">    <span class=\"string\">&quot;&quot;&quot;显示优化过程中2D变量的轨迹&quot;&quot;&quot;</span></span><br><span class=\"line\">    d2l.set_figsize()</span><br><span class=\"line\">    d2l.plt.plot(*<span class=\"built_in\">zip</span>(*results), <span class=\"string\">&#x27;-o&#x27;</span>, color=<span class=\"string\">&#x27;#ff7f0e&#x27;</span>)</span><br><span class=\"line\">    x1, x2 = torch.meshgrid(torch.arange(-<span class=\"number\">5.5</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.1</span>),</span><br><span class=\"line\">                          torch.arange(-<span class=\"number\">3.0</span>, <span class=\"number\">1.0</span>, <span class=\"number\">0.1</span>), indexing=<span class=\"string\">&#x27;ij&#x27;</span>)</span><br><span class=\"line\">    d2l.plt.contour(x1, x2, f(x1, x2), colors=<span class=\"string\">&#x27;#1f77b4&#x27;</span>)</span><br><span class=\"line\">    d2l.plt.xlabel(<span class=\"string\">&#x27;x1&#x27;</span>)</span><br><span class=\"line\">    d2l.plt.ylabel(<span class=\"string\">&#x27;x2&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n<p>接下来，我们观察学习率$\\eta &#x3D; 0.1$时优化变量$\\mathbf{x}$的轨迹。 可以看到，经过20步之后，$\\mathbf{x}$的值接近其位于$[0, 0]$的最小值。 虽然进展相当顺利，但相当缓慢。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f_2d</span>(<span class=\"params\">x1, x2</span>):  <span class=\"comment\"># 目标函数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x1 ** <span class=\"number\">2</span> + <span class=\"number\">2</span> * x2 ** <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f_2d_grad</span>(<span class=\"params\">x1, x2</span>):  <span class=\"comment\"># 目标函数的梯度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> (<span class=\"number\">2</span> * x1, <span class=\"number\">4</span> * x2)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">gd_2d</span>(<span class=\"params\">x1, x2, s1, s2, f_grad</span>):</span><br><span class=\"line\">    g1, g2 = f_grad(x1, x2)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (x1 - eta * g1, x2 - eta * g2, <span class=\"number\">0</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">eta = <span class=\"number\">0.1</span></span><br><span class=\"line\">show_trace_2d(f_2d, train_2d(gd_2d, f_grad=f_2d_grad))</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 20, x1: -0.057646, x2: -0.000073\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-2/11_2_17_1.svg\" alt=\"svg\"></p>\n<h3 id=\"1-2-3-小结\"><a href=\"#1-2-3-小结\" class=\"headerlink\" title=\"1.2.3. 小结\"></a>1.2.3. 小结</h3><ul>\n<li><p>学习率的大小很重要：学习率太大会使模型发散，学习率太小会没有进展。</p>\n</li>\n<li><p>梯度下降会可能陷入局部极小值，而得不到全局最小值。</p>\n</li>\n<li><p>在高维模型中，调整学习率是很复杂的。</p>\n</li>\n</ul>\n"}