{"summary":"<p>基于梯度的学习方法中遇到了两个极端情况： 使用完整数据集来计算梯度并更新参数， 以及一次处理一个训练样本来取得进展。 二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。 而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。 这暗示了两者之间可能有折中方案，这便涉及到小批量随机梯度下降（minibatch gradient descent）。</p>\n<span id=\"more\"></span>\n\n\n<h2 id=\"1-4-小批量随机梯度下降\"><a href=\"#1-4-小批量随机梯度下降\" class=\"headerlink\" title=\"1.4. 小批量随机梯度下降\"></a>1.4. 小批量随机梯度下降</h2><p>到目前为止，我们在基于梯度的学习方法中遇到了两个极端情况： 使用完整数据集来计算梯度并更新参数， 以及一次处理一个训练样本来取得进展。 二者各有利弊：每当数据非常相似时，梯度下降并不是非常“数据高效”。 而由于CPU和GPU无法充分利用向量化，随机梯度下降并不特别“计算高效”。 这暗示了两者之间可能有折中方案，这便涉及到小批量随机梯度下降（minibatch gradient descent）。</p>\n<h3 id=\"1-4-1-小批量\"><a href=\"#1-4-1-小批量\" class=\"headerlink\" title=\"1.4.1 小批量\"></a>1.4.1 小批量</h3><p>使用随机梯度下降时，我们每次只处理一个训练样本。 也就是说，每当我们执行$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta_t \\mathbf{g_t}$ 其中<br>$$<br>\\mathbf{g_t} &#x3D; \\partial_w f(\\mathbf{x}_t, \\mathbf{w}).  \\tag{1.4.1}<br>$$<br>每次取出一个样本，没有充分利用CPU和GPU的并行计算能力。这对与于大型数据集来说尤其成问题。</p>\n<p>当我们使用小批量随机梯度下降时，我们每次处理一个由多个训练样本组成的“小批量”。 也就是说，我们计算$\\mathbf{g_t}$如下：<br>$$<br>\\mathbf{g_t} &#x3D; \\frac{1}{|\\mathcal{B}<em>t|} \\sum</em>{i \\in \\mathcal{B}_t} \\partial_w f(\\mathbf{x}_i, \\mathbf{w}).  \\tag{1.4.2}<br>$$</p>\n<p>由于$\\mathbf{x_t}$<br>和小批量$\\mathcal{B}_t$的所有元素都是从训练集中随机抽出的，因此梯度的期望保持不变。 另一方面，方差显著降低。 由于小批量梯度由正在被平均计算的$b \\in |\\beta_t|$个独立梯度组成，其标准差降低了$b^{-\\frac{1}{2}}$。 这本身就是一件好事，因为这意味着更新与完整的梯度更接近了。</p>\n<p>直观来说，这表明选择大型的小批量$\\mathcal{B}_t$<br>将是普遍可行的。 然而，经过一段时间后，与计算代价的线性增长相比，标准差的额外减少是微乎其微的。 在实践中我们选择一个足够大的小批量，它可以提供良好的计算效率同时仍适合GPU的内存。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> torch <span class=\"keyword\">import</span> nn</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\">d2l.DATA_HUB[<span class=\"string\">&#x27;airfoil&#x27;</span>] = (d2l.DATA_URL + <span class=\"string\">&#x27;airfoil_self_noise.dat&#x27;</span>,</span><br><span class=\"line\">                           <span class=\"string\">&#x27;76e5be1548fd8222e5074cf0faae75edff8cf93f&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">get_data_ch11</span>(<span class=\"params\">batch_size=<span class=\"number\">10</span>, n=<span class=\"number\">1500</span></span>):</span><br><span class=\"line\">    data = np.genfromtxt(d2l.download(<span class=\"string\">&#x27;airfoil&#x27;</span>),</span><br><span class=\"line\">                         dtype=np.float32, delimiter=<span class=\"string\">&#x27;\\t&#x27;</span>)</span><br><span class=\"line\">    data = torch.from_numpy((data - data.mean(axis=<span class=\"number\">0</span>)) / data.std(axis=<span class=\"number\">0</span>))</span><br><span class=\"line\">    data_iter = d2l.load_array((data[:n, :-<span class=\"number\">1</span>], data[:n, -<span class=\"number\">1</span>]),</span><br><span class=\"line\">                               batch_size, is_train=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> data_iter, data.shape[<span class=\"number\">1</span>]-<span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd</span>(<span class=\"params\">params, states, hyperparams</span>):</span><br><span class=\"line\">    <span class=\"keyword\">for</span> p <span class=\"keyword\">in</span> params:</span><br><span class=\"line\">        p.data.sub_(hyperparams[<span class=\"string\">&#x27;lr&#x27;</span>] * p.grad)</span><br><span class=\"line\">        p.grad.data.zero_()</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">#@save</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_ch11</span>(<span class=\"params\">trainer_fn, states, hyperparams, data_iter,</span></span><br><span class=\"line\"><span class=\"params\">               feature_dim, num_epochs=<span class=\"number\">2</span></span>):</span><br><span class=\"line\">    <span class=\"comment\"># 初始化模型</span></span><br><span class=\"line\">    w = torch.normal(mean=<span class=\"number\">0.0</span>, std=<span class=\"number\">0.01</span>, size=(feature_dim, <span class=\"number\">1</span>),</span><br><span class=\"line\">                     requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    b = torch.zeros((<span class=\"number\">1</span>), requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">    net, loss = <span class=\"keyword\">lambda</span> X: d2l.linreg(X, w, b), d2l.squared_loss</span><br><span class=\"line\">    <span class=\"comment\"># 训练模型</span></span><br><span class=\"line\">    animator = d2l.Animator(xlabel=<span class=\"string\">&#x27;epoch&#x27;</span>, ylabel=<span class=\"string\">&#x27;loss&#x27;</span>,</span><br><span class=\"line\">                            xlim=[<span class=\"number\">0</span>, num_epochs], ylim=[<span class=\"number\">0.22</span>, <span class=\"number\">0.35</span>])</span><br><span class=\"line\">    n, timer = <span class=\"number\">0</span>, d2l.Timer()</span><br><span class=\"line\">    <span class=\"keyword\">for</span> _ <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(num_epochs):</span><br><span class=\"line\">        <span class=\"keyword\">for</span> X, y <span class=\"keyword\">in</span> data_iter:</span><br><span class=\"line\">            l = loss(net(X), y).mean()</span><br><span class=\"line\">            l.backward()</span><br><span class=\"line\">            trainer_fn([w, b], states, hyperparams)</span><br><span class=\"line\">            n += X.shape[<span class=\"number\">0</span>]</span><br><span class=\"line\">            <span class=\"keyword\">if</span> n % <span class=\"number\">200</span> == <span class=\"number\">0</span>:</span><br><span class=\"line\">                timer.stop()</span><br><span class=\"line\">                animator.add(n/X.shape[<span class=\"number\">0</span>]/<span class=\"built_in\">len</span>(data_iter),</span><br><span class=\"line\">                             (d2l.evaluate_loss(net, data_iter, loss),))</span><br><span class=\"line\">                timer.start()</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">f&#x27;loss: <span class=\"subst\">&#123;animator.Y[<span class=\"number\">0</span>][-<span class=\"number\">1</span>]:<span class=\"number\">.3</span>f&#125;</span>, <span class=\"subst\">&#123;timer.avg():<span class=\"number\">.3</span>f&#125;</span> sec/epoch&#x27;</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> timer.cumsum(), animator.Y[<span class=\"number\">0</span>]</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">train_sgd</span>(<span class=\"params\">lr, batch_size, num_epochs=<span class=\"number\">2</span></span>):</span><br><span class=\"line\">    data_iter, feature_dim = get_data_ch11(batch_size)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> train_ch11(</span><br><span class=\"line\">        sgd, <span class=\"literal\">None</span>, &#123;<span class=\"string\">&#x27;lr&#x27;</span>: lr&#125;, data_iter, feature_dim, num_epochs)</span><br><span class=\"line\"></span><br><span class=\"line\">gd_res = train_sgd(<span class=\"number\">1</span>, <span class=\"number\">1500</span>, <span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss: 0.243, 0.011 sec/epoch\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-4/11_4_2_1.svg\" alt=\"svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sgd_res = train_sgd(<span class=\"number\">0.005</span>, <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss: 0.245, 0.036 sec/epoch\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-4/11_4_3_1.svg\" alt=\"svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mini1_res = train_sgd(<span class=\"number\">.4</span>, <span class=\"number\">100</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss: 0.243, 0.001 sec/epoch\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-4/11_4_4_1.svg\" alt=\"svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mini2_res = train_sgd(<span class=\"number\">.05</span>, <span class=\"number\">10</span>)</span><br></pre></td></tr></table></figure>\n\n<pre><code>loss: 0.244, 0.005 sec/epoch\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-4/11_4_5_1.svg\" alt=\"svg\"></p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">d2l.set_figsize([<span class=\"number\">6</span>, <span class=\"number\">3</span>])</span><br><span class=\"line\">d2l.plot(*<span class=\"built_in\">list</span>(<span class=\"built_in\">map</span>(<span class=\"built_in\">list</span>, <span class=\"built_in\">zip</span>(gd_res, sgd_res, mini1_res, mini2_res))),</span><br><span class=\"line\">         <span class=\"string\">&#x27;time (sec)&#x27;</span>, <span class=\"string\">&#x27;loss&#x27;</span>, xlim=[<span class=\"number\">1e-2</span>, <span class=\"number\">10</span>],</span><br><span class=\"line\">         legend=[<span class=\"string\">&#x27;gd&#x27;</span>, <span class=\"string\">&#x27;sgd&#x27;</span>, <span class=\"string\">&#x27;batch size=100&#x27;</span>, <span class=\"string\">&#x27;batch size=10&#x27;</span>])</span><br><span class=\"line\">d2l.plt.gca().set_xscale(<span class=\"string\">&#x27;log&#x27;</span>)</span><br></pre></td></tr></table></figure>\n\n\n<p><img src=\"/2024/10/20/deep-learning-optimizer-4/11_4_6_0.svg\" alt=\"svg\"></p>\n<h3 id=\"1-4-2-小结\"><a href=\"#1-4-2-小结\" class=\"headerlink\" title=\"1.4.2 小结\"></a>1.4.2 小结</h3><ul>\n<li><p>随机梯度下降的“统计效率”与大批量一次处理数据的“计算效率”之间存在权衡。小批量随机梯度下降提供了两全其美的答案：计算和统计效率。</p>\n</li>\n<li><p>在小批量随机梯度下降中，我们处理通过训练数据的随机排列获得的批量数据（即每个观测值只处理一次，但按随机顺序）。</p>\n</li>\n<li><p>在训练期间降低学习率有助于训练。</p>\n</li>\n<li><p>一般来说，小批量随机梯度下降比随机梯度下降和梯度下降的速度快，收敛风险较小。</p>\n</li>\n</ul>\n"}