{"summary":"<p>(Tieleman and Hinton, 2012)建议以RMSProp算法作为将速率调度与坐标自适应学习率分离的简单修复方法。</p>\n<span id=\"more\"></span>\n\n<h3 id=\"1-7-RMSProp算法\"><a href=\"#1-7-RMSProp算法\" class=\"headerlink\" title=\"1.7. RMSProp算法\"></a>1.7. RMSProp算法</h3><p>11.7节中的关键问题之一，是学习率按预定时间表$O(t^{-\\frac{1}{2}})$显著降低。 虽然这通常适用于凸问题，但对于深度学习中遇到的非凸问题，可能并不理想。 但是，作为一个预处理Adagrad算法按坐标顺序的适应性是非常可取的。</p>\n<p>(Tieleman and Hinton, 2012)建议以RMSProp算法作为将速率调度与坐标自适应学习率分离的简单修复方法。 问题在于，Adagrad算法将梯度$\\mathbf{g_t}$的平方累加成状态矢量$\\mathbf{s_t} &#x3D; \\mathbf{s_{t-1} + \\mathbf{g_t}^2}$。 因此，由于缺乏规范化，没有约束力，$\\mathbf{s_t}$持续增长，几乎上是在算法收敛时呈线性递增。</p>\n<p>另一种方法是按动量法中的方式使用泄漏平均值，即$\\mathbf{s_t} \\leftarrow \\gamma \\mathbf{s_{t-1} + (1 - \\gamma)\\mathbf{g_t}^2}$，其中参数$\\gamma &gt; 0$。 保持所有其它部分不变就产生了RMSProp算法。</p>\n<h3 id=\"1-7-1-算法\"><a href=\"#1-7-1-算法\" class=\"headerlink\" title=\"1.7.1. 算法\"></a>1.7.1. 算法</h3><p>让我们详细写出这些方程式。<br>$$<br>\\mathbf{s_t} \\leftarrow \\gamma \\mathbf{s_{t-1}} + (1 - \\gamma) \\mathbf{g_t}^2, \\<br>\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\frac{\\eta}{\\sqrt{\\mathbf{s_t}+ \\epsilon} } \\odot \\mathbf{g_t}.<br>$$</p>\n<p>常数 $\\epsilon$通常设置为$10^{-6}$，以确保我们不会因除以零或步长过大而受到影响。 鉴于这种扩展，我们现在可以自由控制学习率$\\eta$，而不考虑基于每个坐标应用的缩放。</p>\n<h3 id=\"1-7-2-从零开始实现\"><a href=\"#1-7-2-从零开始实现\" class=\"headerlink\" title=\"1.7.2. 从零开始实现\"></a>1.7.2. 从零开始实现</h3><p>和之前一样，我们使用二次函数$f(\\mathbf{x}) &#x3D; 0.1x_1^2 + 2x_2^2$<br>来观察RMSProp算法的轨迹。 回想在上一节，当我们使用学习率为0.4的Adagrad算法时，变量在算法的后期阶段移动非常缓慢，因为学习率衰减太快。 RMSProp算法中不会发生这种情况，因为$\\eta$是单独控制的</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">rmsprop_2d</span>(<span class=\"params\">x1, x2, s1, s2</span>):</span><br><span class=\"line\">    g1, g2, eps = <span class=\"number\">0.2</span> * x1, <span class=\"number\">4</span> * x2, <span class=\"number\">1e-6</span></span><br><span class=\"line\">    s1 = gamma * s1 + (<span class=\"number\">1</span> - gamma) * g1 ** <span class=\"number\">2</span></span><br><span class=\"line\">    s2 = gamma * s2 + (<span class=\"number\">1</span> - gamma) * g2 ** <span class=\"number\">2</span></span><br><span class=\"line\">    x1 -= eta / math.sqrt(s1 + eps) * g1</span><br><span class=\"line\">    x2 -= eta / math.sqrt(s2 + eps) * g2</span><br><span class=\"line\">    <span class=\"keyword\">return</span> x1, x2, s1, s2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f_2d</span>(<span class=\"params\">x1, x2</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">0.1</span> * x1 ** <span class=\"number\">2</span> + <span class=\"number\">2</span> * x2 ** <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\">eta, gamma = <span class=\"number\">0.4</span>, <span class=\"number\">0.9</span></span><br><span class=\"line\">d2l.show_trace_2d(f_2d, d2l.train_2d(rmsprop_2d))</span><br></pre></td></tr></table></figure>\n\n\n\n\n\n<h3 id=\"1-7-3-小结\"><a href=\"#1-7-3-小结\" class=\"headerlink\" title=\"1.7.3. 小结\"></a>1.7.3. 小结</h3><ul>\n<li><p>RMSProp算法与Adagrad算法非常相似，因为两者都使用梯度的平方来缩放系数。</p>\n</li>\n<li><p>RMSProp算法与动量法都使用泄漏平均值。但是，RMSProp算法使用该技术来调整按系数顺序的预处理器。</p>\n</li>\n<li><p>在实验中，学习率需要由实验者调度。</p>\n</li>\n<li><p>系数$\\gamma$决定了在调整每坐标比例时历史记录的时长。</p>\n</li>\n</ul>\n"}