{"summary":"<p>Adam算法 (Kingma and Ba, 2014)将所有这些技术汇总到一个高效的学习算法中。 不出预料，作为深度学习中使用的更强大和有效的优化算法之一，它非常受欢迎。但是它并非没有问题，尤其是 (Reddi et al., 2019)表明，有时Adam算法可能由于方差控制不良而发散。 在完善工作中， (Zaheer et al., 2018)给Adam算法提供了一个称为Yogi的热补丁来解决这些问题。 </p>\n<span id=\"more\"></span>\n\n<h2 id=\"1-8-Adam算法\"><a href=\"#1-8-Adam算法\" class=\"headerlink\" title=\"1.8. Adam算法\"></a>1.8. Adam算法</h2><p>我们已经学习了许多有效优化的技术。 在本节讨论之前，我们先详细回顾一下这些技术：</p>\n<ul>\n<li><p>随机梯度下降在解决优化问题时比梯度下降更有效</p>\n</li>\n<li><p>在一个小批量中使用更大的观测值集，可以通过向量化提供额外效率。这是高效的多机、多GPU和整体并行处理的关键。</p>\n</li>\n<li><p>动量法中我们添加了一种机制，用于汇总过去梯度的历史以加速收敛。</p>\n</li>\n<li><p>AdaGrad算法通过对每个坐标缩放来实现高效计算的预处理器。</p>\n</li>\n</ul>\n<p>Adam算法 (Kingma and Ba, 2014)将所有这些技术汇总到一个高效的学习算法中。 不出预料，作为深度学习中使用的更强大和有效的优化算法之一，它非常受欢迎。但是它并非没有问题，尤其是 (Reddi et al., 2019)表明，有时Adam算法可能由于方差控制不良而发散。 在完善工作中， (Zaheer et al., 2018)给Adam算法提供了一个称为Yogi的热补丁来解决这些问题。 下面我们了解一下Adam算法。</p>\n<h3 id=\"1-8-1-算法\"><a href=\"#1-8-1-算法\" class=\"headerlink\" title=\"1.8.1. 算法\"></a>1.8.1. 算法</h3><p>Adam算法的关键组成部分之一是：它使用指数加权移动平均值来估算梯度的动量和二次矩，即它使用状态变量</p>\n<p>$$<br>\\mathbf{v_t} \\leftarrow \\beta_1 \\mathbf{v_{t-1}} + (1-\\beta_1) \\mathbf{g_t}, \\<br>\\mathbf{s_t} \\leftarrow \\beta_2 \\mathbf{s_{t-1}} + (1 - \\beta_2) \\mathbf{g_t}^2.<br>$$</p>\n<p>这里$\\beta_1$和$beta_2$<br>是非负加权参数。 常将它们设置为$\\beta_1 &#x3D; 0.9$和$\\beta_2&#x3D;0.999$。 也就是说，方差估计的移动远远慢于动量估计的移动。 注意，如果我们初始化$\\mathbf{v_0}&#x3D;\\mathbf{s_0} &#x3D; 0$，就会获得一个相当大的初始偏差。 我们可以通过使用$\\sum_{t&#x3D;0}^{t} \\beta^i &#x3D; \\frac{1 - \\beta^t}{1- \\beta}$来解决这个问题。 </p>\n<p>有了正确的估计，我们现在可以写出更新方程。 首先，我们以非常类似于RMSProp算法的方式重新缩放梯度以获得</p>\n<p>$$<br>\\mathbf{g_t}’ &#x3D; \\frac{\\eta \\mathbf{v_t}}{\\sqrt{\\mathbf{s_t}} + \\epsilon}<br>$$</p>\n<p>与RMSProp不同，我们的更新使用动量$\\mathbf{v_t}$而不是梯度本身, 最后，我们简单更新：<br>$$<br>\\mathbf{x_t} \\leftarrow \\mathbf{x_{t-1}} - \\mathbf{g_t}’.<br>$$<br>回顾Adam算法，它的设计灵感很清楚： 首先，动量和规模在状态变量中清晰可见， 它们相当独特的定义使我们移除偏项（这可以通过稍微不同的初始化和更新条件来修正）。 其次，RMSProp算法中两项的组合都非常简单。 最后，明确的学习率<br>使我们能够控制步长来解决收敛问题。</p>\n<h3 id=\"1-8-2-实现\"><a href=\"#1-8-2-实现\" class=\"headerlink\" title=\"1.8.2. 实现\"></a>1.8.2. 实现</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch.cuda</span><br><span class=\"line\">torch.cuda.is_available() </span><br></pre></td></tr></table></figure>\n\n\n\n\n<pre><code>True\n</code></pre>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br><span class=\"line\"></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">init_adam_states</span>(<span class=\"params\">feature_dim</span>):</span><br><span class=\"line\">    v_w, v_b = torch.zeros((feature_dim, <span class=\"number\">1</span>)), torch.zeros(<span class=\"number\">1</span>)</span><br><span class=\"line\">    s_w, s_b = torch.zeros((feature_dim, <span class=\"number\">1</span>)), torch.zeros(<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> ((v_w, s_w), (v_b, s_b))</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">adam</span>(<span class=\"params\">params, states, hyperparams</span>):</span><br><span class=\"line\">    beta1, beta2, eps = <span class=\"number\">0.9</span>, <span class=\"number\">0.999</span>, <span class=\"number\">1e-6</span></span><br><span class=\"line\">    <span class=\"keyword\">for</span> p, (v, s) <span class=\"keyword\">in</span> <span class=\"built_in\">zip</span>(params, states):</span><br><span class=\"line\">        <span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">            v[:] = beta1 * v + (<span class=\"number\">1</span> - beta1) * p.grad</span><br><span class=\"line\">            s[:] = beta2 * s + (<span class=\"number\">1</span> - beta2) * torch.square(p.grad)</span><br><span class=\"line\">            v_bias_corr = v / (<span class=\"number\">1</span> - beta1 ** hyperparams[<span class=\"string\">&#x27;t&#x27;</span>])</span><br><span class=\"line\">            s_bias_corr = s / (<span class=\"number\">1</span> - beta2 ** hyperparams[<span class=\"string\">&#x27;t&#x27;</span>])</span><br><span class=\"line\">            p[:] -= hyperparams[<span class=\"string\">&#x27;lr&#x27;</span>] * v_bias_corr / (torch.sqrt(s_bias_corr)</span><br><span class=\"line\">                                                       + eps)</span><br><span class=\"line\">        p.grad.data.zero_()</span><br><span class=\"line\">    hyperparams[<span class=\"string\">&#x27;t&#x27;</span>] += <span class=\"number\">1</span></span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"1-8-3-小结\"><a href=\"#1-8-3-小结\" class=\"headerlink\" title=\"1.8.3. 小结\"></a>1.8.3. 小结</h3><ul>\n<li><p>Adam算法将许多优化算法的功能结合到了相当强大的更新规则中。</p>\n</li>\n<li><p>Adam算法在RMSProp算法基础上创建的</p>\n</li>\n</ul>\n"}