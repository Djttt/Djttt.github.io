{"summary":"<p>在前面中，我们一直在训练过程中使用随机梯度下降，但没有解释它为什么起作用。为了解释这一点，我们刚在 1.3中描述了梯度下降的基本原则。本节继续更详细地说明随机梯度下（stochastic gradient descent）。</p>\n<span id=\"more\"></span>\n\n\n<h2 id=\"1-3-随机梯度下降\"><a href=\"#1-3-随机梯度下降\" class=\"headerlink\" title=\"1.3. 随机梯度下降\"></a>1.3. 随机梯度下降</h2><p>在前面中，我们一直在训练过程中使用随机梯度下降，但没有解释它为什么起作用。为了解释这一点，我们刚在 1.3中描述了梯度下降的基本原则。本节继续更详细地说明随机梯度下（stochastic gradient descent）。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%matplotlib inline</span><br><span class=\"line\"><span class=\"keyword\">import</span> math</span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\"><span class=\"keyword\">from</span> d2l <span class=\"keyword\">import</span> torch <span class=\"keyword\">as</span> d2l</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"1-3-1-随机梯度更新\"><a href=\"#1-3-1-随机梯度更新\" class=\"headerlink\" title=\"1.3.1. 随机梯度更新\"></a>1.3.1. 随机梯度更新</h3><p>在深度学习中，目标函数通常是训练数据集中每个样本的损失函数的平均值。给定$n$个样本的训练数据集，我们假设$f_i(\\mathbf{x})$是关于索引$i$的训练样本的损失函数，其中$\\mathbf{x}$是参数向量。然后我们得到目标函数<br>$$<br>f(\\mathbf{x}) &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} f_i(\\mathbf{x}). \\tag{1.3.1}<br>$$<br>$\\mathbf{x}$的目标函数的梯度计算为<br>$$<br>\\nabla f(\\mathbf{x}) &#x3D; \\frac{1}{n} \\sum_{i&#x3D;1}^{n} \\nabla f_i(\\mathbf{x}) \\tag{1.3.2}<br>$$</p>\n<p>如果使用梯度下降法，则每个自变量迭代的计算代价为$O(n)$，它随$n$线性增长。因此，当训练数据集较大时，每次迭代的梯度下降计算代价将较高。</p>\n<p>随机梯度下降（SGD）可降低每次迭代时的计算代价。在随机梯度下降的每次迭代中，我们对数据样本随机均匀采样一个索引$i$，其中$i \\in {i,\\dots, n}$，并计算梯度 $\\nabla f_i(\\mathbf{x})$以更新$\\mathbf{x}$：<br>$$<br>\\mathbf{x} \\leftarrow \\mathbf{x} - \\eta \\nabla f_i(\\mathbf{x}), \\tag{1.3.3}<br>$$<br>其中 $\\eta$ 是学习率。我们可以看到，每次迭代的计算代价从梯度下降的 $O(n)$降至常数O(1)。此外，我们要强调，随机梯度 $\\nabla f_i(\\mathbf{x})$ 是对完整梯度$\\nabla f(\\mathbf{x})$的无偏估计，因为<br>$$<br>\\mathbb{E}<em>i \\nabla f_i(\\mathbf{x}) &#x3D; \\frac{1}{n} \\sum</em>{i&#x3D;1}^{n} \\nabla f_i(\\mathbf{x}) &#x3D; \\nabla f(\\mathbf{x}). \\tag{1.3.4}<br>$$<br>这意味着，平均而言，随机梯度是对梯度的良好估计。</p>\n<p>现在，我们将把它与梯度下降进行比较，方法是向梯度添加均值为0、方差为1的随机噪声，以模拟随机梯度下降。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f</span>(<span class=\"params\">x1, x2</span>):  <span class=\"comment\"># 目标函数</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> x1 ** <span class=\"number\">2</span> + <span class=\"number\">2</span> * x2 ** <span class=\"number\">2</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">f_grad</span>(<span class=\"params\">x1, x2</span>):  <span class=\"comment\"># 目标函数的梯度</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">2</span> * x1, <span class=\"number\">4</span> * x2</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd</span>(<span class=\"params\">x1, x2, s1, s2, f_grad</span>):</span><br><span class=\"line\">    g1, g2 = f_grad(x1, x2)</span><br><span class=\"line\">    <span class=\"comment\"># 模拟有噪声的梯度</span></span><br><span class=\"line\">    g1 += torch.normal(<span class=\"number\">0.0</span>, <span class=\"number\">1</span>, (<span class=\"number\">1</span>,)).item()</span><br><span class=\"line\">    g2 += torch.normal(<span class=\"number\">0.0</span>, <span class=\"number\">1</span>, (<span class=\"number\">1</span>,)).item()</span><br><span class=\"line\">    eta_t = eta * lr()</span><br><span class=\"line\">    <span class=\"keyword\">return</span> (x1 - eta_t * g1, x2 - eta_t * g2, <span class=\"number\">0</span>, <span class=\"number\">0</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">constant_lr</span>():</span><br><span class=\"line\">    <span class=\"keyword\">return</span> <span class=\"number\">1</span></span><br><span class=\"line\"></span><br><span class=\"line\">eta = <span class=\"number\">0.1</span></span><br><span class=\"line\">lr = constant_lr  <span class=\"comment\"># 常数学习速度</span></span><br><span class=\"line\">d2l.show_trace_2d(f, d2l.train_2d(sgd, steps=<span class=\"number\">50</span>, f_grad=f_grad))</span><br></pre></td></tr></table></figure>\n\n<pre><code>epoch 50, x1: -0.002198, x2: 0.017078\n</code></pre>\n<p><img src=\"/2024/10/20/deep-learning-optimizer-3/11_3_3_1.svg\" alt=\"svg\"></p>\n<p>正如我们所看到的，随机梯度下降中变量的轨迹比我们在 11.3节中观察到的梯度下降中观察到的轨迹嘈杂得多。这是由于梯度的随机性质。也就是说，即使我们接近最小值，我们仍然受到通过$\\eta \\nabla f_i(\\mathbf{x})$<br>的瞬间梯度所注入的不确定性的影响。即使经过50次迭代，质量仍然不那么好。更糟糕的是，经过额外的步骤，它不会得到改善。</p>\n<h3 id=\"1-3-2-小结\"><a href=\"#1-3-2-小结\" class=\"headerlink\" title=\"1.3.2. 小结\"></a>1.3.2. 小结</h3><ul>\n<li><p>对于凸问题，我们可以证明，对于广泛的学习率选择，随机梯度下降将收敛到最优解。</p>\n</li>\n<li><p>如果学习率太小或太大，就会出现问题。实际上，通常只有经过多次实验后才能找到合适的学习率。</p>\n</li>\n<li><p>当训练数据集中有更多样本时，计算梯度下降的每次迭代的代价更高，因此在这些情况下，首选随机梯度下降。</p>\n</li>\n<li><p>随机梯度下降的最优性保证在非凸情况下一般不可用</p>\n</li>\n</ul>\n"}