<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>Self-Attention, Transformers - Jun Deng</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Jun Deng"><meta name="msapplication-TileImage" content="/images/dp.ico"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Jun Deng"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin."><meta property="og:type" content="blog"><meta property="og:title" content="Self-Attention, Transformers"><meta property="og:url" content="https://djttt.github.io/2024/11/27/Self-Attention-Transformers/"><meta property="og:site_name" content="Jun Deng"><meta property="og:description" content="Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin."><meta property="og:locale" content="en_US"><meta property="og:image" content="https://djttt.github.io/img/og_image.png"><meta property="article:published_time" content="2024-11-27T14:49:20.000Z"><meta property="article:modified_time" content="2024-11-28T14:26:13.399Z"><meta property="article:author" content="DJ"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://djttt.github.io/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://djttt.github.io/2024/11/27/Self-Attention-Transformers/"},"headline":"Self-Attention, Transformers","image":["https://djttt.github.io/img/og_image.png"],"datePublished":"2024-11-27T14:49:20.000Z","dateModified":"2024-11-28T14:26:13.399Z","author":{"@type":"Person","name":"DJ"},"publisher":{"@type":"Organization","name":"Jun Deng","logo":{"@type":"ImageObject","url":"https://djttt.github.io/images/dp.ico"}},"description":"Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin."}</script><link rel="canonical" href="https://djttt.github.io/2024/11/27/Self-Attention-Transformers/"><link rel="icon" href="/images/dp.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link data-pjax rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@11.7.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link data-pjax rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?8d52d949447e08ee9870e34535172c86";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/dp.ico" alt="Jun Deng" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Home</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/about">About</a><a class="navbar-item" href="/download/optimizer.pptx">Download</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item">Posted&nbsp;<time dateTime="2024-11-27T14:49:20.000Z" title="11/27/2024, 10:49:20 PM">2024-11-27</time></span><span class="level-item">Updated&nbsp;<time dateTime="2024-11-28T14:26:13.399Z" title="11/28/2024, 10:26:13 PM">2024-11-28</time></span><span class="level-item">15 minutes read (About 2223 words)</span></div></div><h1 class="title is-3 is-size-4-mobile">Self-Attention, Transformers</h1><div class="content"><p>Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin.</p>
<span id="more"></span>


<h1 id="Self-Attention-Transformer"><a href="#Self-Attention-Transformer" class="headerlink" title="Self-Attention, Transformer"></a>Self-Attention, Transformer</h1><h2 id="1-Attention-Exploration"><a href="#1-Attention-Exploration" class="headerlink" title="1. Attention Exploration"></a>1. Attention Exploration</h2><p>Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin</p>
<p>Recall that attention can be viewed as an operation on a $query \in \mathbb{R^d}$, a set of value vectors $ {v_1,\dots ,v_n }, v_i \in \mathbb{R^d}$, and a set of key vectors $\left{k_1, \dots ,k_n \right}, k_i \in \mathbb{R^d},$ specified as follows:<br>$$<br>c &#x3D; \sum_{i&#x3D;1}^{n} v_i \alpha_i    \tag{1}<br>$$<br>$$<br>\alpha_i &#x3D; \frac{\exp(k_i^{\top}q)}{\sum_{j&#x3D;1}^{n} \exp(k_i^{\top} q)}  \tag{2}<br>$$<br>with $alpha &#x3D; { \alpha_1, \dots ,\alpha_n  } $ termed the “attention weights”. Observe that the output $c \in \mathbb{R^d} $ is an average over the value vectors weighted with respect to $\alpha$</p>
<ul>
<li><p>One advantage of attention is that it’s particularly easy to<br>“copy” a value vector to the output c. In this problem, we’ll motivate why this is the case.</p>
<ul>
<li>The distribution $\alpha$ is typically relatively “diffuse”; the probability mass is spread out between many different $\alpha_i$ However, this is not always the case. Describe (in one sentence)<br>under what conditions the categorical distribution $\alpha$ puts almost all of its weight on some $\alpha_j$, where $j \in {1, \dots, n  }(i.e. ; \alpha_j \gg \sum_{i \neq j} \alpha_i)$<ul>
<li>A: when the query is much similar with $k_i, i \in  {i,\dots, n } $ and query is not similar with all other $k_j, j\neq i$. It’s mean only $\alpha_i$ is enough big, and other $\alpha_j$ is small.</li>
</ul>
</li>
<li>Under the conditions you gave in <strong>describe</strong> the output c.<ul>
<li>output $c$ is more likely with $v_i$. Because $c$ is a average weight sum of value vectors.</li>
</ul>
</li>
</ul>
</li>
<li><p>An average of two. Instead of focusing on just one vector $v_j$ , a Transformer model<br>might want to incorporate information from $multiple$ source vectors.<br>Consider the case where we instead want to incorporate information from two vectors $v_a$ and $v_b$,<br>with corresponding key vectors $k_a$ and $k_b$. Assume that (1) all key vectors are orthogonal, so $k^\top k_j $ for all $ i \neq j$ and all key vectors have norm 1. Find an expression for a query vector<br>$q$ such that $c \approx \frac{1}{2}<br>(v_a + v_b)$, and justify your answer.</p>
<ul>
<li>Orthogonality: $k_a^\top k_b&#x3D;0$ meaning $k_a$ and $k_b$ are orthogonal</li>
<li>Normalization: $||k_a|| &#x3D; ||k_b|| &#x3D; 1$ meaning $k_a^\top k_a &#x3D; 1$</li>
<li>The attention weights for $v_a$ and $v_b$ are determined by the query $q$:<br>  $$<br>  \alpha_a &#x3D; \frac{\exp(k_a^\top q)}{\sum_{j&#x3D;1}^{n} \exp(k_j^\top q)}, ;<br>  \alpha_b &#x3D; \frac{\exp(k_b^\top q)}{\sum_{j&#x3D;1}^{n} \exp(k_j^\top q)}<br>  $$<br>  to make sure $\alpha_a$ and $\alpha_b$ are same. A natural choice is the normalized sum of $k_a$ and $k_b$<br>  $$<br>  q &#x3D; \frac{k_a + k_b}{||k_a + k_b||}<br>  $$<br>Justification:</li>
</ul>
</li>
</ul>
<ul>
<li>Denominator: only keep $\frac{2}{||k_a + k_b||} $ </li>
<li>molecular: $\alpha_i &#x3D; 0 $ for each $i \neq a,b $ so $\alpha_a &#x3D; \frac{1}{||k_a + k_b||}$, $\alpha_b$ is the same value.</li>
</ul>
<p><strong>Drawbacks of single-headed attention</strong>: In the previous part, we saw how it was<br>possible for a single-headed attention to focus equally on two values. The same concept could easily be extended to any subset of values. In this question we’ll see why it’s not a practical solution.Consider a set of key vectors ${k_1, \dots ,k_n }$ that are now randomly sampled, $k_i \sim \mathcal{N}\left(\mu_i, \sum_i \right)$, where the means $\mu_i \in \mathbb{R^d}$ are known to you, but the covariances $\sum_i$ are unknown. Further, assume that the means $\mu_i$ are all perpendicular; $\mu_i^\top \mu_j &#x3D; 0$ if $i \neq j$, and unit norm, $||\mu_i|| &#x3D; 1$.</p>
<ul>
<li>Assume that the covariance matrices are $\sum_i &#x3D; \alpha I, \forall i \in {1, 2, \dots, n}$, for vanishingly small $\alpha$. esign a query $q$ in terms of the $\mu_i$ such that as before, $c \approx \frac{1}{2}(v_a + v_b)$, and provide a brief argument as to why it works.<ul>
<li>designing a query vector $q$ such that the resulting context vector $c$ approximates $\frac{1}{2}$, where $v_a$ and $v_b$ are the corresponding value vectors associated with $k_a$ and $k_b$ respectively. <ul>
<li>The covariance matrices $\Sigma_i &#x3D; \alpha I$ are small, meaning that the key vector $k_i$ are spread out around their respective mean vectors $\mu_i$,<br>  but the variance is small, their spread is controlled by $\alpha$. So the value of $k_i$ are likely to be very close to their mean vector $\mu_i$ </li>
<li>To ensure the context vector $c$ is approximately $\frac{1}{2}(v_a + v_b)$, the query vector $q$ should be designed so that it can give the same values of keys $k_a$ and $k_b$. Since $\mu_a$ and $\mu_b$ the orthogonal(i.e., $\mu_a^\top \mu_b &#x3D; 0 $). We want to create a $q$ that is equally aligned with both $\mu_a$ and $\mu_b$.<br>  Thus, we define the query vector $q$ as:<br>  $$<br>  q &#x3D; \frac{\mu_a + \mu_b}{||\mu_a + \mu_b||}.<br>  $$<br>  when q dot products with $\mu_j, j \neq a, b$ get all values is $0$. so last only get $k_a^\top q$ and $k_b^\top q$. The two valuse is same.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>Though single-headed attention is resistant to small perturbations in the keys, some<br>types of larger perturbations may pose a bigger issue. In some cases, one key vector $k_a$ may be larger or smaller in norm than the others, while still pointing in the same direction as $\mu_a$. (Unlike the original Transformer, newer Transformer modles apply layer normalization before attention, In these prelayernorm models, norms of keys cannot be too different which makes the situation in this question less likely to occur).<br>    - When you sample ${k_1, \dots, k_n  }$ multiple times, and use the $q$ vector that you defined above, what do you expect the vector $c$ will look like qualitatively for different samples? Think about how it differs from part (i) and how c’s variance would be affected.<br>        - The output vector $c$ is average weight sum of values. specifily, the query will apply dot product with the set of keys, then compute $\alpha_i$ repect with $k_i$.<br>        $$<br>        c &#x3D; \sum_{i&#x3D;1}^n &#x3D; \alpha_i v_i<br>        $$<br>        that means if $q$ and $k_i$ apply dot prodoct if $q$ is similar with $k_i$, then get a bigger attention weight $\alpha_i$. in $c$, the $v_i$ will have a bigger proportion. the other $v_j$ is also in $c$, but maybe have not a big proportion.  $c$’s variance will be reflected by the diversity in the sample space, ${k_1, \dots, k_n }$</p>
<h2 id="2-Position-Embeddings-Exploration"><a href="#2-Position-Embeddings-Exploration" class="headerlink" title="2. Position Embeddings Exploration"></a>2. Position Embeddings Exploration</h2><p>Position embeddings are an important component of the Transformer architecture, allowing the model to differentiate between tokens based on their position in the sequence. In this question, we’ll explore the need for positional embeddings in Transformers and how they can be designed.</p>
<p>Recall that the crucial components of the Transformer architecture are the self-attention layer and the<br>feed-forward neural network layer. Given an input tensor $X \in \mathbb{R^{ T \times d} }$, where $T$ is the sequence length and $d$ is the hidden dimension,<br>the self-attention layer computes the following:<br>$$<br>\mathbf{Q} &#x3D; \mathbf{X} \mathbf{W_Q},  \space  \mathbf{K} &#x3D; \mathbf{X} \mathbf{W_K},   \space \mathbf{V} &#x3D; \mathbf{X} \mathbf{W_V}<br>$$<br>$$<br>\mathbf{H} &#x3D; \text{softmax}\left(\frac{\mathbf{Q \mathbf{K^\top}}}{ \mathbf{\sqrt{d}} }\right) \mathbf{V}<br>$$<br>where $\mathbf{W_Q}, \mathbf{W_K}, \mathbf{W_V} \in \mathbb{R^{d \times d}} $ are weight matrices, and $\mathbf{H} \in \mathbb{R^{T \times d}}$ is the output.<br>Next, the feed-forward layer applies the following transformation:<br>$$<br>\mathbf{Z} &#x3D; \text{ReLU}(\mathbf{H} \mathbf{W_1} + \mathbf{1} \cdot \mathbf{b_1}) \mathbf{W_2} + \mathbf{1} \cdot \mathbf{b_2}<br>$$<br>where $\mathbf{W_1}, \mathbf{W_2} \in \mathbb{R^{d \times d}}$ and $\mathbf{b_1}, \mathbf{b_2} \in \mathbb{R^{1 \times d}}$ are weights and biases; $\mathbf{1} \in \mathbb{R^{T \times 1}}$ is a vector of ones; and $\mathbf{Z} \in \mathbb{R^{ T \times d}}$ is the final output. </p>
<p>Permuting the input.<br>    - suppose we permute the input sequence $\mathbf{X}$ such that the tokens are shuffled ran-domly. This can be represented as multiplication by a permutation matrix $\mathbf{P} \in \mathbb{R^{T \times T}}$, i.e. $\mathbf{X_perm} &#x3D; \mathbf{PX}.$<br>    - Show that the output $\mathbf{Z_perm}$ for the permuterd input $\mathbf{X_{\text{perm}}}$ will be $\mathbf{Z_{\text{perm}}} &#x3D;  \mathbf{PZ}$.<br>    You are given that for any permutation matrix $\mathbf{P}$ and any matrix $\mathbf{A}$, the following hold:<br>    $\text{softmax}(\mathbf{PAP^\top}) &#x3D; \mathbf{P} \text{softmax}(\mathbf{A}) \mathbf{P^\top}$ and $\text{ReLU}(\mathbf{PA}) &#x3D; \mathbf{P} \text{ReLU}(\mathbf{A})$.<br>        - $$\mathbf{Q_{\text{perm}}} &#x3D; \mathbf{X_{\text{perm}}} \mathbf{W_Q} &#x3D; (\mathbf{PX}) \mathbf{W_Q} &#x3D; \mathbf{P} (\mathbf{XW_Q}) &#x3D; \mathbf{PQ}<br>        $$<br>        $$<br>        \mathbf{K_{\text{perm}}} &#x3D; \mathbf{PK}<br>        $$<br>        $$<br>        \mathbf{V_{\text{perm}}} &#x3D; \mathbf{PV} \<br>        $$<br>        The self-attention output $\mathbf{H_\text{perm}}$ for the permuted input is computed as:<br>        $$<br>        \mathbf{H_\text{perm}} &#x3D; \text{softmax}\left( \frac{\mathbf{Q_\text{perm}}  \mathbf{K_\text{perm}^\top}}{\sqrt{d}}  \right) \mathbf{V_\text{perm}} &#x3D; \text{softmax}\left( \frac{\mathbf{PQK^\top P^\top}}{ \sqrt{d} }  \right) \mathbf{PV}  \<br>        $$<br>        so, using the given property of the softmax function for permutation matrices:<br>        $$<br>        \mathbf{H_\text{perm}} &#x3D; \mathbf{PH}<br>        $$<br>        Next, we apply the feed-forward layer to the self-attention output:<br>        $$<br>        \mathbf{Z_\text{perm}} &#x3D; \text{ReLU}( \mathbf{H_{\text{perm}}} \mathbf{W_1} + \mathbf{1} \mathbf{b_1} ) \mathbf{W_2} + \mathbf{1} \mathbf{b_2} \<br>        \mathbf{Z_\text{perm}} &#x3D; \mathbf{P} \text{ReLU}( \mathbf{H} \mathbf{W_1} + \mathbf{1} \mathbf{b_1} ) \mathbf{W_2} + \mathbf{1} \mathbf{b_2}<br>        $$<br>        last, get the final result:<br>        $$<br>        \mathbf{Z_\text{perm}} &#x3D; \mathbf{PZ}<br>        $$<br>Think about the implications of the result you derived in. Explain why this<br>property of the Transformer model could be problematic when processing text.<br>    - A: Permuting the Input Causes Corresponding Changes in the Output. The result tells us that if we permute the input sequence $X$ by a permutation matrix $P$, the output $Z$ will also be permuted by the same matrix.<br>    but for example, the sentences:<br>        - “The cat chased the dog.”<br>        - “The dog chased the cat.”<br>    These two sentences have completely different meanings, and this difference is entirely due to the order of the words.<br>    - Transformer’s Indifference to Sequence Order: The result indicates that if the Transformer receives a permuted version of the input sequence, the output is simply the same sequence permuted. This implies that the model, without any additional mechanisms (like position encoding), would not “understand” the inherent order of the input sequence. This is a significant issue for tasks where word order is crucial for meaning</p>
<p><strong>Position embeddings</strong> are ectors that encode the position of each token in the se-quence. They are added to the input word embeddings before feeding them into the Transformer. One approach is to generate position embedding using a fixed function of the position and the dimension of the embedding. If the input word embeddings are $\mathbf{X} \in \mathbb{R^{T \times d}}$, the position embeddings<br>$\Phi \in \mathbb{R^{T \times d}}$ are generated as follows:<br>$$<br>\Phi_{(t, 2i)} &#x3D; \sin \left(t &#x2F; 10000^{\frac{2i}{d}} \right) \<br>\Phi_{(t, 2i+1)} &#x3D; \cos \left( t &#x2F; 10000^{\frac{2i}{d}}    \right)<br>$$<br>where $t \in {0,1, \dots, \text{T} - 1}$ and $i \in {0, 1, \dots, \frac{d}{2} -1  }$.<br>Specifically, the position embeddings are added to the input word embeddings.<br>$$<br>\mathbf{X_{\text{pos}}} &#x3D; \mathbf{X} + \phi<br>$$<br>Do you think the position embeddings will help the issue you identified in part (a)?<br>If yes, explain how and if not, explain why not.</p>
<ul>
<li>yes, <strong>Position Sensitivity</strong>: By adding the position embeddings to the word embeddings, the Transformer can now distinguish tokens based not only on their content but also on their position in the sequence. The position embeddings inject information about where a token appears in the sequence (e.g., first, second, last), which allows the model to learn the order-dependent relationships between tokens.<br><strong>Preserving Sequential Structure:</strong>: After applying position embeddings, the Transformer can distinguish between different permutations of the input sequence. This is crucial because, in natural language, the meaning of a sentence depends on the relative positions of words (e.g., “cat chased dog” is different from “dog chased cat”). Without position embeddings, the Transformer would treat all tokens as unordered, which would make it impossible for the model to understand such dependencies.</li>
</ul>
<p>Can the position embeddings for two different tokens in the input sequence be the<br>same? If yes, provide an example. If not, explain why not.</p>
<ul>
<li>No, The position embeddings are designed to uniquely encode the position of each token in the sequence. They are generated using a fixed function based on the position $t$  of each token, which <strong>means that each position in the sequence has a distinct embedding.</strong></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Self-Attention, Transformers</p><p><a href="https://djttt.github.io/2024/11/27/Self-Attention-Transformers/">https://djttt.github.io/2024/11/27/Self-Attention-Transformers/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>DJ</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>2024-11-27</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>2024-11-28</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><!--!--></article></div><!--!--><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2025/01/11/2024-year-commit/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">2024 year commit</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2024/11/24/Contrastive_Learning/"><span class="level-item">自监督学习(Self-Supervesed Learning)</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><!--!--></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/images/logo.png" alt="Jun Deng"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Jun Deng</p><p class="is-size-6 is-block">a student major in computer science</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>chengdu China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">0</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">2</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/djttt" target="_blank" rel="me noopener">Follow</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Github" href="https://github.com/djttt"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Facebook" href="https://facebook.com"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Twitter" href="https://twitter.com"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="Dribbble" href="https://dribbble.com"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="me noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><!--!--><div class="card widget" data-type="links"><div class="card-content"><div class="menu"><h3 class="menu-label">Links</h3><ul class="menu-list"><li><a class="level is-mobile" href="https://hexo.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Hexo</span></span><span class="level-right"><span class="level-item tag">hexo.io</span></span></a></li><li><a class="level is-mobile" href="https://bulma.io" target="_blank" rel="noopener"><span class="level-left"><span class="level-item">Bulma</span></span><span class="level-right"><span class="level-item tag">bulma.io</span></span></a></li></ul></div></div></div><!--!--><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">Recents</h3><article class="media"><div class="media-content"><p class="date"><time dateTime="2025-01-11T07:54:57.000Z">2025-01-11</time></p><p class="title"><a href="/2025/01/11/2024-year-commit/">2024 year commit</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-27T14:49:20.000Z">2024-11-27</time></p><p class="title"><a href="/2024/11/27/Self-Attention-Transformers/">Self-Attention, Transformers</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-11-24T06:16:09.000Z">2024-11-24</time></p><p class="title"><a href="/2024/11/24/Contrastive_Learning/">自监督学习(Self-Supervesed Learning)</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-31T14:35:46.000Z">2024-10-31</time></p><p class="title"><a href="/2024/10/31/Essay-Writing-Tips/">Essay-Writing-Tips</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time dateTime="2024-10-20T05:47:20.000Z">2024-10-20</time></p><p class="title"><a href="/2024/10/20/deep-learning-optimizer-8/">deep-learning-optimizer-8</a></p></div></article></div></div><div class="card widget" data-type="archives"><div class="card-content"><div class="menu"><h3 class="menu-label">Archives</h3><ul class="menu-list"><li><a class="level is-mobile" href="/archives/2025/01/"><span class="level-start"><span class="level-item">January 2025</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/11/"><span class="level-start"><span class="level-item">November 2024</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/archives/2024/10/"><span class="level-start"><span class="level-item">October 2024</span></span><span class="level-end"><span class="level-item tag">9</span></span></a></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">Tags</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/deep-learning/"><span class="tag">deep learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/essay-writing/"><span class="tag">essay writing</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/dp.ico" alt="Jun Deng" height="28"></a><p class="is-size-7"><span>&copy; 2025 DJ</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script data-pjax src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script data-pjax src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "This website uses cookies to improve your experience.",
          dismiss: "Got it!",
          allow: "Allow cookies",
          deny: "Decline",
          link: "Learn more",
          policy: "Cookie Policy",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/mhchem.min.js" defer></script><script>window.addEventListener("load", function() {
            document.querySelectorAll('[role="article"] > .content').forEach(function(element) {
                renderMathInElement(element);
            });
        });</script><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax@0.2.8/pjax.min.js"></script><script src="/js/pjax.js"></script><!--!--><!--!--><!--!--><script data-pjax src="/js/main.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>