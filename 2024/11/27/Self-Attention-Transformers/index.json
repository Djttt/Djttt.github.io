{"summary":"<p>Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin.</p>\n<span id=\"more\"></span>\n\n\n<h1 id=\"Self-Attention-Transformer\"><a href=\"#Self-Attention-Transformer\" class=\"headerlink\" title=\"Self-Attention, Transformer\"></a>Self-Attention, Transformer</h1><h2 id=\"Attention-Exploration\"><a href=\"#Attention-Exploration\" class=\"headerlink\" title=\"Attention Exploration\"></a>Attention Exploration</h2><p>Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin.</p>\n<p>Recall that attention can be viewed as an operation on a $query \\in \\mathbb{R^d}$, a set of value vectors $ {v_1,\\dots ,v_n }, v_i \\in \\mathbb{R^d}$, and a set of key vectors $\\left{k_1, \\dots ,k_n \\right}, k_i \\in \\mathbb{R^d},$ specified as follows:<br>$$<br>c &#x3D; \\sum_{i&#x3D;1}^{n} v_i \\alpha_i    \\tag{1}<br>$$<br>$$<br>\\alpha_i &#x3D; \\frac{\\exp(k_i^{\\top}q)}{\\sum_{j&#x3D;1}^{n} \\exp(k_i^{\\top} q)}  \\tag{2}<br>$$<br>with $alpha &#x3D; { \\alpha_1, \\dots ,\\alpha_n  } $ termed the “attention weights”. Observe that the output $c \\in \\mathbb{R^d} $ is an average over the value vectors weighted with respect to $\\alpha$</p>\n<ul>\n<li><p>One advantage of attention is that it’s particularly easy to<br>“copy” a value vector to the output c. In this problem, we’ll motivate why this is the case.</p>\n<ul>\n<li>The distribution $\\alpha$ is typically relatively “diffuse”; the probability mass is spread out between many different $\\alpha_i$ However, this is not always the case. Describe (in one sentence)<br>under what conditions the categorical distribution $\\alpha$ puts almost all of its weight on some $\\alpha_j$, where $j \\in {1, \\dots, n  }(i.e. ; \\alpha_j \\gg \\sum_{i \\neq j} \\alpha_i)$<ul>\n<li>A: when the query is much similar with $k_i, i \\in  {i,\\dots, n } $ and query is not similar with all other $k_j, j\\neq i$. It’s mean only $\\alpha_i$ is enough big, and other $\\alpha_j$ is small.</li>\n</ul>\n</li>\n<li>Under the conditions you gave in <strong>describe</strong> the output c.<ul>\n<li>output $c$ is more likely with $v_i$. Because $c$ is a average weight sum of value vectors.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>An average of two. Instead of focusing on just one vector $v_j$ , a Transformer model<br>might want to incorporate information from $multiple$ source vectors.<br>Consider the case where we instead want to incorporate information from two vectors $v_a$ and $v_b$,<br>with corresponding key vectors $k_a$ and $k_b$. Assume that (1) all key vectors are orthogonal, so $k^\\top k_j $ for all $ i \\neq j$ and all key vectors have norm 1. Find an expression for a query vector<br>$q$ such that $c \\approx \\frac{1}{2}<br>(v_a + v_b)$, and justify your answer.</p>\n<ul>\n<li>Orthogonality: $k_a^\\top k_b&#x3D;0$ meaning $k_a$ and $k_b$ are orthogonal</li>\n<li>Normalization: $||k_a|| &#x3D; ||k_b|| &#x3D; 1$ meaning $k_a^\\top k_a &#x3D; 1$</li>\n<li>The attention weights for $v_a$ and $v_b$ are determined by the query $q$:<br>  $$<br>  \\alpha_a &#x3D; \\frac{\\exp(k_a^\\top q)}{\\sum_{j&#x3D;1}^{n} \\exp(k_j^\\top q)}, ;<br>  \\alpha_b &#x3D; \\frac{\\exp(k_b^\\top q)}{\\sum_{j&#x3D;1}^{n} \\exp(k_j^\\top q)}<br>  $$<br>  to make sure $\\alpha_a$ and $\\alpha_b$ are same. A natural choice is the normalized sum of $k_a$ and $k_b$<br>  $$<br>  q &#x3D; \\frac{k_a + k_b}{||k_a + k_b||}<br>  $$<br>Justification:</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Denominator: only keep $\\frac{2}{||k_a + k_b||} $ </li>\n<li>molecular: $\\alpha_i &#x3D; 0 $ for each $i \\neq a,b $ so $\\alpha_a &#x3D; \\frac{1}{||k_a + k_b||}$, $\\alpha_b$ is the same value.</li>\n</ul>\n"}