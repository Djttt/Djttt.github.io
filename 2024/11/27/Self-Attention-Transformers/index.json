{"summary":"<p>Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin.</p>\n<span id=\"more\"></span>\n\n\n<h1 id=\"Self-Attention-Transformer\"><a href=\"#Self-Attention-Transformer\" class=\"headerlink\" title=\"Self-Attention, Transformer\"></a>Self-Attention, Transformer</h1><h2 id=\"1-Attention-Exploration\"><a href=\"#1-Attention-Exploration\" class=\"headerlink\" title=\"1. Attention Exploration\"></a>1. Attention Exploration</h2><p>Multi-head self-attention is the core modeling component of Transformer. So why multi-headed self-attention can be preferable to single-headed self-attentin</p>\n<p>Recall that attention can be viewed as an operation on a $query \\in \\mathbb{R^d}$, a set of value vectors $ {v_1,\\dots ,v_n }, v_i \\in \\mathbb{R^d}$, and a set of key vectors $\\left{k_1, \\dots ,k_n \\right}, k_i \\in \\mathbb{R^d},$ specified as follows:<br>$$<br>c &#x3D; \\sum_{i&#x3D;1}^{n} v_i \\alpha_i    \\tag{1}<br>$$<br>$$<br>\\alpha_i &#x3D; \\frac{\\exp(k_i^{\\top}q)}{\\sum_{j&#x3D;1}^{n} \\exp(k_i^{\\top} q)}  \\tag{2}<br>$$<br>with $alpha &#x3D; { \\alpha_1, \\dots ,\\alpha_n  } $ termed the “attention weights”. Observe that the output $c \\in \\mathbb{R^d} $ is an average over the value vectors weighted with respect to $\\alpha$</p>\n<ul>\n<li><p>One advantage of attention is that it’s particularly easy to<br>“copy” a value vector to the output c. In this problem, we’ll motivate why this is the case.</p>\n<ul>\n<li>The distribution $\\alpha$ is typically relatively “diffuse”; the probability mass is spread out between many different $\\alpha_i$ However, this is not always the case. Describe (in one sentence)<br>under what conditions the categorical distribution $\\alpha$ puts almost all of its weight on some $\\alpha_j$, where $j \\in {1, \\dots, n  }(i.e. ; \\alpha_j \\gg \\sum_{i \\neq j} \\alpha_i)$<ul>\n<li>A: when the query is much similar with $k_i, i \\in  {i,\\dots, n } $ and query is not similar with all other $k_j, j\\neq i$. It’s mean only $\\alpha_i$ is enough big, and other $\\alpha_j$ is small.</li>\n</ul>\n</li>\n<li>Under the conditions you gave in <strong>describe</strong> the output c.<ul>\n<li>output $c$ is more likely with $v_i$. Because $c$ is a average weight sum of value vectors.</li>\n</ul>\n</li>\n</ul>\n</li>\n<li><p>An average of two. Instead of focusing on just one vector $v_j$ , a Transformer model<br>might want to incorporate information from $multiple$ source vectors.<br>Consider the case where we instead want to incorporate information from two vectors $v_a$ and $v_b$,<br>with corresponding key vectors $k_a$ and $k_b$. Assume that (1) all key vectors are orthogonal, so $k^\\top k_j $ for all $ i \\neq j$ and all key vectors have norm 1. Find an expression for a query vector<br>$q$ such that $c \\approx \\frac{1}{2}<br>(v_a + v_b)$, and justify your answer.</p>\n<ul>\n<li>Orthogonality: $k_a^\\top k_b&#x3D;0$ meaning $k_a$ and $k_b$ are orthogonal</li>\n<li>Normalization: $||k_a|| &#x3D; ||k_b|| &#x3D; 1$ meaning $k_a^\\top k_a &#x3D; 1$</li>\n<li>The attention weights for $v_a$ and $v_b$ are determined by the query $q$:<br>  $$<br>  \\alpha_a &#x3D; \\frac{\\exp(k_a^\\top q)}{\\sum_{j&#x3D;1}^{n} \\exp(k_j^\\top q)}, ;<br>  \\alpha_b &#x3D; \\frac{\\exp(k_b^\\top q)}{\\sum_{j&#x3D;1}^{n} \\exp(k_j^\\top q)}<br>  $$<br>  to make sure $\\alpha_a$ and $\\alpha_b$ are same. A natural choice is the normalized sum of $k_a$ and $k_b$<br>  $$<br>  q &#x3D; \\frac{k_a + k_b}{||k_a + k_b||}<br>  $$<br>Justification:</li>\n</ul>\n</li>\n</ul>\n<ul>\n<li>Denominator: only keep $\\frac{2}{||k_a + k_b||} $ </li>\n<li>molecular: $\\alpha_i &#x3D; 0 $ for each $i \\neq a,b $ so $\\alpha_a &#x3D; \\frac{1}{||k_a + k_b||}$, $\\alpha_b$ is the same value.</li>\n</ul>\n<p><strong>Drawbacks of single-headed attention</strong>: In the previous part, we saw how it was<br>possible for a single-headed attention to focus equally on two values. The same concept could easily be extended to any subset of values. In this question we’ll see why it’s not a practical solution.Consider a set of key vectors ${k_1, \\dots ,k_n }$ that are now randomly sampled, $k_i \\sim \\mathcal{N}\\left(\\mu_i, \\sum_i \\right)$, where the means $\\mu_i \\in \\mathbb{R^d}$ are known to you, but the covariances $\\sum_i$ are unknown. Further, assume that the means $\\mu_i$ are all perpendicular; $\\mu_i^\\top \\mu_j &#x3D; 0$ if $i \\neq j$, and unit norm, $||\\mu_i|| &#x3D; 1$.</p>\n<ul>\n<li>Assume that the covariance matrices are $\\sum_i &#x3D; \\alpha I, \\forall i \\in {1, 2, \\dots, n}$, for vanishingly small $\\alpha$. esign a query $q$ in terms of the $\\mu_i$ such that as before, $c \\approx \\frac{1}{2}(v_a + v_b)$, and provide a brief argument as to why it works.<ul>\n<li>designing a query vector $q$ such that the resulting context vector $c$ approximates $\\frac{1}{2}$, where $v_a$ and $v_b$ are the corresponding value vectors associated with $k_a$ and $k_b$ respectively. <ul>\n<li>The covariance matrices $\\Sigma_i &#x3D; \\alpha I$ are small, meaning that the key vector $k_i$ are spread out around their respective mean vectors $\\mu_i$,<br>  but the variance is small, their spread is controlled by $\\alpha$. So the value of $k_i$ are likely to be very close to their mean vector $\\mu_i$ </li>\n<li>To ensure the context vector $c$ is approximately $\\frac{1}{2}(v_a + v_b)$, the query vector $q$ should be designed so that it can give the same values of keys $k_a$ and $k_b$. Since $\\mu_a$ and $\\mu_b$ the orthogonal(i.e., $\\mu_a^\\top \\mu_b &#x3D; 0 $). We want to create a $q$ that is equally aligned with both $\\mu_a$ and $\\mu_b$.<br>  Thus, we define the query vector $q$ as:<br>  $$<br>  q &#x3D; \\frac{\\mu_a + \\mu_b}{||\\mu_a + \\mu_b||}.<br>  $$<br>  when q dot products with $\\mu_j, j \\neq a, b$ get all values is $0$. so last only get $k_a^\\top q$ and $k_b^\\top q$. The two valuse is same.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n<p>Though single-headed attention is resistant to small perturbations in the keys, some<br>types of larger perturbations may pose a bigger issue. In some cases, one key vector $k_a$ may be larger or smaller in norm than the others, while still pointing in the same direction as $\\mu_a$. (Unlike the original Transformer, newer Transformer modles apply layer normalization before attention, In these prelayernorm models, norms of keys cannot be too different which makes the situation in this question less likely to occur).<br>    - When you sample ${k_1, \\dots, k_n  }$ multiple times, and use the $q$ vector that you defined above, what do you expect the vector $c$ will look like qualitatively for different samples? Think about how it differs from part (i) and how c’s variance would be affected.<br>        - The output vector $c$ is average weight sum of values. specifily, the query will apply dot product with the set of keys, then compute $\\alpha_i$ repect with $k_i$.<br>        $$<br>        c &#x3D; \\sum_{i&#x3D;1}^n &#x3D; \\alpha_i v_i<br>        $$<br>        that means if $q$ and $k_i$ apply dot prodoct if $q$ is similar with $k_i$, then get a bigger attention weight $\\alpha_i$. in $c$, the $v_i$ will have a bigger proportion. the other $v_j$ is also in $c$, but maybe have not a big proportion.  $c$’s variance will be reflected by the diversity in the sample space, ${k_1, \\dots, k_n }$</p>\n<h2 id=\"2-Position-Embeddings-Exploration\"><a href=\"#2-Position-Embeddings-Exploration\" class=\"headerlink\" title=\"2. Position Embeddings Exploration\"></a>2. Position Embeddings Exploration</h2><p>Position embeddings are an important component of the Transformer architecture, allowing the model to differentiate between tokens based on their position in the sequence. In this question, we’ll explore the need for positional embeddings in Transformers and how they can be designed.</p>\n<p>Recall that the crucial components of the Transformer architecture are the self-attention layer and the<br>feed-forward neural network layer. Given an input tensor $X \\in \\mathbb{R^{ T \\times d} }$, where $T$ is the sequence length and $d$ is the hidden dimension,<br>the self-attention layer computes the following:<br>$$<br>\\mathbf{Q} &#x3D; \\mathbf{X} \\mathbf{W_Q},  \\space  \\mathbf{K} &#x3D; \\mathbf{X} \\mathbf{W_K},   \\space \\mathbf{V} &#x3D; \\mathbf{X} \\mathbf{W_V}<br>$$<br>$$<br>\\mathbf{H} &#x3D; \\text{softmax}\\left(\\frac{\\mathbf{Q \\mathbf{K^\\top}}}{ \\mathbf{\\sqrt{d}} }\\right) \\mathbf{V}<br>$$<br>where $\\mathbf{W_Q}, \\mathbf{W_K}, \\mathbf{W_V} \\in \\mathbb{R^{d \\times d}} $ are weight matrices, and $\\mathbf{H} \\in \\mathbb{R^{T \\times d}}$ is the output.<br>Next, the feed-forward layer applies the following transformation:<br>$$<br>\\mathbf{Z} &#x3D; \\text{ReLU}(\\mathbf{H} \\mathbf{W_1} + \\mathbf{1} \\cdot \\mathbf{b_1}) \\mathbf{W_2} + \\mathbf{1} \\cdot \\mathbf{b_2}<br>$$<br>where $\\mathbf{W_1}, \\mathbf{W_2} \\in \\mathbb{R^{d \\times d}}$ and $\\mathbf{b_1}, \\mathbf{b_2} \\in \\mathbb{R^{1 \\times d}}$ are weights and biases; $\\mathbf{1} \\in \\mathbb{R^{T \\times 1}}$ is a vector of ones; and $\\mathbf{Z} \\in \\mathbb{R^{ T \\times d}}$ is the final output. </p>\n<p>Permuting the input.<br>    - suppose we permute the input sequence $\\mathbf{X}$ such that the tokens are shuffled ran-domly. This can be represented as multiplication by a permutation matrix $\\mathbf{P} \\in \\mathbb{R^{T \\times T}}$, i.e. $\\mathbf{X_perm} &#x3D; \\mathbf{PX}.$<br>    - Show that the output $\\mathbf{Z_perm}$ for the permuterd input $\\mathbf{X_{\\text{perm}}}$ will be $\\mathbf{Z_{\\text{perm}}} &#x3D;  \\mathbf{PZ}$.<br>    You are given that for any permutation matrix $\\mathbf{P}$ and any matrix $\\mathbf{A}$, the following hold:<br>    $\\text{softmax}(\\mathbf{PAP^\\top}) &#x3D; \\mathbf{P} \\text{softmax}(\\mathbf{A}) \\mathbf{P^\\top}$ and $\\text{ReLU}(\\mathbf{PA}) &#x3D; \\mathbf{P} \\text{ReLU}(\\mathbf{A})$.<br>        - $$\\mathbf{Q_{\\text{perm}}} &#x3D; \\mathbf{X_{\\text{perm}}} \\mathbf{W_Q} &#x3D; (\\mathbf{PX}) \\mathbf{W_Q} &#x3D; \\mathbf{P} (\\mathbf{XW_Q}) &#x3D; \\mathbf{PQ}<br>        $$<br>        $$<br>        \\mathbf{K_{\\text{perm}}} &#x3D; \\mathbf{PK}<br>        $$<br>        $$<br>        \\mathbf{V_{\\text{perm}}} &#x3D; \\mathbf{PV} \\<br>        $$<br>        The self-attention output $\\mathbf{H_\\text{perm}}$ for the permuted input is computed as:<br>        $$<br>        \\mathbf{H_\\text{perm}} &#x3D; \\text{softmax}\\left( \\frac{\\mathbf{Q_\\text{perm}}  \\mathbf{K_\\text{perm}^\\top}}{\\sqrt{d}}  \\right) \\mathbf{V_\\text{perm}} &#x3D; \\text{softmax}\\left( \\frac{\\mathbf{PQK^\\top P^\\top}}{ \\sqrt{d} }  \\right) \\mathbf{PV}  \\<br>        $$<br>        so, using the given property of the softmax function for permutation matrices:<br>        $$<br>        \\mathbf{H_\\text{perm}} &#x3D; \\mathbf{PH}<br>        $$<br>        Next, we apply the feed-forward layer to the self-attention output:<br>        $$<br>        \\mathbf{Z_\\text{perm}} &#x3D; \\text{ReLU}( \\mathbf{H_{\\text{perm}}} \\mathbf{W_1} + \\mathbf{1} \\mathbf{b_1} ) \\mathbf{W_2} + \\mathbf{1} \\mathbf{b_2} \\<br>        \\mathbf{Z_\\text{perm}} &#x3D; \\mathbf{P} \\text{ReLU}( \\mathbf{H} \\mathbf{W_1} + \\mathbf{1} \\mathbf{b_1} ) \\mathbf{W_2} + \\mathbf{1} \\mathbf{b_2}<br>        $$<br>        last, get the final result:<br>        $$<br>        \\mathbf{Z_\\text{perm}} &#x3D; \\mathbf{PZ}<br>        $$<br>Think about the implications of the result you derived in. Explain why this<br>property of the Transformer model could be problematic when processing text.<br>    - A: Permuting the Input Causes Corresponding Changes in the Output. The result tells us that if we permute the input sequence $X$ by a permutation matrix $P$, the output $Z$ will also be permuted by the same matrix.<br>    but for example, the sentences:<br>        - “The cat chased the dog.”<br>        - “The dog chased the cat.”<br>    These two sentences have completely different meanings, and this difference is entirely due to the order of the words.<br>    - Transformer’s Indifference to Sequence Order: The result indicates that if the Transformer receives a permuted version of the input sequence, the output is simply the same sequence permuted. This implies that the model, without any additional mechanisms (like position encoding), would not “understand” the inherent order of the input sequence. This is a significant issue for tasks where word order is crucial for meaning</p>\n<p><strong>Position embeddings</strong> are ectors that encode the position of each token in the se-quence. They are added to the input word embeddings before feeding them into the Transformer. One approach is to generate position embedding using a fixed function of the position and the dimension of the embedding. If the input word embeddings are $\\mathbf{X} \\in \\mathbb{R^{T \\times d}}$, the position embeddings<br>$\\Phi \\in \\mathbb{R^{T \\times d}}$ are generated as follows:<br>$$<br>\\Phi_{(t, 2i)} &#x3D; \\sin \\left(t &#x2F; 10000^{\\frac{2i}{d}} \\right) \\<br>\\Phi_{(t, 2i+1)} &#x3D; \\cos \\left( t &#x2F; 10000^{\\frac{2i}{d}}    \\right)<br>$$<br>where $t \\in {0,1, \\dots, \\text{T} - 1}$ and $i \\in {0, 1, \\dots, \\frac{d}{2} -1  }$.<br>Specifically, the position embeddings are added to the input word embeddings.<br>$$<br>\\mathbf{X_{\\text{pos}}} &#x3D; \\mathbf{X} + \\phi<br>$$<br>Do you think the position embeddings will help the issue you identified in part (a)?<br>If yes, explain how and if not, explain why not.</p>\n<ul>\n<li>yes, <strong>Position Sensitivity</strong>: By adding the position embeddings to the word embeddings, the Transformer can now distinguish tokens based not only on their content but also on their position in the sequence. The position embeddings inject information about where a token appears in the sequence (e.g., first, second, last), which allows the model to learn the order-dependent relationships between tokens.<br><strong>Preserving Sequential Structure:</strong>: After applying position embeddings, the Transformer can distinguish between different permutations of the input sequence. This is crucial because, in natural language, the meaning of a sentence depends on the relative positions of words (e.g., “cat chased dog” is different from “dog chased cat”). Without position embeddings, the Transformer would treat all tokens as unordered, which would make it impossible for the model to understand such dependencies.</li>\n</ul>\n<p>Can the position embeddings for two different tokens in the input sequence be the<br>same? If yes, provide an example. If not, explain why not.</p>\n<ul>\n<li>No, The position embeddings are designed to uniquely encode the position of each token in the sequence. They are generated using a fixed function based on the position $t$  of each token, which <strong>means that each position in the sequence has a distinct embedding.</strong></li>\n</ul>\n"}